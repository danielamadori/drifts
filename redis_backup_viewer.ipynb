{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizzatore di backup Redis\n",
        "\n",
        "Questo notebook consente di ispezionare i file JSON generati da ``redis_backup``. Carica il payload, ne mostra il riepilogo e prova a decodificare i valori memorizzati come stringhe per presentarli in chiaro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurazione del percorso\n",
        "\n",
        "Imposta il percorso del file di backup JSON esportato con ``save_backup_to_file``. Il notebook non esegue alcuna connessione a Redis: tutte le informazioni vengono lette direttamente dal file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Any, Iterable\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "from redis_backup import (\n",
        "    decode_bytes,\n",
        "    display_backup_summary,\n",
        "    load_backup_from_file,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Percorso del backup da analizzare.\n",
        "BACKUP_PATH = Path('/percorso/al/tuo/backup.json')\n",
        "\n",
        "backup = None\n",
        "if BACKUP_PATH.exists():\n",
        "    backup = load_backup_from_file(BACKUP_PATH)\n",
        "    display_backup_summary(backup)\n",
        "else:\n",
        "    print(\"⚠️ Aggiorna BACKUP_PATH con il percorso corretto del file di backup.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "class DumpDecodeError(RuntimeError):\n",
        "    \"\"\"Errore generico per la decodifica di payload DUMP.\"\"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DumpSections:\n",
        "    payload: bytes\n",
        "    version: int\n",
        "    checksum: bytes\n",
        "\n",
        "\n",
        "def split_dump_sections(raw: bytes) -> DumpSections:\n",
        "    \"\"\"Separa payload, versione RDB e checksum da un dump Redis.\"\"\"\n",
        "\n",
        "    if len(raw) < 10:\n",
        "        raise DumpDecodeError(\"Payload DUMP troppo corto per contenere metadati\")\n",
        "    checksum = raw[-8:]\n",
        "    version_bytes = raw[-10:-8]\n",
        "    version = int.from_bytes(version_bytes, \"little\", signed=False)\n",
        "    payload = raw[:-10]\n",
        "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
        "\n",
        "\n",
        "class _LengthEncoding:\n",
        "    __slots__ = (\"value\", \"encoding\")\n",
        "\n",
        "    def __init__(self, value: int | None, encoding: int | None = None) -> None:\n",
        "        self.value = value\n",
        "        self.encoding = encoding\n",
        "\n",
        "\n",
        "RDB_ENCODING_INT8 = 0\n",
        "RDB_ENCODING_INT16 = 1\n",
        "RDB_ENCODING_INT32 = 2\n",
        "RDB_ENCODING_LZF = 3\n",
        "\n",
        "\n",
        "def _read_length_info(buffer: bytes, offset: int) -> tuple[_LengthEncoding, int]:\n",
        "    if offset >= len(buffer):\n",
        "        raise DumpDecodeError(\"Offset fuori range durante la lettura della lunghezza\")\n",
        "    first = buffer[offset]\n",
        "    prefix = first >> 6\n",
        "    if prefix == 0:\n",
        "        length = first & 0x3F\n",
        "        return _LengthEncoding(length), offset + 1\n",
        "    if prefix == 1:\n",
        "        if offset + 1 >= len(buffer):\n",
        "            raise DumpDecodeError(\"Lunghezza codificata su 14 bit troncata\")\n",
        "        second = buffer[offset + 1]\n",
        "        length = ((first & 0x3F) << 8) | second\n",
        "        return _LengthEncoding(length), offset + 2\n",
        "    if prefix == 2:\n",
        "        if offset + 4 >= len(buffer):\n",
        "            raise DumpDecodeError(\"Lunghezza codificata su 32 bit troncata\")\n",
        "        length = int.from_bytes(buffer[offset + 1 : offset + 5], \"big\", signed=False)\n",
        "        return _LengthEncoding(length), offset + 5\n",
        "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
        "\n",
        "\n",
        "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
        "    \"\"\"Implementazione minimale della decompressione LZF usata da Redis.\"\"\"\n",
        "\n",
        "    output = bytearray()\n",
        "    idx = 0\n",
        "    data_len = len(data)\n",
        "    while idx < data_len:\n",
        "        ctrl = data[idx]\n",
        "        idx += 1\n",
        "        if ctrl < 32:\n",
        "            literal_len = ctrl + 1\n",
        "            if idx + literal_len > data_len:\n",
        "                raise DumpDecodeError(\"Sequenza LZF letterale troncata\")\n",
        "            output.extend(data[idx : idx + literal_len])\n",
        "            idx += literal_len\n",
        "        else:\n",
        "            length = ctrl >> 5\n",
        "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
        "            if length == 7:\n",
        "                if idx >= data_len:\n",
        "                    raise DumpDecodeError(\"Sequenza LZF troncata durante l'estensione della lunghezza\")\n",
        "                length += data[idx]\n",
        "                idx += 1\n",
        "            if idx >= data_len:\n",
        "                raise DumpDecodeError(\"Sequenza LZF troncata durante il calcolo del riferimento\")\n",
        "            ref_offset -= data[idx]\n",
        "            idx += 1\n",
        "            length += 2\n",
        "            if ref_offset < 0:\n",
        "                raise DumpDecodeError(\"Riferimento LZF negativo\")\n",
        "            for _ in range(length):\n",
        "                if ref_offset >= len(output):\n",
        "                    raise DumpDecodeError(\"Riferimento LZF fuori range\")\n",
        "                output.append(output[ref_offset])\n",
        "                ref_offset += 1\n",
        "    if len(output) != expected_length:\n",
        "        raise DumpDecodeError(\n",
        "            f\"Lunghezza decompressa inattesa: atteso {expected_length}, ottenuto {len(output)}\"\n",
        "        )\n",
        "    return bytes(output)\n",
        "\n",
        "\n",
        "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int) -> tuple[bytes, int]:\n",
        "    if encoding == RDB_ENCODING_INT8:\n",
        "        if offset >= len(buffer):\n",
        "            raise DumpDecodeError(\"Intero codificato su 8 bit troncato\")\n",
        "        value = int.from_bytes(buffer[offset : offset + 1], \"little\", signed=True)\n",
        "        return str(value).encode(\"ascii\"), offset + 1\n",
        "    if encoding == RDB_ENCODING_INT16:\n",
        "        if offset + 1 >= len(buffer):\n",
        "            raise DumpDecodeError(\"Intero codificato su 16 bit troncato\")\n",
        "        value = int.from_bytes(buffer[offset : offset + 2], \"little\", signed=True)\n",
        "        return str(value).encode(\"ascii\"), offset + 2\n",
        "    if encoding == RDB_ENCODING_INT32:\n",
        "        if offset + 3 >= len(buffer):\n",
        "            raise DumpDecodeError(\"Intero codificato su 32 bit troncato\")\n",
        "        value = int.from_bytes(buffer[offset : offset + 4], \"little\", signed=True)\n",
        "        return str(value).encode(\"ascii\"), offset + 4\n",
        "    if encoding == RDB_ENCODING_LZF:\n",
        "        length_info, offset = _read_length_info(buffer, offset)\n",
        "        if length_info.value is None:\n",
        "            raise DumpDecodeError(\"Lunghezza compressa non valida\")\n",
        "        compressed_len = length_info.value\n",
        "        length_info, offset = _read_length_info(buffer, offset)\n",
        "        if length_info.value is None:\n",
        "            raise DumpDecodeError(\"Lunghezza originale non valida\")\n",
        "        uncompressed_len = length_info.value\n",
        "        end = offset + compressed_len\n",
        "        if end > len(buffer):\n",
        "            raise DumpDecodeError(\"Dati LZF compressi troncati\")\n",
        "        chunk = buffer[offset:end]\n",
        "        offset = end\n",
        "        return lzf_decompress(chunk, uncompressed_len), offset\n",
        "    raise DumpDecodeError(f\"Codifica speciale non supportata: {encoding}\")\n",
        "\n",
        "\n",
        "def _read_encoded_string(buffer: bytes, offset: int) -> tuple[bytes, int]:\n",
        "    length_info, offset = _read_length_info(buffer, offset)\n",
        "    if length_info.value is not None:\n",
        "        length = length_info.value\n",
        "        end = offset + length\n",
        "        if end > len(buffer):\n",
        "            raise DumpDecodeError(\"Stringa codificata troncata\")\n",
        "        return buffer[offset:end], end\n",
        "    if length_info.encoding is None:\n",
        "        raise DumpDecodeError(\"Codifica stringa sconosciuta\")\n",
        "    return _decode_special_encoding(buffer, offset, length_info.encoding)\n",
        "\n",
        "\n",
        "def decode_string_from_dump(raw: bytes) -> bytes:\n",
        "    sections = split_dump_sections(raw)\n",
        "    payload = sections.payload\n",
        "    if not payload:\n",
        "        raise DumpDecodeError(\"Payload vuoto\")\n",
        "    object_type = payload[0]\n",
        "    if object_type != 0:\n",
        "        raise DumpDecodeError(f\"Tipo di oggetto non stringa: {object_type}\")\n",
        "    value, offset = _read_encoded_string(payload, 1)\n",
        "    if offset != len(payload):\n",
        "        # Ignora eventuali byte addizionali non previsti\n",
        "        value = value\n",
        "    return value\n",
        "\n",
        "\n",
        "def decode_key(entry: dict[str, Any]) -> bytes:\n",
        "    return decode_bytes(entry[\"key\"])\n",
        "\n",
        "\n",
        "def text_preview(value: bytes, *, limit: int = 120) -> str:\n",
        "    text = value.decode(\"utf-8\", errors=\"replace\")\n",
        "    if len(text) > limit:\n",
        "        return text[: limit - 1] + \"…\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def try_decode_value(entry: dict[str, Any]) -> tuple[str, dict[str, Any]]:\n",
        "    value_info = dict(entry.get(\"value\") or {})\n",
        "    data_b64 = value_info.get(\"data\")\n",
        "    if not data_b64:\n",
        "        return \"<nessun valore>\", value_info\n",
        "    raw = decode_bytes(data_b64)\n",
        "    details: dict[str, Any] = {\n",
        "        \"dump_size\": len(raw),\n",
        "    }\n",
        "    try:\n",
        "        sections = split_dump_sections(raw)\n",
        "        details[\"rdb_version\"] = sections.version\n",
        "        details[\"checksum\"] = sections.checksum.hex()\n",
        "    except DumpDecodeError as exc:\n",
        "        details[\"errore_dump\"] = str(exc)\n",
        "        return \"<dump non valido>\", details\n",
        "    if entry.get(\"type\") == \"string\":\n",
        "        try:\n",
        "            decoded = decode_string_from_dump(raw)\n",
        "        except DumpDecodeError as exc:\n",
        "            details[\"errore_decodifica\"] = str(exc)\n",
        "            return \"<stringa non decodificata>\", details\n",
        "        details[\"decoded_bytes\"] = decoded\n",
        "        preview = text_preview(decoded)\n",
        "        return preview, details\n",
        "    return f\"<{entry.get('type')} - {len(sections.payload)} byte>\", details\n",
        "\n",
        "\n",
        "def summarise_entries(entries: Iterable[dict[str, Any]], limit: int = 20) -> list[dict[str, Any]]:\n",
        "    summary: list[dict[str, Any]] = []\n",
        "    for index, entry in enumerate(entries):\n",
        "        if index >= limit:\n",
        "            break\n",
        "        key_bytes = decode_key(entry)\n",
        "        value_preview, _ = try_decode_value(entry)\n",
        "        summary.append(\n",
        "            {\n",
        "                \"key\": text_preview(key_bytes),\n",
        "                \"type\": entry.get(\"type\"),\n",
        "                \"ttl_ms\": entry.get(\"pttl\"),\n",
        "                \"value_preview\": value_preview,\n",
        "            }\n",
        "        )\n",
        "    return summary\n",
        "\n",
        "\n",
        "def find_entry(entries: Iterable[dict[str, Any]], key: bytes | str) -> dict[str, Any]:\n",
        "    if isinstance(key, str):\n",
        "        key_bytes = key.encode(\"utf-8\")\n",
        "    else:\n",
        "        key_bytes = key\n",
        "    for entry in entries:\n",
        "        if decode_key(entry) == key_bytes:\n",
        "            return entry\n",
        "    raise KeyError(key)\n",
        "\n",
        "\n",
        "def inspect_entry(entry: dict[str, Any]) -> None:\n",
        "    key_bytes = decode_key(entry)\n",
        "    print(f\"Chiave: {key_bytes!r}\")\n",
        "    print(f\"Tipo Redis: {entry.get('type')}\")\n",
        "    ttl = entry.get(\"pttl\")\n",
        "    print(f\"TTL (ms): {ttl if ttl is not None else 'persistente'}\")\n",
        "    preview, details = try_decode_value(entry)\n",
        "    print(f\"Anteprima valore: {preview}\")\n",
        "    print(\"Dettagli:\")\n",
        "    pprint.pprint(details)\n",
        "    decoded = details.get(\"decoded_bytes\")\n",
        "    if isinstance(decoded, (bytes, bytearray)):\n",
        "        text = decoded.decode(\"utf-8\", errors=\"replace\")\n",
        "        print(\"\n",
        "Contenuto in chiaro:\")\n",
        "        print(text)\n",
        "        stripped = text.strip()\n",
        "        if stripped.startswith(\"{\") and stripped.endswith(\"}\"):\n",
        "            try:\n",
        "                parsed = json.loads(text)\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "            else:\n",
        "                print(\"\n",
        "JSON decodificato:\")\n",
        "                pprint.pprint(parsed)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if backup is not None:\n",
        "    entries = list(backup.get('entries', []))\n",
        "    print(f'Chiavi totali nel backup: {len(entries)}')\n",
        "    preview_rows = summarise_entries(entries, limit=20)\n",
        "    try:\n",
        "        import pandas as pd  # type: ignore\n",
        "    except ModuleNotFoundError:\n",
        "        pd = None\n",
        "    if preview_rows:\n",
        "        if 'pd' in locals() and pd is not None:\n",
        "            display(pd.DataFrame(preview_rows))\n",
        "        else:\n",
        "            for row in preview_rows:\n",
        "                print(row)\n",
        "    else:\n",
        "        print('Nessuna chiave nel backup selezionato.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ispezione di una chiave specifica\n",
        "\n",
        "Utilizza ``find_entry`` per recuperare una chiave dal backup e ``inspect_entry`` per mostrarne i dettagli e il contenuto (quando possibile).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if backup is not None:\n",
        "    # Esempio: sostituisci 'nome:della:chiave' con la chiave che vuoi analizzare.\n",
        "    try:\n",
        "        entry = find_entry(entries, 'nome:della:chiave')\n",
        "    except KeyError:\n",
        "        print('Chiave non trovata: aggiorna il nome per visualizzare il contenuto.')\n",
        "    else:\n",
        "        inspect_entry(entry)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}