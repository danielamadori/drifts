{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55041dcae764f7ce",
   "metadata": {},
   "source": [
    "# Results archive inspection\n",
    "\n",
    "This notebook automatically inspects every `.zip` file stored in the `results` directory.\n",
    "It parses the filename of each archive to extract useful metadata, relies on the included\n",
    "`manifest.json` file to map Redis database dumps to their logical meaning, and previews\n",
    "all extracted files directly below. Large files are truncated to the first bytes so the\n",
    "notebook stays responsive."
   ]
  },
  {
   "cell_type": "code",
   "id": "49976e1c760312e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:47:02.065415Z",
     "start_time": "2025-10-23T16:47:02.058639Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import base64\n",
    "import binascii\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "HOST_BASE = 9\n",
    "DB_LABELS = {\n",
    "    0: \"DATA\",\n",
    "    1: \"CAN\",\n",
    "    2: \"R\",\n",
    "    3: \"NR\",\n",
    "    4: \"CAR\",\n",
    "    5: \"AR\",\n",
    "    6: \"GP\",\n",
    "    7: \"BP\",\n",
    "    8: \"PR\",\n",
    "    9: \"AP\",\n",
    "    10: \"LOGS\",\n",
    "}\n",
    "MAX_FULL_BYTES = 200_000\n",
    "MAX_PREVIEW_BYTES = 10_000\n",
    "\n",
    "zip_paths = sorted(RESULTS_DIR.glob(\"*.zip\"))\n",
    "zip_inventory = {\n",
    "    \"results_dir\": str(RESULTS_DIR),\n",
    "    \"count\": len(zip_paths),\n",
    "    \"found\": bool(zip_paths),\n",
    "    \"paths\": [str(path) for path in zip_paths],\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "fbb975d1e10ba092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:47:02.110198Z",
     "start_time": "2025-10-23T16:47:02.089357Z"
    }
   },
   "source": [
    "def format_bytes(size):\n",
    "    \"\"\"Return a human-readable representation of a file size.\"\"\"\n",
    "    if size is None:\n",
    "        return \"-\"\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    value = float(size)\n",
    "    for unit in units:\n",
    "        if value < 1024 or unit == units[-1]:\n",
    "            if unit == \"B\":\n",
    "                return f\"{int(value)} {unit}\"\n",
    "            return f\"{value:.2f} {unit}\"\n",
    "        value /= 1024\n",
    "    return f\"{value:.2f} B\"\n",
    "\n",
    "\n",
    "def parse_zip_metadata(zip_path):\n",
    "    \"\"\"Extract dataset, class, completion flag, and host numbers from the archive name.\"\"\"\n",
    "    dataset, class_name, completion_flag, host_fragment = zip_path.stem.rsplit(\"_\", 3)\n",
    "    try:\n",
    "        host_offset = int(host_fragment)\n",
    "        host_id = host_offset + HOST_BASE\n",
    "    except ValueError:\n",
    "        host_offset = None\n",
    "        host_id = None\n",
    "    flag_lower = completion_flag.lower()\n",
    "    if flag_lower in {\"true\", \"false\"}:\n",
    "        is_completed = flag_lower == \"true\"\n",
    "    else:\n",
    "        is_completed = None\n",
    "    size_bytes = zip_path.stat().st_size\n",
    "    return {\n",
    "        \"zip_path\": str(zip_path),\n",
    "        \"zip_name\": zip_path.name,\n",
    "        \"dataset\": dataset,\n",
    "        \"class\": class_name,\n",
    "        \"completion_raw\": completion_flag,\n",
    "        \"is_completed\": is_completed,\n",
    "        \"size_bytes\": size_bytes,\n",
    "        \"size_text\": format_bytes(size_bytes),\n",
    "        \"host_offset\": host_offset,\n",
    "        \"host_id\": host_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_root_prefix(archive, zip_path):\n",
    "    \"\"\"Guess the common directory prefix used inside the archive.\"\"\"\n",
    "    stem_prefix = f\"{zip_path.stem}/\"\n",
    "    has_stem = any(\n",
    "        info.filename.startswith(stem_prefix)\n",
    "        for info in archive.infolist()\n",
    "        if not info.is_dir()\n",
    "    )\n",
    "    if has_stem:\n",
    "        return stem_prefix\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def resolve_manifest(archive, zip_path):\n",
    "    \"\"\"Return the manifest data together with the prefix used inside the archive.\"\"\"\n",
    "    candidates = []\n",
    "    stem_prefix = f\"{zip_path.stem}/\"\n",
    "    candidates.append(stem_prefix)\n",
    "    for info in archive.infolist():\n",
    "        if info.is_dir():\n",
    "            dirname = info.filename\n",
    "            if dirname.startswith(\"__MACOSX/\"):\n",
    "                continue\n",
    "            if not dirname.endswith(\"/\"):\n",
    "                dirname += \"/\"\n",
    "            candidates.append(dirname)\n",
    "    candidates.append(\"\")\n",
    "    seen = set()\n",
    "    for prefix in candidates:\n",
    "        if prefix in seen:\n",
    "            continue\n",
    "        seen.add(prefix)\n",
    "        manifest_path = f\"{prefix}manifest.json\"\n",
    "        try:\n",
    "            with archive.open(manifest_path) as manifest_file:\n",
    "                manifest = json.load(manifest_file)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        else:\n",
    "            return prefix, manifest\n",
    "    raise KeyError(\"manifest.json not found\")\n",
    "\n",
    "\n",
    "class DumpDecodeError(RuntimeError):\n",
    "    \"\"\"Generic error raised while decoding a Redis DUMP payload.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DumpSections:\n",
    "    payload: bytes\n",
    "    version: int\n",
    "    checksum: bytes\n",
    "\n",
    "\n",
    "class _LengthEncoding:\n",
    "    __slots__ = (\"value\", \"encoding\")\n",
    "\n",
    "    def __init__(self, value=None, encoding=None):\n",
    "        self.value = value\n",
    "        self.encoding = encoding\n",
    "\n",
    "\n",
    "RDB_ENCODING_INT8 = 0\n",
    "RDB_ENCODING_INT16 = 1\n",
    "RDB_ENCODING_INT32 = 2\n",
    "RDB_ENCODING_LZF = 3\n",
    "\n",
    "\n",
    "def split_dump_sections(raw: bytes) -> DumpSections:\n",
    "    \"\"\"Split payload, RDB version, and checksum from a Redis dump.\"\"\"\n",
    "    if len(raw) < 10:\n",
    "        raise DumpDecodeError(\"DUMP payload is too short to contain metadata\")\n",
    "    checksum = raw[-8:]\n",
    "    version_bytes = raw[-10:-8]\n",
    "    version = int.from_bytes(version_bytes, \"little\", signed=False)\n",
    "    payload = raw[:-10]\n",
    "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
    "\n",
    "\n",
    "def _read_length_info(buffer: bytes, offset: int):\n",
    "    if offset >= len(buffer):\n",
    "        raise DumpDecodeError(\"Offset out of range while reading length\")\n",
    "    first = buffer[offset]\n",
    "    prefix = first >> 6\n",
    "    if prefix == 0:\n",
    "        length = first & 0x3F\n",
    "        return _LengthEncoding(length), offset + 1\n",
    "    if prefix == 1:\n",
    "        if offset + 1 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 14-bit encoded length\")\n",
    "        second = buffer[offset + 1]\n",
    "        length = ((first & 0x3F) << 8) | second\n",
    "        return _LengthEncoding(length), offset + 2\n",
    "    if prefix == 2:\n",
    "        if offset + 4 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded length\")\n",
    "        length = int.from_bytes(buffer[offset + 1 : offset + 5], \"big\", signed=False)\n",
    "        return _LengthEncoding(length), offset + 5\n",
    "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
    "\n",
    "\n",
    "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
    "    \"\"\"Minimal implementation of the LZF decompression used by Redis.\"\"\"\n",
    "    output = bytearray()\n",
    "    idx = 0\n",
    "    data_len = len(data)\n",
    "    while idx < data_len:\n",
    "        ctrl = data[idx]\n",
    "        idx += 1\n",
    "        if ctrl < 32:\n",
    "            literal_len = ctrl + 1\n",
    "            if idx + literal_len > data_len:\n",
    "                raise DumpDecodeError(\"Truncated literal LZF sequence\")\n",
    "            output.extend(data[idx : idx + literal_len])\n",
    "            idx += literal_len\n",
    "        else:\n",
    "            length = ctrl >> 5\n",
    "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
    "            if length == 7:\n",
    "                if idx >= data_len:\n",
    "                    raise DumpDecodeError(\"Truncated LZF sequence while extending length\")\n",
    "                length += data[idx]\n",
    "                idx += 1\n",
    "            if idx >= data_len:\n",
    "                raise DumpDecodeError(\"Truncated LZF sequence while resolving reference\")\n",
    "            ref_offset -= data[idx]\n",
    "            idx += 1\n",
    "            length += 2\n",
    "            if ref_offset < 0:\n",
    "                raise DumpDecodeError(\"Negative LZF reference\")\n",
    "            for _ in range(length):\n",
    "                if ref_offset >= len(output):\n",
    "                    raise DumpDecodeError(\"LZF reference out of range\")\n",
    "                output.append(output[ref_offset])\n",
    "                ref_offset += 1\n",
    "    if len(output) != expected_length:\n",
    "        raise DumpDecodeError(\n",
    "            f\"Unexpected decompressed length: expected {expected_length}, got {len(output)}\"\n",
    "        )\n",
    "    return bytes(output)\n",
    "\n",
    "\n",
    "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int):\n",
    "    if encoding == RDB_ENCODING_INT8:\n",
    "        if offset >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 8-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 1], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 1\n",
    "    if encoding == RDB_ENCODING_INT16:\n",
    "        if offset + 2 > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 16-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 2], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 2\n",
    "    if encoding == RDB_ENCODING_INT32:\n",
    "        if offset + 4 > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 4], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 4\n",
    "    if encoding == RDB_ENCODING_LZF:\n",
    "        data_len_info, next_offset = _read_length_info(buffer, offset)\n",
    "        compressed_len_info, compressed_offset = _read_length_info(buffer, next_offset)\n",
    "        if data_len_info.value is None or compressed_len_info.value is None:\n",
    "            raise DumpDecodeError(\"Invalid LZF length encoding\")\n",
    "        end = compressed_offset + compressed_len_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        compressed = buffer[compressed_offset:end]\n",
    "        decompressed = lzf_decompress(compressed, data_len_info.value)\n",
    "        return decompressed, end\n",
    "    raise DumpDecodeError(\"Unknown string encoding\")\n",
    "\n",
    "\n",
    "def _read_encoded_string(buffer: bytes, offset: int):\n",
    "    length_info, next_offset = _read_length_info(buffer, offset)\n",
    "    if length_info.encoding is None:\n",
    "        end = next_offset + length_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        return buffer[next_offset:end], end\n",
    "    return _decode_special_encoding(buffer, next_offset, length_info.encoding)\n",
    "\n",
    "\n",
    "def decode_string_from_dump(raw: bytes) -> bytes:\n",
    "    sections = split_dump_sections(raw)\n",
    "    payload = sections.payload\n",
    "    if not payload:\n",
    "        raise DumpDecodeError(\"Empty payload\")\n",
    "    object_type = payload[0]\n",
    "    if object_type != 0:\n",
    "        raise DumpDecodeError(f\"Non-string object type: {object_type}\")\n",
    "    value, _ = _read_encoded_string(payload, 1)\n",
    "    return value\n",
    "\n",
    "\n",
    "def decode_bytes(value: str) -> bytes:\n",
    "    if not isinstance(value, str):\n",
    "        raise DumpDecodeError(\"Encoded value must be a string\")\n",
    "    try:\n",
    "        return base64.b64decode(value.encode(\"ascii\"))\n",
    "    except (UnicodeEncodeError, binascii.Error) as exc:\n",
    "        raise DumpDecodeError(f\"Invalid base64 payload: {exc}\") from exc\n",
    "\n",
    "\n",
    "def decode_key(entry):\n",
    "    return decode_bytes(entry[\"key\"])\n",
    "\n",
    "\n",
    "def text_preview(value: bytes, limit: int = 120) -> str:\n",
    "    text = value.decode(\"utf-8\", errors=\"replace\")\n",
    "    if len(text) > limit:\n",
    "        return text[: limit - 1] + \".\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def try_decode_value(entry):\n",
    "    value_info = dict(entry.get(\"value\") or {})\n",
    "    data_b64 = value_info.get(\"data\")\n",
    "    if not data_b64:\n",
    "        return \"<no value>\", value_info\n",
    "    try:\n",
    "        raw = decode_bytes(data_b64)\n",
    "    except DumpDecodeError as exc:\n",
    "        value_info[\"decode_error\"] = str(exc)\n",
    "        return \"<invalid base64>\", value_info\n",
    "    details = {\n",
    "        \"dump_size\": len(raw),\n",
    "    }\n",
    "    try:\n",
    "        sections = split_dump_sections(raw)\n",
    "        details[\"rdb_version\"] = sections.version\n",
    "        details[\"checksum\"] = sections.checksum.hex()\n",
    "    except DumpDecodeError as exc:\n",
    "        details[\"dump_error\"] = str(exc)\n",
    "        return \"<invalid dump>\", details\n",
    "    if entry.get(\"type\") == \"string\":\n",
    "        try:\n",
    "            decoded = decode_string_from_dump(raw)\n",
    "        except DumpDecodeError as exc:\n",
    "            details[\"decode_error\"] = str(exc)\n",
    "            return \"<string not decoded>\", details\n",
    "        details[\"decoded_bytes\"] = decoded\n",
    "        preview = text_preview(decoded)\n",
    "        return preview, details\n",
    "    return f\"<{entry.get('type')} - {len(sections.payload)} bytes>\", details\n",
    "\n",
    "\n",
    "def shorten_text(text: str, limit: int = 600) -> str:\n",
    "    sanitized = text.replace(\"````\", \"``` `\")\n",
    "    if len(sanitized) > limit:\n",
    "        return sanitized[: limit - 1] + \".\"\n",
    "    return sanitized\n",
    "\n",
    "\n",
    "def summarise_backup_entries(entries, limit: int = 3):\n",
    "    if not entries:\n",
    "        return [\"> No entries stored in this backup.\"]\n",
    "    lines = []\n",
    "    for index, entry in enumerate(entries[:limit], start=1):\n",
    "        try:\n",
    "            key_bytes = decode_key(entry)\n",
    "            key_text = key_bytes.decode(\"utf-8\", errors=\"replace\") or \"<empty key>\"\n",
    "        except (KeyError, DumpDecodeError) as exc:\n",
    "            key_text = f\"<unable to decode key: {exc}>\"\n",
    "        preview, details = try_decode_value(entry)\n",
    "        entry_type = entry.get(\"type\", \"unknown\")\n",
    "        ttl = entry.get(\"pttl\")\n",
    "        ttl_text = f\"{ttl}\" if isinstance(ttl, int) else \"persistent\"\n",
    "        lines.append(f\"Entry {index}: key `{key_text}`\")\n",
    "        lines.append(f\"Type: `{entry_type}`; TTL (ms): `{ttl_text}`\")\n",
    "        decoded_bytes = details.get(\"decoded_bytes\")\n",
    "        error = details.get(\"decode_error\") or details.get(\"dump_error\")\n",
    "        if isinstance(decoded_bytes, (bytes, bytearray)):\n",
    "            text_value = decoded_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "            lines.append(shorten_text(text_value))\n",
    "        else:\n",
    "            lines.append(shorten_text(str(preview)))\n",
    "        if error:\n",
    "            lines.append(f\"Warning: {error}\")\n",
    "    if len(entries) > limit:\n",
    "        lines.append(f\"Additional entries not shown: {len(entries) - limit}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def build_backup_preview(data):\n",
    "    entries = data.get(\"entries\") or []\n",
    "    metadata = data.get(\"metadata\") or {}\n",
    "    return {\n",
    "        \"key_count\": metadata.get(\"key_count\", len(entries)),\n",
    "        \"created_at\": metadata.get(\"created_at_utc\"),\n",
    "        \"source\": metadata.get(\"source\") or {},\n",
    "        \"type_summary\": metadata.get(\"type_summary\") or {},\n",
    "        \"sample_entries\": summarise_backup_entries(entries),\n",
    "    }\n",
    "\n",
    "\n",
    "def try_render_backup_preview(relative_name: str, payload: bytes):\n",
    "    try:\n",
    "        text = payload.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    if not isinstance(data, dict):\n",
    "        return None\n",
    "    if \"entries\" not in data or \"metadata\" not in data:\n",
    "        return None\n",
    "    return build_backup_preview(data)\n",
    "\n",
    "\n",
    "def get_relative_member_name(info, prefix):\n",
    "    member_name = info.filename\n",
    "    if prefix and member_name.startswith(prefix):\n",
    "        return member_name[len(prefix):]\n",
    "    return member_name\n",
    "\n",
    "\n",
    "def is_logs_entry(relative_name):\n",
    "    normalized = relative_name.replace('\\\\', '/').lstrip('./')\n",
    "    return normalized == 'logs' or normalized.startswith('logs/')\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "85d9d26be28752f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:47:03.044278Z",
     "start_time": "2025-10-23T16:47:02.115607Z"
    }
   },
   "source": [
    "READ_JSON_LIMIT_BYTES = 5_000_000\n",
    "\n",
    "def collect_archive_data(zip_path):\n",
    "    meta = parse_zip_metadata(zip_path)\n",
    "    result = {\n",
    "        'zip_name': zip_path.name,\n",
    "        'zip_path': str(zip_path),\n",
    "        'metadata': meta,\n",
    "        'manifest': None,\n",
    "        'db_overview': [],\n",
    "        'members': [],\n",
    "        'backups': {},\n",
    "    }\n",
    "    with zipfile.ZipFile(zip_path) as archive:\n",
    "        try:\n",
    "            prefix, manifest = resolve_manifest(archive, zip_path)\n",
    "            result['manifest'] = manifest\n",
    "        except Exception:\n",
    "            prefix = detect_root_prefix(archive, zip_path)\n",
    "            manifest = None\n",
    "        if manifest:\n",
    "            files_map = manifest.get('files', {})\n",
    "            dbs = manifest.get('databases', [])\n",
    "            for db_index in dbs:\n",
    "                file_name = files_map.get(str(db_index))\n",
    "                if not file_name:\n",
    "                    continue\n",
    "                archive_name = f\"{prefix}{file_name}\"\n",
    "                try:\n",
    "                    size = archive.getinfo(archive_name).file_size\n",
    "                except KeyError:\n",
    "                    size = None\n",
    "                result['db_overview'].append({\n",
    "                    'db_index': db_index,\n",
    "                    'label': DB_LABELS.get(db_index, 'Unknown'),\n",
    "                    'json_file': file_name,\n",
    "                    'size_bytes': size,\n",
    "                    'size_text': format_bytes(size) if size is not None else None,\n",
    "                })\n",
    "        members = sorted((info for info in archive.infolist() if not info.is_dir()), key=lambda info: info.filename)\n",
    "        for info in members:\n",
    "            relative = get_relative_member_name(info, prefix)\n",
    "            if is_logs_entry(relative):\n",
    "                continue\n",
    "            size = info.file_size\n",
    "            entry = {\n",
    "                'relative_name': relative,\n",
    "                'size_bytes': size,\n",
    "                'size_text': format_bytes(size),\n",
    "                'json_data': None,\n",
    "                'json_truncated': False,\n",
    "                'text_preview': None,\n",
    "                'backup_preview': None,\n",
    "            }\n",
    "            read_entire = size <= MAX_FULL_BYTES or relative.endswith('.json')\n",
    "            with archive.open(info.filename) as handle:\n",
    "                payload = handle.read() if read_entire else handle.read(MAX_PREVIEW_BYTES)\n",
    "            if relative.endswith('.json') and (size is None or size <= READ_JSON_LIMIT_BYTES):\n",
    "                try:\n",
    "                    text = payload.decode('utf-8')\n",
    "                    data = json.loads(text)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "                if data is not None:\n",
    "                    entry['json_data'] = data\n",
    "                    preview = try_render_backup_preview(relative, payload)\n",
    "                    if preview is not None:\n",
    "                        entry['backup_preview'] = preview\n",
    "                        result['backups'][relative] = data\n",
    "                else:\n",
    "                    entry['text_preview'] = payload.decode('utf-8', errors='replace')[:1000]\n",
    "            else:\n",
    "                entry['json_truncated'] = relative.endswith('.json') and (size is not None and size > READ_JSON_LIMIT_BYTES)\n",
    "                try:\n",
    "                    entry['text_preview'] = payload.decode('utf-8', errors='replace')[:1000]\n",
    "                except Exception:\n",
    "                    entry['text_preview'] = None\n",
    "            result['members'].append(entry)\n",
    "    return result\n",
    "\n",
    "archives_metadata = [parse_zip_metadata(path) for path in zip_paths]\n",
    "archives_data = [collect_archive_data(path) for path in zip_paths]\n",
    "manifests_by_archive = {item['zip_name']: item['manifest'] for item in archives_data}\n",
    "backups_by_archive = {item['zip_name']: item['backups'] for item in archives_data}\n"
   ],
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
