{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55041dcae764f7ce",
   "metadata": {},
   "source": [
    "# Results archive inspection\n",
    "\n",
    "This notebook automatically inspects every `.zip` file stored in the `results` directory.\n",
    "It parses the filename of each archive to extract useful metadata, relies on the included\n",
    "`manifest.json` file to map Redis database dumps to their logical meaning, and previews\n",
    "all extracted files directly below. Large files are truncated to the first bytes so the\n",
    "notebook stays responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49976e1c760312e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:52:14.692624Z",
     "start_time": "2025-10-23T16:52:14.687601Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "import base64\n",
    "import binascii\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "HOST_BASE = 9\n",
    "DB_LABELS = {\n",
    "    0: \"DATA\",\n",
    "    1: \"CAN\",\n",
    "    2: \"R\",\n",
    "    3: \"NR\",\n",
    "    4: \"CAR\",\n",
    "    5: \"AR\",\n",
    "    6: \"GP\",\n",
    "    7: \"BP\",\n",
    "    8: \"PR\",\n",
    "    9: \"AP\",\n",
    "    10: \"LOGS\",\n",
    "}\n",
    "MAX_FULL_BYTES = 200_000\n",
    "MAX_PREVIEW_BYTES = 10_000\n",
    "\n",
    "zip_paths = sorted(RESULTS_DIR.glob(\"*.zip\"))\n",
    "zip_names = [path.name for path in zip_paths]\n",
    "zip_inventory = {\n",
    "    \"results_dir\": str(RESULTS_DIR),\n",
    "    \"count\": len(zip_paths),\n",
    "    \"found\": bool(zip_paths),\n",
    "    \"paths\": [str(path) for path in zip_paths],\n",
    "    \"names\": zip_names,\n",
    "}\n",
    "\n",
    "env_selected_index = os.environ.get(\"RESULTS_SELECTED_ZIP_INDEX\")\n",
    "env_selected_zip = os.environ.get(\"RESULTS_SELECTED_ZIP\")\n",
    "selected_zip_index = None\n",
    "selected_zip_name = None\n",
    "\n",
    "if env_selected_index is not None:\n",
    "    try:\n",
    "        candidate_index = int(env_selected_index)\n",
    "    except ValueError:\n",
    "        candidate_index = None\n",
    "    if isinstance(candidate_index, int) and 0 <= candidate_index < len(zip_paths):\n",
    "        selected_zip_index = candidate_index\n",
    "        selected_zip_name = zip_names[selected_zip_index]\n",
    "if selected_zip_name is None and env_selected_zip in zip_names:\n",
    "    selected_zip_name = env_selected_zip\n",
    "    selected_zip_index = zip_names.index(selected_zip_name)\n",
    "if selected_zip_name is None and zip_names:\n",
    "    selected_zip_index = 0\n",
    "    selected_zip_name = zip_names[0]\n",
    "\n",
    "selected_zip_path = zip_paths[selected_zip_index] if selected_zip_index is not None else None\n",
    "\n",
    "if zip_names:\n",
    "    print(\"Available ZIP archives:\")\n",
    "    for index, name in enumerate(zip_names):\n",
    "        print(f\"[{index}] {name}\")\n",
    "    user_choice = input(\"Select ZIP by index or name (press Enter to keep current selection): \").strip()\n",
    "    if user_choice:\n",
    "        resolved_index = None\n",
    "        try:\n",
    "            resolved_index = int(user_choice)\n",
    "        except ValueError:\n",
    "            resolved_index = None\n",
    "        if resolved_index is not None and 0 <= resolved_index < len(zip_paths):\n",
    "            selected_zip_index = resolved_index\n",
    "            selected_zip_name = zip_names[selected_zip_index]\n",
    "        elif user_choice in zip_names:\n",
    "            selected_zip_name = user_choice\n",
    "            selected_zip_index = zip_names.index(selected_zip_name)\n",
    "        else:\n",
    "            print(\"Invalid selection, keeping previous choice.\")\n",
    "    selected_zip_path = zip_paths[selected_zip_index] if selected_zip_index is not None else None\n",
    "    if selected_zip_name is not None and selected_zip_path is not None:\n",
    "        print(f\"Current selection: [{selected_zip_index}] {selected_zip_name}\")\n",
    "    else:\n",
    "        print(\"Current selection: none\")\n",
    "else:\n",
    "    print(\"No ZIP archives found in results directory.\")\n",
    "\n",
    "zip_inventory[\"selection\"] = {\n",
    "    \"name\": selected_zip_name,\n",
    "    \"index\": selected_zip_index,\n",
    "    \"path\": str(selected_zip_path) if selected_zip_path else None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb975d1e10ba092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:52:14.750483Z",
     "start_time": "2025-10-23T16:52:14.726038Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_bytes(size):\n",
    "    \"\"\"Return a human-readable representation of a file size.\"\"\"\n",
    "    if size is None:\n",
    "        return \"-\"\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    value = float(size)\n",
    "    for unit in units:\n",
    "        if value < 1024 or unit == units[-1]:\n",
    "            if unit == \"B\":\n",
    "                return f\"{int(value)} {unit}\"\n",
    "            return f\"{value:.2f} {unit}\"\n",
    "        value /= 1024\n",
    "    return f\"{value:.2f} B\"\n",
    "\n",
    "\n",
    "def parse_zip_metadata(zip_path):\n",
    "    \"\"\"Extract dataset, class, completion flag, and host numbers from the archive name.\"\"\"\n",
    "    dataset, class_name, completion_flag, host_fragment = zip_path.stem.rsplit(\"_\", 3)\n",
    "    try:\n",
    "        host_offset = int(host_fragment)\n",
    "        host_id = host_offset + HOST_BASE\n",
    "    except ValueError:\n",
    "        host_offset = None\n",
    "        host_id = None\n",
    "    flag_lower = completion_flag.lower()\n",
    "    if flag_lower in {\"true\", \"false\"}:\n",
    "        is_completed = flag_lower == \"true\"\n",
    "    else:\n",
    "        is_completed = None\n",
    "    size_bytes = zip_path.stat().st_size\n",
    "    return {\n",
    "        \"zip_path\": str(zip_path),\n",
    "        \"zip_name\": zip_path.name,\n",
    "        \"dataset\": dataset,\n",
    "        \"class\": class_name,\n",
    "        \"completion_raw\": completion_flag,\n",
    "        \"is_completed\": is_completed,\n",
    "        \"size_bytes\": size_bytes,\n",
    "        \"size_text\": format_bytes(size_bytes),\n",
    "        \"host_offset\": host_offset,\n",
    "        \"host_id\": host_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_root_prefix(archive, zip_path):\n",
    "    \"\"\"Guess the common directory prefix used inside the archive.\"\"\"\n",
    "    stem_prefix = f\"{zip_path.stem}/\"\n",
    "    has_stem = any(\n",
    "        info.filename.startswith(stem_prefix)\n",
    "        for info in archive.infolist()\n",
    "        if not info.is_dir()\n",
    "    )\n",
    "    if has_stem:\n",
    "        return stem_prefix\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def resolve_manifest(archive, zip_path):\n",
    "    \"\"\"Return the manifest data together with the prefix used inside the archive.\"\"\"\n",
    "    candidates = []\n",
    "    stem_prefix = f\"{zip_path.stem}/\"\n",
    "    candidates.append(stem_prefix)\n",
    "    for info in archive.infolist():\n",
    "        if info.is_dir():\n",
    "            dirname = info.filename\n",
    "            if dirname.startswith(\"__MACOSX/\"):\n",
    "                continue\n",
    "            if not dirname.endswith(\"/\"):\n",
    "                dirname += \"/\"\n",
    "            candidates.append(dirname)\n",
    "    candidates.append(\"\")\n",
    "    seen = set()\n",
    "    for prefix in candidates:\n",
    "        if prefix in seen:\n",
    "            continue\n",
    "        seen.add(prefix)\n",
    "        manifest_path = f\"{prefix}manifest.json\"\n",
    "        try:\n",
    "            with archive.open(manifest_path) as manifest_file:\n",
    "                manifest = json.load(manifest_file)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        else:\n",
    "            return prefix, manifest\n",
    "    raise KeyError(\"manifest.json not found\")\n",
    "\n",
    "\n",
    "class DumpDecodeError(RuntimeError):\n",
    "    \"\"\"Generic error raised while decoding a Redis DUMP payload.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DumpSections:\n",
    "    payload: bytes\n",
    "    version: int\n",
    "    checksum: bytes\n",
    "\n",
    "\n",
    "class _LengthEncoding:\n",
    "    __slots__ = (\"value\", \"encoding\")\n",
    "\n",
    "    def __init__(self, value=None, encoding=None):\n",
    "        self.value = value\n",
    "        self.encoding = encoding\n",
    "\n",
    "\n",
    "RDB_ENCODING_INT8 = 0\n",
    "RDB_ENCODING_INT16 = 1\n",
    "RDB_ENCODING_INT32 = 2\n",
    "RDB_ENCODING_LZF = 3\n",
    "\n",
    "\n",
    "def split_dump_sections(raw: bytes) -> DumpSections:\n",
    "    \"\"\"Split payload, RDB version, and checksum from a Redis dump.\"\"\"\n",
    "    if len(raw) < 10:\n",
    "        raise DumpDecodeError(\"DUMP payload is too short to contain metadata\")\n",
    "    checksum = raw[-8:]\n",
    "    version_bytes = raw[-10:-8]\n",
    "    version = int.from_bytes(version_bytes, \"little\", signed=False)\n",
    "    payload = raw[:-10]\n",
    "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
    "\n",
    "\n",
    "def _read_length_info(buffer: bytes, offset: int):\n",
    "    if offset >= len(buffer):\n",
    "        raise DumpDecodeError(\"Offset out of range while reading length\")\n",
    "    first = buffer[offset]\n",
    "    prefix = first >> 6\n",
    "    if prefix == 0:\n",
    "        length = first & 0x3F\n",
    "        return _LengthEncoding(length), offset + 1\n",
    "    if prefix == 1:\n",
    "        if offset + 1 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 14-bit encoded length\")\n",
    "        second = buffer[offset + 1]\n",
    "        length = ((first & 0x3F) << 8) | second\n",
    "        return _LengthEncoding(length), offset + 2\n",
    "    if prefix == 2:\n",
    "        if offset + 4 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded length\")\n",
    "        length = int.from_bytes(buffer[offset + 1 : offset + 5], \"big\", signed=False)\n",
    "        return _LengthEncoding(length), offset + 5\n",
    "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
    "\n",
    "\n",
    "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
    "    \"\"\"Minimal implementation of the LZF decompression used by Redis.\"\"\"\n",
    "    output = bytearray()\n",
    "    idx = 0\n",
    "    data_len = len(data)\n",
    "    while idx < data_len:\n",
    "        ctrl = data[idx]\n",
    "        idx += 1\n",
    "        if ctrl < 32:\n",
    "            literal_len = ctrl + 1\n",
    "            if idx + literal_len > data_len:\n",
    "                raise DumpDecodeError(\"Truncated literal LZF sequence\")\n",
    "            output.extend(data[idx : idx + literal_len])\n",
    "            idx += literal_len\n",
    "        else:\n",
    "            length = ctrl >> 5\n",
    "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
    "            if length == 7:\n",
    "                if idx >= data_len:\n",
    "                    raise DumpDecodeError(\"Truncated LZF sequence while extending length\")\n",
    "                length += data[idx]\n",
    "                idx += 1\n",
    "            if idx >= data_len:\n",
    "                raise DumpDecodeError(\"Truncated LZF sequence while resolving reference\")\n",
    "            ref_offset -= data[idx]\n",
    "            idx += 1\n",
    "            length += 2\n",
    "            if ref_offset < 0:\n",
    "                raise DumpDecodeError(\"Negative LZF reference\")\n",
    "            for _ in range(length):\n",
    "                if ref_offset >= len(output):\n",
    "                    raise DumpDecodeError(\"LZF reference out of range\")\n",
    "                output.append(output[ref_offset])\n",
    "                ref_offset += 1\n",
    "    if len(output) != expected_length:\n",
    "        raise DumpDecodeError(\n",
    "            f\"Unexpected decompressed length: expected {expected_length}, got {len(output)}\"\n",
    "        )\n",
    "    return bytes(output)\n",
    "\n",
    "\n",
    "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int):\n",
    "    if encoding == RDB_ENCODING_INT8:\n",
    "        if offset >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 8-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 1], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 1\n",
    "    if encoding == RDB_ENCODING_INT16:\n",
    "        if offset + 2 > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 16-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 2], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 2\n",
    "    if encoding == RDB_ENCODING_INT32:\n",
    "        if offset + 4 > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 4], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 4\n",
    "    if encoding == RDB_ENCODING_LZF:\n",
    "        compressed_len_info, next_offset = _read_length_info(buffer, offset)\n",
    "        data_len_info, data_offset = _read_length_info(buffer, next_offset)\n",
    "        if compressed_len_info.value is None or data_len_info.value is None:\n",
    "            raise DumpDecodeError(\"Invalid LZF length encoding\")\n",
    "        end = data_offset + compressed_len_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        compressed = buffer[data_offset:end]\n",
    "        decompressed = lzf_decompress(compressed, data_len_info.value)\n",
    "        return decompressed, end\n",
    "    raise DumpDecodeError(\"Unknown string encoding\")\n",
    "\n",
    "\n",
    "def _read_encoded_string(buffer: bytes, offset: int):\n",
    "    length_info, next_offset = _read_length_info(buffer, offset)\n",
    "    if length_info.encoding is None:\n",
    "        end = next_offset + length_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        return buffer[next_offset:end], end\n",
    "    return _decode_special_encoding(buffer, next_offset, length_info.encoding)\n",
    "\n",
    "\n",
    "def decode_string_from_dump(raw: bytes) -> bytes:\n",
    "    sections = split_dump_sections(raw)\n",
    "    payload = sections.payload\n",
    "    if not payload:\n",
    "        raise DumpDecodeError(\"Empty payload\")\n",
    "    object_type = payload[0]\n",
    "    if object_type != 0:\n",
    "        raise DumpDecodeError(f\"Non-string object type: {object_type}\")\n",
    "    value, _ = _read_encoded_string(payload, 1)\n",
    "    return value\n",
    "\n",
    "\n",
    "def decode_bytes(value: str) -> bytes:\n",
    "    if not isinstance(value, str):\n",
    "        raise DumpDecodeError(\"Encoded value must be a string\")\n",
    "    try:\n",
    "        return base64.b64decode(value.encode(\"ascii\"))\n",
    "    except (UnicodeEncodeError, binascii.Error) as exc:\n",
    "        raise DumpDecodeError(f\"Invalid base64 payload: {exc}\") from exc\n",
    "\n",
    "\n",
    "def decode_key(entry):\n",
    "    return decode_bytes(entry[\"key\"])\n",
    "\n",
    "\n",
    "def text_preview(value: bytes, limit: int = 120) -> str:\n",
    "    text = value.decode(\"utf-8\", errors=\"replace\")\n",
    "    if len(text) > limit:\n",
    "        return text[: limit - 1] + \".\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def try_decode_value(entry):\n",
    "    value_info = dict(entry.get(\"value\") or {})\n",
    "    data_b64 = value_info.get(\"data\")\n",
    "    if not data_b64:\n",
    "        return \"<no value>\", value_info\n",
    "    try:\n",
    "        raw = decode_bytes(data_b64)\n",
    "    except DumpDecodeError as exc:\n",
    "        value_info[\"decode_error\"] = str(exc)\n",
    "        return \"<invalid base64>\", value_info\n",
    "    details = {\n",
    "        \"dump_size\": len(raw),\n",
    "    }\n",
    "    try:\n",
    "        sections = split_dump_sections(raw)\n",
    "        details[\"rdb_version\"] = sections.version\n",
    "        details[\"checksum\"] = sections.checksum.hex()\n",
    "    except DumpDecodeError as exc:\n",
    "        details[\"dump_error\"] = str(exc)\n",
    "        return \"<invalid dump>\", details\n",
    "    if entry.get(\"type\") == \"string\":\n",
    "        try:\n",
    "            decoded = decode_string_from_dump(raw)\n",
    "        except DumpDecodeError as exc:\n",
    "            details[\"decode_error\"] = str(exc)\n",
    "            return \"<string not decoded>\", details\n",
    "        details[\"decoded_bytes\"] = decoded\n",
    "        preview = text_preview(decoded)\n",
    "        return preview, details\n",
    "    return f\"<{entry.get('type')} - {len(sections.payload)} bytes>\", details\n",
    "\n",
    "\n",
    "def shorten_text(text: str, limit: int = 600) -> str:\n",
    "    sanitized = text.replace(\"````\", \"``` `\")\n",
    "    if len(sanitized) > limit:\n",
    "        return sanitized[: limit - 1] + \".\"\n",
    "    return sanitized\n",
    "\n",
    "\n",
    "def summarise_backup_entries(entries, limit: int = 3):\n",
    "    if not entries:\n",
    "        return [\"> No entries stored in this backup.\"]\n",
    "    lines = []\n",
    "    for index, entry in enumerate(entries[:limit], start=1):\n",
    "        try:\n",
    "            key_bytes = decode_key(entry)\n",
    "            key_text = key_bytes.decode(\"utf-8\", errors=\"replace\") or \"<empty key>\"\n",
    "        except (KeyError, DumpDecodeError) as exc:\n",
    "            key_text = f\"<unable to decode key: {exc}>\"\n",
    "        preview, details = try_decode_value(entry)\n",
    "        entry_type = entry.get(\"type\", \"unknown\")\n",
    "        ttl = entry.get(\"pttl\")\n",
    "        ttl_text = f\"{ttl}\" if isinstance(ttl, int) else \"persistent\"\n",
    "        lines.append(f\"Entry {index}: key `{key_text}`\")\n",
    "        lines.append(f\"Type: `{entry_type}`; TTL (ms): `{ttl_text}`\")\n",
    "        decoded_bytes = details.get(\"decoded_bytes\")\n",
    "        error = details.get(\"decode_error\") or details.get(\"dump_error\")\n",
    "        if isinstance(decoded_bytes, (bytes, bytearray)):\n",
    "            text_value = decoded_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "            lines.append(shorten_text(text_value))\n",
    "        else:\n",
    "            lines.append(shorten_text(str(preview)))\n",
    "        if error:\n",
    "            lines.append(f\"Warning: {error}\")\n",
    "    if len(entries) > limit:\n",
    "        lines.append(f\"Additional entries not shown: {len(entries) - limit}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def build_backup_preview(data):\n",
    "    entries = data.get(\"entries\") or []\n",
    "    metadata = data.get(\"metadata\") or {}\n",
    "    return {\n",
    "        \"key_count\": metadata.get(\"key_count\", len(entries)),\n",
    "        \"created_at\": metadata.get(\"created_at_utc\"),\n",
    "        \"source\": metadata.get(\"source\") or {},\n",
    "        \"type_summary\": metadata.get(\"type_summary\") or {},\n",
    "        \"sample_entries\": summarise_backup_entries(entries),\n",
    "    }\n",
    "\n",
    "\n",
    "def try_render_backup_preview(relative_name: str, payload: bytes):\n",
    "    try:\n",
    "        text = payload.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    if not isinstance(data, dict):\n",
    "        return None\n",
    "    if \"entries\" not in data or \"metadata\" not in data:\n",
    "        return None\n",
    "    return build_backup_preview(data)\n",
    "\n",
    "\n",
    "def get_relative_member_name(info, prefix):\n",
    "    member_name = info.filename\n",
    "    if prefix and member_name.startswith(prefix):\n",
    "        return member_name[len(prefix):]\n",
    "    return member_name\n",
    "\n",
    "\n",
    "def is_logs_entry(relative_name):\n",
    "    normalized = relative_name.replace('\\\\', '/').lstrip('./')\n",
    "    return normalized == 'logs' or normalized.startswith('logs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9d26be28752f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:52:15.624863Z",
     "start_time": "2025-10-23T16:52:14.759766Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "READ_JSON_LIMIT_BYTES = 5_000_000\n",
    "\n",
    "def collect_archive_data(zip_path):\n",
    "    meta = parse_zip_metadata(zip_path)\n",
    "    result = {\n",
    "        'zip_name': zip_path.name,\n",
    "        'zip_path': str(zip_path),\n",
    "        'metadata': meta,\n",
    "        'manifest': None,\n",
    "        'manifest_prefix': '',\n",
    "        'db_overview': [],\n",
    "        'members': [],\n",
    "        'backups': {},\n",
    "    }\n",
    "    with zipfile.ZipFile(zip_path) as archive:\n",
    "        try:\n",
    "            prefix, manifest = resolve_manifest(archive, zip_path)\n",
    "            result['manifest'] = manifest\n",
    "            result['manifest_prefix'] = prefix\n",
    "        except Exception:\n",
    "            prefix = detect_root_prefix(archive, zip_path)\n",
    "            manifest = None\n",
    "            result['manifest_prefix'] = prefix\n",
    "        if manifest:\n",
    "            files_map = manifest.get('files', {})\n",
    "            dbs = manifest.get('databases', [])\n",
    "            for db_index in dbs:\n",
    "                file_name = files_map.get(str(db_index))\n",
    "                if not file_name:\n",
    "                    continue\n",
    "                archive_name = f\"{prefix}{file_name}\"\n",
    "                try:\n",
    "                    size = archive.getinfo(archive_name).file_size\n",
    "                except KeyError:\n",
    "                    size = None\n",
    "                result['db_overview'].append({\n",
    "                    'db_index': db_index,\n",
    "                    'label': DB_LABELS.get(db_index, 'Unknown'),\n",
    "                    'json_file': file_name,\n",
    "                    'size_bytes': size,\n",
    "                    'size_text': format_bytes(size) if size is not None else None,\n",
    "                })\n",
    "        members = sorted((info for info in archive.infolist() if not info.is_dir()), key=lambda info: info.filename)\n",
    "        for info in members:\n",
    "            relative = get_relative_member_name(info, prefix)\n",
    "            if is_logs_entry(relative):\n",
    "                continue\n",
    "            size = info.file_size\n",
    "            entry = {\n",
    "                'relative_name': relative,\n",
    "                'size_bytes': size,\n",
    "                'size_text': format_bytes(size),\n",
    "                'json_data': None,\n",
    "                'json_truncated': False,\n",
    "                'text_preview': None,\n",
    "                'backup_preview': None,\n",
    "            }\n",
    "            read_entire = size <= MAX_FULL_BYTES or relative.endswith('.json')\n",
    "            with archive.open(info.filename) as handle:\n",
    "                payload = handle.read() if read_entire else handle.read(MAX_PREVIEW_BYTES)\n",
    "            if relative.endswith('.json') and (size is None or size <= READ_JSON_LIMIT_BYTES):\n",
    "                try:\n",
    "                    text = payload.decode('utf-8')\n",
    "                    data = json.loads(text)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "                if data is not None:\n",
    "                    entry['json_data'] = data\n",
    "                    preview = try_render_backup_preview(relative, payload)\n",
    "                    if preview is not None:\n",
    "                        entry['backup_preview'] = preview\n",
    "                        result['backups'][relative] = data\n",
    "                else:\n",
    "                    entry['text_preview'] = payload.decode('utf-8', errors='replace')[:1000]\n",
    "            else:\n",
    "                entry['json_truncated'] = relative.endswith('.json') and (size is not None and size > READ_JSON_LIMIT_BYTES)\n",
    "                try:\n",
    "                    entry['text_preview'] = payload.decode('utf-8', errors='replace')[:1000]\n",
    "                except Exception:\n",
    "                    entry['text_preview'] = None\n",
    "            result['members'].append(entry)\n",
    "    return result\n",
    "\n",
    "def _format_number(value: float) -> str:\n",
    "    if math.isinf(value):\n",
    "        return '\u221e' if value > 0 else '-\u221e'\n",
    "    if math.isnan(value):\n",
    "        return 'NaN'\n",
    "    if abs(value) >= 1_000:\n",
    "        return f\"{value:.3g}\"\n",
    "    return f\"{value:.6g}\"\n",
    "\n",
    "def _short_text(text: str, limit: int = 120) -> str:\n",
    "    return text if len(text) <= limit else text[: limit - 1] + '\u2026'\n",
    "\n",
    "def _inline_summary(value, depth: int = 0) -> str:\n",
    "    if isinstance(value, dict):\n",
    "        return f\"object({len(value)})\"\n",
    "    if isinstance(value, list):\n",
    "        return f\"array({len(value)})\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        return _format_number(value)\n",
    "    if isinstance(value, str):\n",
    "        return repr(_short_text(value))\n",
    "    if value is None:\n",
    "        return 'null'\n",
    "    return repr(value)\n",
    "\n",
    "def _extract_metadata_from_chunk(text: str):\n",
    "    key = '\"metadata\"'\n",
    "    idx = text.find(key)\n",
    "    if idx == -1:\n",
    "        return {}\n",
    "    brace_start = text.find('{', idx)\n",
    "    if brace_start == -1:\n",
    "        return {}\n",
    "    depth = 0\n",
    "    for pos in range(brace_start, len(text)):\n",
    "        char = text[pos]\n",
    "        if char == '{':\n",
    "            depth += 1\n",
    "        elif char == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                snippet = text[brace_start : pos + 1]\n",
    "                try:\n",
    "                    return json.loads(snippet)\n",
    "                except json.JSONDecodeError:\n",
    "                    return {}\n",
    "    return {}\n",
    "\n",
    "def _fetch_backup_metadata(zip_path, member_name, size_limit=2_000_000):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path) as archive:\n",
    "            with archive.open(member_name) as handle:\n",
    "                chunk = handle.read(size_limit)\n",
    "    except KeyError:\n",
    "        return {}\n",
    "    text = chunk.decode('utf-8', errors='replace')\n",
    "    return _extract_metadata_from_chunk(text)\n",
    "\n",
    "def _describe_json(value, depth: int = 0):\n",
    "    if isinstance(value, dict):\n",
    "        keys = list(value.keys())\n",
    "        preview_keys = keys[:5]\n",
    "        headline = f\"object with {len(keys)} keys: {', '.join(preview_keys)}\" + (\"\u2026\" if len(keys) > 5 else '')\n",
    "        detail_lines = []\n",
    "        for key in keys[:3]:\n",
    "            detail_lines.append(f\"{key}: {_inline_summary(value[key], depth + 1)}\")\n",
    "        if len(keys) > 3:\n",
    "            detail_lines.append('\u2026')\n",
    "        return headline, detail_lines\n",
    "    if isinstance(value, list):\n",
    "        length = len(value)\n",
    "        headline = f\"array with {length} items\"\n",
    "        sample = [_inline_summary(item, depth + 1) for item in value[:3]]\n",
    "        detail_lines = []\n",
    "        if sample:\n",
    "            detail = ', '.join(sample)\n",
    "            detail_lines.append(f\"sample: {detail}{'\u2026' if length > 3 else ''}\")\n",
    "        return headline, detail_lines\n",
    "    return _inline_summary(value, depth), []\n",
    "\n",
    "def summarise_endpoint_univers(series_map):\n",
    "    if not isinstance(series_map, dict):\n",
    "        return 'time series (unexpected structure)', []\n",
    "    feature_names = sorted(series_map.keys())\n",
    "    lengths = []\n",
    "    details = []\n",
    "    for name in feature_names[:5]:\n",
    "        points = series_map.get(name)\n",
    "        if isinstance(points, list):\n",
    "            length = len(points)\n",
    "            lengths.append(length)\n",
    "            sample = []\n",
    "            for value in points[:3]:\n",
    "                if isinstance(value, (int, float)):\n",
    "                    sample.append(_format_number(value))\n",
    "                else:\n",
    "                    sample.append(str(value))\n",
    "            preview = ', '.join(sample)\n",
    "            details.append(f\"{name}: len={length}, sample=[{preview}{'\u2026' if len(points) > 3 else ''}]\")\n",
    "        else:\n",
    "            details.append(f\"{name}: unexpected {type(points).__name__}\")\n",
    "    if len(series_map) > 5:\n",
    "        details.append('\u2026')\n",
    "    if lengths:\n",
    "        distinct_lengths = sorted(set(lengths))\n",
    "        if len(distinct_lengths) == 1:\n",
    "            headline = f\"time series (Endpoint Univers) with {len(series_map)} features; length {distinct_lengths[0]}\"\n",
    "        else:\n",
    "            headline = (\n",
    "                f\"time series (Endpoint Univers) with {len(series_map)} features; \"\n",
    "                + f\"lengths {', '.join(str(l) for l in distinct_lengths)}\"\n",
    "            )\n",
    "    else:\n",
    "        headline = f\"time series (Endpoint Univers) with {len(series_map)} features\"\n",
    "    return headline, details\n",
    "\n",
    "def summarise_random_forest(trees):\n",
    "    if not isinstance(trees, list):\n",
    "        return 'random forest (unexpected structure)', []\n",
    "    count = len(trees)\n",
    "    details = []\n",
    "    for tree in trees[:5]:\n",
    "        if isinstance(tree, dict):\n",
    "            tree_id = tree.get('tree_id')\n",
    "            feature = tree.get('feature')\n",
    "            value = tree.get('value')\n",
    "            feature_text = feature if feature is not None else '?'\n",
    "            threshold = _format_number(value) if isinstance(value, (int, float)) else str(value)\n",
    "            prefix = f\"tree {tree_id}\" if tree_id is not None else 'tree'\n",
    "            details.append(f\"{prefix}: root feature {feature_text}, threshold {threshold}\")\n",
    "        else:\n",
    "            details.append(f\"tree: unexpected {type(tree).__name__}\")\n",
    "    if count > 5:\n",
    "        details.append('\u2026')\n",
    "    return f\"random forest with {count} trees\", details\n",
    "\n",
    "def summarise_rf_optimization(result):\n",
    "    if not isinstance(result, dict):\n",
    "        return 'RF optimisation summary (unexpected structure)', []\n",
    "    best = result.get('best_params') or {}\n",
    "    best_keys = list(best.keys())\n",
    "    headline_parts = []\n",
    "    cv_score = result.get('best_cv_score')\n",
    "    test_score = result.get('test_score')\n",
    "    if isinstance(cv_score, (int, float)):\n",
    "        headline_parts.append(f\"best CV {cv_score:.3f}\")\n",
    "    if isinstance(test_score, (int, float)):\n",
    "        headline_parts.append(f\"test {test_score:.3f}\")\n",
    "    iter_count = result.get('n_iter')\n",
    "    if isinstance(iter_count, int):\n",
    "        headline_parts.append(f\"n_iter {iter_count}\")\n",
    "    headline = \"RF optimisation results\"\n",
    "    if headline_parts:\n",
    "        headline += \" (\" + ', '.join(headline_parts) + \")\"\n",
    "    details = []\n",
    "    for param in best_keys[:5]:\n",
    "        details.append(f\"best_params.{param} = {best[param]}\")\n",
    "    if len(best_keys) > 5:\n",
    "        details.append('\u2026')\n",
    "    used_test = result.get('used_test_for_validation')\n",
    "    if isinstance(used_test, bool):\n",
    "        details.append(f\"used_test_for_validation: {used_test}\")\n",
    "    timestamp = result.get('timestamp')\n",
    "    if timestamp:\n",
    "        details.append(f\"timestamp: {timestamp}\")\n",
    "    return headline, details\n",
    "\n",
    "def summarise_entry(key, value_json, value_text, skip_sample_keys=True):\n",
    "    if skip_sample_keys and key.startswith('sample_'):\n",
    "        return None, None\n",
    "    if key == 'EU' and value_json is not None:\n",
    "        return summarise_endpoint_univers(value_json)\n",
    "    if key == 'RF' and value_json is not None:\n",
    "        return summarise_random_forest(value_json)\n",
    "    if key == 'RF_OPTIMIZATION_RESULTS' and value_json is not None:\n",
    "        return summarise_rf_optimization(value_json)\n",
    "    if value_json is not None:\n",
    "        return _describe_json(value_json)\n",
    "    return _short_text(value_text), []\n",
    "\n",
    "def summarise_entry_generic(key, value_json, value_text):\n",
    "    if key == 'EU' and value_json is not None:\n",
    "        return summarise_endpoint_univers(value_json)\n",
    "    if key == 'RF' and value_json is not None:\n",
    "        return summarise_random_forest(value_json)\n",
    "    if key == 'RF_OPTIMIZATION_RESULTS' and value_json is not None:\n",
    "        return summarise_rf_optimization(value_json)\n",
    "    if value_json is not None:\n",
    "        return _describe_json(value_json)\n",
    "    return _short_text(value_text), []\n",
    "\n",
    "archives_metadata = [parse_zip_metadata(path) for path in zip_paths]\n",
    "archives_data = [collect_archive_data(path) for path in zip_paths]\n",
    "manifests_by_archive = {item['zip_name']: item['manifest'] for item in archives_data}\n",
    "manifest_prefix_by_archive = {item['zip_name']: item.get('manifest_prefix', '') for item in archives_data}\n",
    "backups_by_archive = {item['zip_name']: item['backups'] for item in archives_data}\n",
    "selected_archive_data = next((item for item in archives_data if item['zip_name'] == selected_zip_name), None)\n",
    "selected_manifest = manifests_by_archive.get(selected_zip_name)\n",
    "selected_manifest_prefix = manifest_prefix_by_archive.get(selected_zip_name, '')\n",
    "selected_backups = backups_by_archive.get(selected_zip_name)\n",
    "\n",
    "selected_db0_file_name = None\n",
    "if selected_manifest:\n",
    "    files_map = selected_manifest.get('files', {}) or {}\n",
    "    selected_db0_file_name = files_map.get('0')\n",
    "\n",
    "selected_db0_backup = None\n",
    "if selected_backups and selected_db0_file_name:\n",
    "    selected_db0_backup = selected_backups.get(selected_db0_file_name)\n",
    "\n",
    "selected_db0_entries = []\n",
    "selected_db0_values = []\n",
    "if selected_db0_backup:\n",
    "    selected_db0_entries = selected_db0_backup.get('entries') or []\n",
    "    for entry in selected_db0_entries:\n",
    "        try:\n",
    "            key_bytes = decode_key(entry)\n",
    "            key_text = key_bytes.decode('utf-8', errors='replace')\n",
    "        except Exception as exc:\n",
    "            key_text = f'<unable to decode key: {exc}>'\n",
    "        preview, details = try_decode_value(entry)\n",
    "        value_bytes = details.get('decoded_bytes') if isinstance(details, dict) else None\n",
    "        if isinstance(value_bytes, (bytes, bytearray)):\n",
    "            value_text = value_bytes.decode('utf-8', errors='replace')\n",
    "        else:\n",
    "            value_text = str(preview)\n",
    "        value_json = None\n",
    "        if isinstance(value_text, str):\n",
    "            try:\n",
    "                value_json = json.loads(value_text)\n",
    "            except Exception:\n",
    "                value_json = None\n",
    "        selected_db0_values.append({\n",
    "            'key': key_text,\n",
    "            'type': entry.get('type'),\n",
    "            'ttl_ms': entry.get('pttl'),\n",
    "            'value_text': value_text,\n",
    "            'value_bytes': value_bytes,\n",
    "            'value_json': value_json,\n",
    "            'details': details,\n",
    "        })\n",
    "\n",
    "selected_db0_values_by_key = {item['key']: item for item in selected_db0_values}\n",
    "\n",
    "db1_entries = []\n",
    "if selected_manifest and selected_backups is not None:\n",
    "    files_map = selected_manifest.get('files', {}) or {}\n",
    "    db1_file = files_map.get('1')\n",
    "    if db1_file:\n",
    "        data = selected_backups.get(db1_file) if isinstance(selected_backups, dict) else None\n",
    "        if isinstance(data, dict):\n",
    "            db1_entries = data.get('entries') or []\n",
    "\n",
    "db1_entries_summary = []\n",
    "for entry in db1_entries:\n",
    "    try:\n",
    "        key_bytes = decode_key(entry)\n",
    "        key_text = key_bytes.decode('utf-8', errors='replace')\n",
    "    except Exception as exc:\n",
    "        key_text = f'<unable to decode key: {exc}>'\n",
    "    preview, details = try_decode_value(entry)\n",
    "    value_bytes = details.get('decoded_bytes') if isinstance(details, dict) else None\n",
    "    if isinstance(value_bytes, (bytes, bytearray)):\n",
    "        value_text = value_bytes.decode('utf-8', errors='replace')\n",
    "    else:\n",
    "        value_text = str(preview)\n",
    "    value_json = None\n",
    "    if isinstance(value_text, str):\n",
    "        try:\n",
    "            value_json = json.loads(value_text)\n",
    "        except Exception:\n",
    "            value_json = None\n",
    "    headline, extra = summarise_entry_generic(key_text, value_json, value_text)\n",
    "    db1_entries_summary.append({\n",
    "        'key': key_text,\n",
    "        'type': entry.get('type'),\n",
    "        'ttl_ms': entry.get('pttl'),\n",
    "        'headline': headline,\n",
    "        'details': extra,\n",
    "    })\n",
    "\n",
    "other_db_summaries = []\n",
    "if selected_manifest and selected_backups is not None:\n",
    "    files_map = selected_manifest.get('files', {}) or {}\n",
    "    selected_zip_path = Path(selected_archive_data['zip_path']) if selected_archive_data else None\n",
    "    for db_index in range(1, 10):\n",
    "        file_name = files_map.get(str(db_index))\n",
    "        if not file_name or selected_zip_path is None:\n",
    "            continue\n",
    "        label = DB_LABELS.get(db_index, 'Unknown')\n",
    "        data = selected_backups.get(file_name) if isinstance(selected_backups, dict) else None\n",
    "        metadata = {}\n",
    "        if isinstance(data, dict):\n",
    "            metadata = data.get('metadata') or {}\n",
    "        if not metadata:\n",
    "            member_name = f\"{selected_manifest_prefix}{file_name}\"\n",
    "            metadata = _fetch_backup_metadata(selected_zip_path, member_name)\n",
    "        key_count = metadata.get('key_count') if isinstance(metadata, dict) else None\n",
    "        type_summary = metadata.get('type_summary') if isinstance(metadata, dict) else None\n",
    "        other_db_summaries.append({\n",
    "            'db_index': db_index,\n",
    "            'label': label,\n",
    "            'file_name': file_name,\n",
    "            'key_count': key_count,\n",
    "            'type_summary': type_summary if isinstance(type_summary, dict) else None,\n",
    "        })\n",
    "\n",
    "selected_db0_summary = []\n",
    "for item in selected_db0_values:\n",
    "    headline, extra = summarise_entry(item['key'], item['value_json'], item['value_text'], skip_sample_keys=True)\n",
    "    if headline is None and extra is None:\n",
    "        continue\n",
    "    selected_db0_summary.append({\n",
    "        'key': item['key'],\n",
    "        'type': item['type'],\n",
    "        'ttl_ms': item['ttl_ms'],\n",
    "        'headline': headline,\n",
    "        'details': extra,\n",
    "    })\n",
    "\n",
    "if selected_db0_summary:\n",
    "    print(f\"DB 0 entries for {selected_zip_name}:\")\n",
    "    for entry in selected_db0_summary:\n",
    "        ttl = entry['ttl_ms'] if isinstance(entry['ttl_ms'], int) else 'persistent'\n",
    "        print(f\"{entry['key']} (type={entry['type']}, ttl={ttl})\")\n",
    "        print(f\"{entry['headline']}\")\n",
    "        for detail in entry['details']:\n",
    "            print(f\"    {detail}\")\n",
    "else:\n",
    "    print('No DB 0 data available for the current selection.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "def summarise_db10_workers(selected_zip_name, *, show_summary=True):\n",
    "    global DB10_WORKER_CACHE\n",
    "    DB10_WORKER_CACHE = {}\n",
    "    if not selected_zip_name:\n",
    "        print(\"No archive selected.\")\n",
    "        return\n",
    "    files_map = (selected_manifest or {}).get('files', {}) or {}\n",
    "    db10_file = files_map.get('10')\n",
    "    if not db10_file:\n",
    "        print(\"No DB 10 available for the current selection.\")\n",
    "        return\n",
    "    backups = selected_backups if isinstance(selected_backups, dict) else {}\n",
    "    db10_data = backups.get(db10_file)\n",
    "    if not isinstance(db10_data, dict):\n",
    "        zip_path_str = selected_archive_data.get('zip_path') if selected_archive_data else None\n",
    "        if zip_path_str:\n",
    "            zip_path = Path(zip_path_str)\n",
    "            if zip_path.exists():\n",
    "                try:\n",
    "                    with zipfile.ZipFile(zip_path) as archive:\n",
    "                        payload = archive.read(f\"{selected_manifest_prefix}{db10_file}\")\n",
    "                    db10_data = json.loads(payload.decode('utf-8', errors='replace'))\n",
    "                except Exception:\n",
    "                    db10_data = None\n",
    "    if not isinstance(db10_data, dict):\n",
    "        print(\"Unable to load DB 10 data.\")\n",
    "        return\n",
    "    entries = db10_data.get('entries') or []\n",
    "    if not entries:\n",
    "        print(\"No entries available in DB 10.\")\n",
    "        DB10_WORKER_CACHE = {\n",
    "            'zip_name': selected_zip_name,\n",
    "            'db10_file': db10_file,\n",
    "            'entries': entries,\n",
    "            'db10_data': db10_data,\n",
    "            'worker_stats': {},\n",
    "            'worker_summaries': [],\n",
    "        }\n",
    "        return\n",
    "    worker_stats = {}\n",
    "    for entry in entries:\n",
    "        preview, details = try_decode_value(entry)\n",
    "        raw_bytes = details.get('decoded_bytes') if isinstance(details, dict) else None\n",
    "        if isinstance(raw_bytes, (bytes, bytearray)):\n",
    "            text = raw_bytes.decode('utf-8', errors='replace')\n",
    "        elif isinstance(preview, (bytes, bytearray)):\n",
    "            text = preview.decode('utf-8', errors='replace')\n",
    "        else:\n",
    "            text = str(preview)\n",
    "        try:\n",
    "            payload = json.loads(text)\n",
    "        except Exception:\n",
    "            continue\n",
    "        worker_id = payload.get('worker_id')\n",
    "        if not worker_id:\n",
    "            try:\n",
    "                key_text = decode_key(entry).decode('utf-8', errors='replace')\n",
    "            except Exception:\n",
    "                key_text = entry.get('key')\n",
    "            if isinstance(key_text, str) and ':' in key_text:\n",
    "                worker_id = key_text.rsplit(':', 1)[0]\n",
    "            else:\n",
    "                worker_id = str(key_text)\n",
    "        stats = worker_stats.setdefault(worker_id, {\n",
    "            'records': 0,\n",
    "            'iterations': [],\n",
    "            'queue_sizes': [],\n",
    "            'car_queue_sizes': [],\n",
    "            'total_seconds': [],\n",
    "            'car_seconds': [],\n",
    "            'can_seconds': [],\n",
    "            'car_results': defaultdict(int),\n",
    "            'can_results': defaultdict(int),\n",
    "            'outcomes': defaultdict(int),\n",
    "            'events': [],\n",
    "        })\n",
    "        stats['records'] += 1\n",
    "        iteration = payload.get('iteration')\n",
    "        if isinstance(iteration, (int, float)):\n",
    "            stats['iterations'].append(int(iteration))\n",
    "        queue_size = payload.get('queue_size')\n",
    "        if isinstance(queue_size, (int, float)):\n",
    "            stats['queue_sizes'].append(float(queue_size))\n",
    "        car_queue_size = payload.get('car_queue_size')\n",
    "        if isinstance(car_queue_size, (int, float)):\n",
    "            stats['car_queue_sizes'].append(float(car_queue_size))\n",
    "        timings = payload.get('timings') or {}\n",
    "        total_seconds = timings.get('total_seconds')\n",
    "        if isinstance(total_seconds, (int, float)):\n",
    "            stats['total_seconds'].append(float(total_seconds))\n",
    "        car_seconds = timings.get('car_seconds')\n",
    "        if isinstance(car_seconds, (int, float)):\n",
    "            stats['car_seconds'].append(float(car_seconds))\n",
    "        can_seconds = timings.get('can_seconds')\n",
    "        if isinstance(can_seconds, (int, float)):\n",
    "            stats['can_seconds'].append(float(can_seconds))\n",
    "        car_result = (payload.get('car_processing') or {}).get('result')\n",
    "        if car_result:\n",
    "            stats['car_results'][car_result] += 1\n",
    "        can_result = (payload.get('can_processing') or {}).get('result')\n",
    "        if can_result:\n",
    "            stats['can_results'][can_result] += 1\n",
    "        for outcome_key, outcome_value in (payload.get('outcomes') or {}).items():\n",
    "            if isinstance(outcome_value, bool):\n",
    "                label = f\"{outcome_key}={'T' if outcome_value else 'F'}\"\n",
    "                stats['outcomes'][label] += 1\n",
    "        stats['events'].append(payload)\n",
    "\n",
    "    def summarize(values):\n",
    "        if not values:\n",
    "            return (None, None, None)\n",
    "        return (min(values), statistics.mean(values), max(values))\n",
    "\n",
    "    def format_number(value, digits=1):\n",
    "        return f\"{value:.{digits}f}\" if isinstance(value, (int, float)) else '-'\n",
    "\n",
    "    def format_hours(values):\n",
    "        total = sum(values)\n",
    "        return total / 3600 if total else 0.0\n",
    "\n",
    "    def format_counter(mapping):\n",
    "        if not mapping:\n",
    "            return '-'\n",
    "        items = sorted(dict(mapping).items())\n",
    "        return ', '.join(f\"{key}:{value}\" for key, value in items)\n",
    "\n",
    "    workers = []\n",
    "    for worker_id, stats in worker_stats.items():\n",
    "        queue_min, queue_avg, queue_max = summarize(stats['queue_sizes'])\n",
    "        car_min, car_avg, car_max = summarize(stats['car_queue_sizes'])\n",
    "        workers.append({\n",
    "            'worker_id': worker_id,\n",
    "            'records': stats['records'],\n",
    "            'iter_min': min(stats['iterations']) if stats['iterations'] else None,\n",
    "            'iter_max': max(stats['iterations']) if stats['iterations'] else None,\n",
    "            'queue_avg': queue_avg,\n",
    "            'car_queue_avg': car_avg,\n",
    "            'queue_range': (queue_min, queue_max),\n",
    "            'car_queue_range': (car_min, car_max),\n",
    "            'total_hours': format_hours(stats['total_seconds']),\n",
    "            'car_hours': format_hours(stats['car_seconds']),\n",
    "            'can_hours': format_hours(stats['can_seconds']),\n",
    "            'car_results': dict(stats['car_results']),\n",
    "            'can_results': dict(stats['can_results']),\n",
    "            'outcomes': dict(stats['outcomes']),\n",
    "        })\n",
    "\n",
    "    workers.sort(key=lambda item: item['worker_id'])\n",
    "    DB10_WORKER_CACHE = {\n",
    "        'zip_name': selected_zip_name,\n",
    "        'db10_file': db10_file,\n",
    "        'entries': entries,\n",
    "        'db10_data': db10_data,\n",
    "        'worker_stats': worker_stats,\n",
    "        'worker_summaries': workers,\n",
    "    }\n",
    "    if show_summary:\n",
    "        print(f\"DB 10: {len(entries)} events, {len(workers)} workers for {selected_zip_name}\")\n",
    "    if not workers:\n",
    "        return\n",
    "    if not show_summary:\n",
    "        return\n",
    "    for worker in workers:\n",
    "        iter_range = f\"{worker['iter_min']}\u2013{worker['iter_max']}\" if worker['iter_min'] is not None else '-'\n",
    "        queue_range = worker['queue_range']\n",
    "        car_queue_range = worker['car_queue_range']\n",
    "        queue_range_text = f\"{format_number(queue_range[0])}-{format_number(queue_range[1])}\" if queue_range[0] is not None else '-'\n",
    "        car_range_text = f\"{format_number(car_queue_range[0])}-{format_number(car_queue_range[1])}\" if car_queue_range[0] is not None else '-'\n",
    "        queue_avg_text = format_number(worker['queue_avg'])\n",
    "        car_queue_avg_text = format_number(worker['car_queue_avg'])\n",
    "        total_hours_text = format_number(worker['total_hours'], digits=2)\n",
    "        car_hours_text = format_number(worker['car_hours'], digits=2)\n",
    "        can_hours_text = format_number(worker['can_hours'], digits=2)\n",
    "        print(\n",
    "            f\"- {worker['worker_id']}: records={worker['records']}, iter={iter_range}, \"\n",
    "            f\"queue_avg={queue_avg_text}, car_queue_avg={car_queue_avg_text}, \"\n",
    "            f\"queue_range={queue_range_text}, car_queue_range={car_range_text}, \"\n",
    "            f\"h_tot={total_hours_text}, h_car={car_hours_text}, h_can={can_hours_text}\"\n",
    "        )\n",
    "        print(\n",
    "            \"  \"\n",
    "            + \" | \".join([\n",
    "                f\"car_results[{format_counter(worker['car_results'])}]\",\n",
    "                f\"can_results[{format_counter(worker['can_results'])}]\",\n",
    "                f\"outcomes[{format_counter(worker['outcomes'])}]\",\n",
    "            ])\n",
    "        )\n",
    "\n",
    "summarise_db10_workers(selected_zip_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "\n",
    "def _select_worker_id(worker_stats):\n",
    "    if not worker_stats:\n",
    "        return None\n",
    "    workers = sorted(worker_stats)\n",
    "    selected = globals().get('SELECTED_DB10_WORKER')\n",
    "    env_selected = os.environ.get('RESULTS_SELECTED_WORKER')\n",
    "    if env_selected in worker_stats:\n",
    "        selected = env_selected\n",
    "    if selected not in worker_stats:\n",
    "        selected = workers[0]\n",
    "    print(\"Workers found in DB 10:\")\n",
    "    for idx, wid in enumerate(workers):\n",
    "        marker = \"*\" if wid == selected else \" \"\n",
    "        print(f\"{marker}[{idx}] {wid}\")\n",
    "    user_choice = input(\"Select worker by index or name (press Enter to keep the current selection): \").strip()\n",
    "    if user_choice:\n",
    "        resolved = None\n",
    "        if user_choice.isdigit():\n",
    "            index = int(user_choice)\n",
    "            if 0 <= index < len(workers):\n",
    "                resolved = workers[index]\n",
    "        if resolved is None and user_choice in worker_stats:\n",
    "            resolved = user_choice\n",
    "        if resolved is None:\n",
    "            print(\"Invalid selection, keeping the current worker.\")\n",
    "        else:\n",
    "            selected = resolved\n",
    "    globals()['SELECTED_DB10_WORKER'] = selected\n",
    "    return selected\n",
    "\n",
    "def inspect_db10_worker(worker_id=None, *, max_events=5, sort_by='iteration', interactive=False):\n",
    "    cache = globals().get('DB10_WORKER_CACHE')\n",
    "    if not cache or cache.get('zip_name') != selected_zip_name:\n",
    "        summarise_db10_workers(selected_zip_name, show_summary=False)\n",
    "        cache = globals().get('DB10_WORKER_CACHE')\n",
    "    if not cache:\n",
    "        print(\"DB 10 statistics are not available.\")\n",
    "        return\n",
    "    worker_stats = cache.get('worker_stats') or {}\n",
    "    if not worker_stats:\n",
    "        print(\"No workers found in DB 10.\")\n",
    "        return\n",
    "    if worker_id and worker_id in worker_stats:\n",
    "        globals()['SELECTED_DB10_WORKER'] = worker_id\n",
    "    selected_worker = worker_id or globals().get('SELECTED_DB10_WORKER')\n",
    "    if selected_worker not in worker_stats:\n",
    "        if interactive:\n",
    "            selected_worker = _select_worker_id(worker_stats)\n",
    "        else:\n",
    "            print('Specify a worker_id or call the function with interactive=True to pick one interactively.')\n",
    "            print('Available workers:')\n",
    "            for wid in sorted(worker_stats):\n",
    "                print(f' - {wid}')\n",
    "            return\n",
    "    elif interactive:\n",
    "        change = input(\"Press Enter to keep the current worker or type 'c' to choose a different worker: \").strip().lower()\n",
    "        if change == 'c':\n",
    "            selected_worker = _select_worker_id(worker_stats)\n",
    "        else:\n",
    "            globals()['SELECTED_DB10_WORKER'] = selected_worker\n",
    "    else:\n",
    "        globals()['SELECTED_DB10_WORKER'] = selected_worker\n",
    "    if selected_worker not in worker_stats:\n",
    "        print('No worker selected.')\n",
    "        return\n",
    "    stats = worker_stats[selected_worker]\n",
    "\n",
    "    def fmt(value, digits=2):\n",
    "        return f\"{value:.{digits}f}\" if isinstance(value, (int, float)) else '-'\n",
    "\n",
    "    def safe_mean(values):\n",
    "        return statistics.mean(values) if values else None\n",
    "\n",
    "    iterations = stats.get('iterations', [])\n",
    "    queue_sizes = stats.get('queue_sizes', [])\n",
    "    car_queue_sizes = stats.get('car_queue_sizes', [])\n",
    "    total_seconds = stats.get('total_seconds', [])\n",
    "    car_seconds = stats.get('car_seconds', [])\n",
    "    can_seconds = stats.get('can_seconds', [])\n",
    "\n",
    "    print(f\"Worker {selected_worker} in DB 10 ({cache.get('zip_name')}):\")\n",
    "    print(f\"- recorded events: {stats.get('records', 0)}\")\n",
    "    if iterations:\n",
    "        print(f\"- iteration range: {min(iterations)}\u2013{max(iterations)}\")\n",
    "    else:\n",
    "        print(\"- iteration range: -\")\n",
    "    if queue_sizes:\n",
    "        print(\n",
    "            f\"- queue size: avg {fmt(safe_mean(queue_sizes), 1)}, \"\n",
    "            f\"min {fmt(min(queue_sizes), 0)}, max {fmt(max(queue_sizes), 0)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"- queue size: no data\")\n",
    "    if car_queue_sizes:\n",
    "        print(\n",
    "            f\"- CAR queue size: avg {fmt(safe_mean(car_queue_sizes), 1)}, \"\n",
    "            f\"min {fmt(min(car_queue_sizes), 0)}, max {fmt(max(car_queue_sizes), 0)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"- CAR queue size: no data\")\n",
    "\n",
    "    def sum_hours(values):\n",
    "        return fmt(sum(values) / 3600 if values else None, 2)\n",
    "\n",
    "    print(f\"- total hours: {sum_hours(total_seconds)}\")\n",
    "    print(f\"  - CAR: {sum_hours(car_seconds)}\")\n",
    "    print(f\"  - CAN: {sum_hours(can_seconds)}\")\n",
    "\n",
    "    def format_counter(mapping):\n",
    "        if not mapping:\n",
    "            return '-'\n",
    "        items = sorted(dict(mapping).items())\n",
    "        return ', '.join(f\"{key}:{value}\" for key, value in items)\n",
    "\n",
    "    print(f\"- CAR outcomes: {format_counter(stats.get('car_results'))}\")\n",
    "    print(f\"- CAN outcomes: {format_counter(stats.get('can_results'))}\")\n",
    "    print(f\"- outcomes: {format_counter(stats.get('outcomes'))}\")\n",
    "\n",
    "    events = list(stats.get('events') or [])\n",
    "    if not events:\n",
    "        print(\"No event details stored.\")\n",
    "        return\n",
    "\n",
    "    def event_key(event):\n",
    "        if sort_by == 'timestamp':\n",
    "            return (event.get('timestamp_start') or '', event.get('iteration') or float('inf'))\n",
    "        iteration = event.get('iteration')\n",
    "        return (iteration if iteration is not None else float('inf'), event.get('timestamp_start') or '')\n",
    "\n",
    "    events_sorted = sorted(events, key=event_key)\n",
    "    selected_events = events_sorted[-max_events:] if max_events and max_events > 0 else events_sorted\n",
    "    print(f\"Showing {len(selected_events)} events out of {len(events_sorted)} (sorted by {sort_by}).\")\n",
    "\n",
    "    for event in selected_events:\n",
    "        iteration = event.get('iteration')\n",
    "        timestamp_start = event.get('timestamp_start')\n",
    "        timestamp_end = event.get('timestamp_end')\n",
    "        queue_size = event.get('queue_size')\n",
    "        car_queue_size = event.get('car_queue_size')\n",
    "        timings = event.get('timings') or {}\n",
    "        car_processing = event.get('car_processing') or {}\n",
    "        can_processing = event.get('can_processing') or {}\n",
    "        outcomes = event.get('outcomes') or {}\n",
    "        line = (\n",
    "            f\"- iter={iteration}, start={timestamp_start}, end={timestamp_end}, \"\n",
    "            f\"queue={queue_size}, car_queue={car_queue_size}, \"\n",
    "            f\"tot_s={fmt(timings.get('total_seconds'), 1)}, \"\n",
    "            f\"car={car_processing.get('result') or '-'} ({fmt(timings.get('car_seconds'), 1)}s), \"\n",
    "            f\"can={can_processing.get('result') or '-'} ({fmt(timings.get('can_seconds'), 1)}s)\"\n",
    "        )\n",
    "        print(line)\n",
    "        extra_parts = []\n",
    "        if car_processing.get('time_seconds'):\n",
    "            extra_parts.append(f\"car_step={fmt(car_processing.get('time_seconds'), 1)}s\")\n",
    "        if can_processing.get('time_seconds'):\n",
    "            extra_parts.append(f\"can_step={fmt(can_processing.get('time_seconds'), 1)}s\")\n",
    "        extensions = event.get('extensions')\n",
    "        if isinstance(extensions, dict) and extensions:\n",
    "            ext_summary = ', '.join(f\"{k}:{v}\" for k, v in extensions.items())\n",
    "            extra_parts.append(f\"extensions[{ext_summary}]\")\n",
    "        raw_info = event.get('raw_info')\n",
    "        if isinstance(raw_info, dict) and raw_info:\n",
    "            info_summary = ', '.join(f\"{k}:{v}\" for k, v in raw_info.items())\n",
    "            extra_parts.append(f\"raw_info[{info_summary}]\")\n",
    "        if outcomes:\n",
    "            outcomes_summary = ', '.join(\n",
    "                f\"{key}={'T' if bool(value) else 'F'}\" for key, value in sorted(outcomes.items())\n",
    "            )\n",
    "            extra_parts.append(f\"outcomes[{outcomes_summary}]\")\n",
    "        if extra_parts:\n",
    "            print(\"  \" + \" | \".join(extra_parts))\n",
    "\n",
    "# Imposta a True per attivare la selezione interattiva\n",
    "RUN_INTERACTIVE_INSPECTION = False\n",
    "if RUN_INTERACTIVE_INSPECTION:\n",
    "    inspect_db10_worker(max_events=5, interactive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def build_db10_worker_report(*, max_events=5, sort_by='iteration'):\n",
    "    cache = globals().get('DB10_WORKER_CACHE')\n",
    "    if not cache or cache.get('zip_name') != selected_zip_name:\n",
    "        summarise_db10_workers(selected_zip_name, show_summary=False)\n",
    "        cache = globals().get('DB10_WORKER_CACHE')\n",
    "    if not cache:\n",
    "        print('DB 10 statistics are not available.')\n",
    "        return {}\n",
    "    worker_stats = cache.get('worker_stats') or {}\n",
    "    if not worker_stats:\n",
    "        print('No workers found in DB 10.')\n",
    "        return {}\n",
    "\n",
    "    def safe_mean(values):\n",
    "        return statistics.mean(values) if values else None\n",
    "\n",
    "    def hours(values):\n",
    "        return sum(values) / 3600 if values else 0.0\n",
    "\n",
    "    def event_key(event):\n",
    "        if sort_by == 'timestamp':\n",
    "            return (event.get('timestamp_start') or '', event.get('iteration') or float('inf'))\n",
    "        iteration = event.get('iteration')\n",
    "        return (iteration if iteration is not None else float('inf'), event.get('timestamp_start') or '')\n",
    "\n",
    "    def serialize_event(event):\n",
    "        timings = event.get('timings') or {}\n",
    "        car_processing = event.get('car_processing') or {}\n",
    "        can_processing = event.get('can_processing') or {}\n",
    "        return {\n",
    "            'iteration': event.get('iteration'),\n",
    "            'timestamp_start': event.get('timestamp_start'),\n",
    "            'timestamp_end': event.get('timestamp_end'),\n",
    "            'queue_size': event.get('queue_size'),\n",
    "            'car_queue_size': event.get('car_queue_size'),\n",
    "            'timings': {\n",
    "                'total_seconds': timings.get('total_seconds'),\n",
    "                'car_seconds': timings.get('car_seconds'),\n",
    "                'can_seconds': timings.get('can_seconds'),\n",
    "            },\n",
    "            'car_processing': {\n",
    "                'result': car_processing.get('result'),\n",
    "                'time_seconds': car_processing.get('time_seconds'),\n",
    "                'raw_info': car_processing.get('raw_info'),\n",
    "                'extensions': car_processing.get('extensions'),\n",
    "            },\n",
    "            'can_processing': {\n",
    "                'result': can_processing.get('result'),\n",
    "                'time_seconds': can_processing.get('time_seconds'),\n",
    "                'raw_info': can_processing.get('raw_info'),\n",
    "                'extensions': can_processing.get('extensions'),\n",
    "            },\n",
    "            'outcomes': event.get('outcomes'),\n",
    "        }\n",
    "\n",
    "    report = {\n",
    "        'zip_name': cache.get('zip_name'),\n",
    "        'worker_count': len(worker_stats),\n",
    "        'workers': {}\n",
    "    }\n",
    "\n",
    "    for worker_id in sorted(worker_stats):\n",
    "        stats = worker_stats[worker_id]\n",
    "        iterations = stats.get('iterations') or []\n",
    "        queue_sizes = stats.get('queue_sizes') or []\n",
    "        car_queue_sizes = stats.get('car_queue_sizes') or []\n",
    "        summary = {\n",
    "            'records': stats.get('records', 0),\n",
    "            'iteration_range': [min(iterations), max(iterations)] if iterations else None,\n",
    "            'queue': {\n",
    "                'min': min(queue_sizes) if queue_sizes else None,\n",
    "                'mean': safe_mean(queue_sizes),\n",
    "                'max': max(queue_sizes) if queue_sizes else None,\n",
    "            },\n",
    "            'car_queue': {\n",
    "                'min': min(car_queue_sizes) if car_queue_sizes else None,\n",
    "                'mean': safe_mean(car_queue_sizes),\n",
    "                'max': max(car_queue_sizes) if car_queue_sizes else None,\n",
    "            },\n",
    "            'timings_hours': {\n",
    "                'total': hours(stats.get('total_seconds') or []),\n",
    "                'car': hours(stats.get('car_seconds') or []),\n",
    "                'can': hours(stats.get('can_seconds') or []),\n",
    "            },\n",
    "            'car_results': dict(stats.get('car_results') or {}),\n",
    "            'can_results': dict(stats.get('can_results') or {}),\n",
    "            'outcomes': dict(stats.get('outcomes') or {}),\n",
    "        }\n",
    "        events = list(stats.get('events') or [])\n",
    "        events_sorted = sorted(events, key=event_key)\n",
    "        if max_events is not None and max_events > 0:\n",
    "            events_sorted = events_sorted[-max_events:]\n",
    "        summary['events'] = [serialize_event(event) for event in events_sorted]\n",
    "        report['workers'][worker_id] = summary\n",
    "\n",
    "    globals()['DB10_WORKER_REPORT'] = report\n",
    "    print(f\"Report built for {report['worker_count']} workers.\")\n",
    "    return report\n",
    "\n",
    "DB10_WORKER_REPORT = build_db10_worker_report(max_events=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import re\n",
    "import statistics\n",
    "from collections import OrderedDict\n",
    "from typing import Iterable\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "DB_DISPLAY_NAMES = {\n",
    "    'DATA': 'Data',\n",
    "    'CAN': 'Candidate reasons',\n",
    "    'R': 'Reasons',\n",
    "    'NR': 'Non-reasons',\n",
    "    'CAR': 'Candidate anti-reasons',\n",
    "    'AR': 'Anti-reasons',\n",
    "    'GP': 'Good profiles',\n",
    "    'BP': 'Bad profiles',\n",
    "    'PR': 'Preferred reasons',\n",
    "    'AP': 'Anti-reason profiles',\n",
    "    'LOGS': 'Worker iteration logs',\n",
    "}\n",
    "\n",
    "CANDIDATE_REASONS_NAME = DB_DISPLAY_NAMES['CAN']\n",
    "CANDIDATE_ANTI_REASONS_NAME = DB_DISPLAY_NAMES['CAR']\n",
    "\n",
    "BASE_COLUMNS = [\n",
    "    'worker_id',\n",
    "    'records',\n",
    "    'iter_min',\n",
    "    'iter_max',\n",
    "    'queue_min',\n",
    "    'queue_mean',\n",
    "    'queue_max',\n",
    "    'car_queue_min',\n",
    "    'car_queue_mean',\n",
    "    'car_queue_max',\n",
    "    'hours_total',\n",
    "    'hours_car',\n",
    "    'hours_can',\n",
    "]\n",
    "\n",
    "SCATTER_PLOTS = [\n",
    "    {\n",
    "        'title': 'Queue size vs iteration range',\n",
    "        'x': 'iter_max',\n",
    "        'y': 'queue_mean',\n",
    "        'xlabel': None,\n",
    "        'ylabel': None,\n",
    "    },\n",
    "    {\n",
    "        'title': f'{CANDIDATE_ANTI_REASONS_NAME} queue vs queue size',\n",
    "        'x': 'queue_mean',\n",
    "        'y': 'car_queue_mean',\n",
    "        'xlabel': None,\n",
    "        'ylabel': None,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Total hours vs records',\n",
    "        'x': 'records',\n",
    "        'y': 'hours_total',\n",
    "        'xlabel': None,\n",
    "        'ylabel': None,\n",
    "    },\n",
    "    {\n",
    "        'title': f'Processing hours ({CANDIDATE_ANTI_REASONS_NAME} vs {CANDIDATE_REASONS_NAME})',\n",
    "        'x': 'hours_car',\n",
    "        'y': 'hours_can',\n",
    "        'xlabel': None,\n",
    "        'ylabel': None,\n",
    "    },\n",
    "]\n",
    "\n",
    "BAR_PLOTS = [\n",
    "    {\n",
    "        'columns': ['queue_mean'],\n",
    "        'title': 'Average queue size per worker',\n",
    "        'ylabel': 'Average queue size',\n",
    "        'sort_by': 'queue_mean',\n",
    "    },\n",
    "    {\n",
    "        'columns': ['car_queue_mean'],\n",
    "        'title': f'Average {CANDIDATE_ANTI_REASONS_NAME} queue size per worker',\n",
    "        'ylabel': f'Average {CANDIDATE_ANTI_REASONS_NAME} queue size',\n",
    "        'sort_by': 'car_queue_mean',\n",
    "    },\n",
    "    {\n",
    "        'columns': ['records'],\n",
    "        'title': 'Events processed per worker',\n",
    "        'ylabel': 'Events',\n",
    "        'sort_by': 'records',\n",
    "    },\n",
    "]\n",
    "\n",
    "STACKED_BAR_CONFIG = [\n",
    "    {\n",
    "        'prefix': 'car_result_',\n",
    "        'title': f'{CANDIDATE_ANTI_REASONS_NAME} results per worker',\n",
    "        'ylabel': 'Count',\n",
    "    },\n",
    "    {\n",
    "        'prefix': 'can_result_',\n",
    "        'title': f'{CANDIDATE_REASONS_NAME} results per worker',\n",
    "        'ylabel': 'Count',\n",
    "    },\n",
    "    {\n",
    "        'prefix': 'outcome_',\n",
    "        'title': 'Outcome flags per worker',\n",
    "        'ylabel': 'Count',\n",
    "    },\n",
    "]\n",
    "\n",
    "HISTOGRAMS = [\n",
    "    {\n",
    "        'column': 'queue_mean',\n",
    "        'title': 'Distribution of average queue size',\n",
    "        'xlabel': None,\n",
    "    },\n",
    "    {\n",
    "        'column': 'hours_total',\n",
    "        'title': 'Distribution of total processing hours',\n",
    "        'xlabel': None,\n",
    "    },\n",
    "]\n",
    "\n",
    "ADDITIONAL_SCATTER_PREFIX_PAIRS = [\n",
    "    ('car_result_CONFIRMED_AR', 'car_result_NOT_AR', f'{CANDIDATE_ANTI_REASONS_NAME} confirmed vs not'),\n",
    "    ('can_result_GOOD', 'hours_total', f\"{CANDIDATE_REASONS_NAME} GOOD vs total hours\"),\n",
    "]\n",
    "\n",
    "def _ensure_report():\n",
    "    report = globals().get('DB10_WORKER_REPORT')\n",
    "    if not report or report.get('zip_name') != selected_zip_name:\n",
    "        report = build_db10_worker_report(max_events=5)\n",
    "    return report\n",
    "\n",
    "\n",
    "def _flatten_worker_summary(report):\n",
    "    rows = []\n",
    "    car_keys = set()\n",
    "    can_keys = set()\n",
    "    outcome_keys = set()\n",
    "    workers = report.get('workers') or {}\n",
    "    for summary in workers.values():\n",
    "        car_keys.update((summary.get('car_results') or {}).keys())\n",
    "        can_keys.update((summary.get('can_results') or {}).keys())\n",
    "        outcome_keys.update((summary.get('outcomes') or {}).keys())\n",
    "    car_keys = sorted(car_keys)\n",
    "    can_keys = sorted(can_keys)\n",
    "    outcome_keys = sorted(outcome_keys)\n",
    "\n",
    "    sorted_workers = sorted(workers.items())\n",
    "    for index, (worker_id, summary) in enumerate(sorted_workers, start=1):\n",
    "        queue = summary.get('queue') or {}\n",
    "        car_queue = summary.get('car_queue') or {}\n",
    "        timings = summary.get('timings_hours') or {}\n",
    "        iteration_range = summary.get('iteration_range') or [None, None]\n",
    "        short_name = worker_id.split(':')[-1]\n",
    "        label = f\"W{index:02d}\"\n",
    "        row = OrderedDict(\n",
    "            worker_id=worker_id,\n",
    "            worker_index=index,\n",
    "            worker_label=label,\n",
    "            worker_short_name=short_name,\n",
    "            records=summary.get('records'),\n",
    "            iter_min=iteration_range[0],\n",
    "            iter_max=iteration_range[1],\n",
    "            queue_min=queue.get('min'),\n",
    "            queue_mean=queue.get('mean'),\n",
    "            queue_max=queue.get('max'),\n",
    "            car_queue_min=car_queue.get('min'),\n",
    "            car_queue_mean=car_queue.get('mean'),\n",
    "            car_queue_max=car_queue.get('max'),\n",
    "            hours_total=timings.get('total'),\n",
    "            hours_car=timings.get('car'),\n",
    "            hours_can=timings.get('can'),\n",
    "        )\n",
    "        car_results = summary.get('car_results') or {}\n",
    "        can_results = summary.get('can_results') or {}\n",
    "        outcomes = summary.get('outcomes') or {}\n",
    "        for key in car_keys:\n",
    "            row[f'car_result_{key}'] = car_results.get(key, 0)\n",
    "        for key in can_keys:\n",
    "            row[f'can_result_{key}'] = can_results.get(key, 0)\n",
    "        for key in outcome_keys:\n",
    "            row[f'outcome_{key}'] = outcomes.get(key, 0)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _filter_valid_points(df: pd.DataFrame, x_key: str, y_key: str):\n",
    "    subset = df.dropna(subset=[x_key, y_key])\n",
    "    subset = subset[subset[[x_key, y_key]].applymap(lambda v: isinstance(v, (int, float))).all(axis=1)]\n",
    "    if subset.empty:\n",
    "        return []\n",
    "    return list(zip(subset[x_key], subset[y_key], subset['worker_label']))\n",
    "\n",
    "\n",
    "def _token_to_display(token: str) -> str:\n",
    "    upper = token.upper()\n",
    "    if upper in DB_DISPLAY_NAMES:\n",
    "        return DB_DISPLAY_NAMES[upper]\n",
    "    lookup = {\n",
    "        'CAR': CANDIDATE_ANTI_REASONS_NAME,\n",
    "        'CAN': CANDIDATE_REASONS_NAME,\n",
    "    }\n",
    "    if upper in lookup:\n",
    "        return lookup[upper]\n",
    "    if token.isupper():\n",
    "        return token.capitalize()\n",
    "    return token.replace('_', ' ').capitalize()\n",
    "\n",
    "\n",
    "def _format_result_label(prefix: str, raw: str) -> str:\n",
    "    parts = raw.split('_') if raw else []\n",
    "    if parts and parts[-1].upper() in DB_DISPLAY_NAMES:\n",
    "        parts = parts[:-1]\n",
    "    descriptor = ' '.join(_token_to_display(part.lower()) for part in parts) if parts else 'Total'\n",
    "    return f\"{prefix} result: {descriptor}\"\n",
    "\n",
    "\n",
    "def _format_outcome_label(raw: str) -> str:\n",
    "    key, sep, value = raw.partition('=')\n",
    "    tokens = key.split('_') if key else []\n",
    "    words = [_token_to_display(tok) for tok in tokens]\n",
    "    label = ' '.join(words) if words else 'Outcome'\n",
    "    if sep:\n",
    "        value_text = {'T': 'True', 'F': 'False', 'TRUE': 'True', 'FALSE': 'False'}.get(value.upper(), value)\n",
    "        return f\"Outcome: {label} ({value_text})\"\n",
    "    return f\"Outcome: {label}\"\n",
    "\n",
    "\n",
    "def _rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rename_map = {\n",
    "        'worker_label': 'Worker',\n",
    "        'worker_id': 'Worker ID',\n",
    "        'records': 'Records',\n",
    "        'iter_min': 'Iteration min',\n",
    "        'iter_max': 'Iteration max',\n",
    "        'queue_min': 'Queue min',\n",
    "        'queue_mean': 'Queue mean',\n",
    "        'queue_max': 'Queue max',\n",
    "        'car_queue_min': f'{CANDIDATE_ANTI_REASONS_NAME} queue min',\n",
    "        'car_queue_mean': f'{CANDIDATE_ANTI_REASONS_NAME} queue mean',\n",
    "        'car_queue_max': f'{CANDIDATE_ANTI_REASONS_NAME} queue max',\n",
    "        'hours_total': 'Total hours',\n",
    "        'hours_car': f'{CANDIDATE_ANTI_REASONS_NAME} hours',\n",
    "        'hours_can': f'{CANDIDATE_REASONS_NAME} hours',\n",
    "    }\n",
    "    computed = {}\n",
    "    for col in df.columns:\n",
    "        if col.startswith('car_result_'):\n",
    "            computed[col] = _format_result_label(CANDIDATE_ANTI_REASONS_NAME, col[len('car_result_'):])\n",
    "        elif col.startswith('can_result_'):\n",
    "            computed[col] = _format_result_label(CANDIDATE_REASONS_NAME, col[len('can_result_'):])\n",
    "        elif col.startswith('outcome_'):\n",
    "            computed[col] = _format_outcome_label(col[len('outcome_'):])\n",
    "        elif col in rename_map:\n",
    "            computed[col] = rename_map[col]\n",
    "        else:\n",
    "            computed[col] = col\n",
    "    return df.rename(columns=computed)\n",
    "\n",
    "\n",
    "def _column_display_name(column: str) -> str:\n",
    "    temp = _rename_columns(pd.DataFrame(columns=[column]))\n",
    "    return temp.columns[0]\n",
    "\n",
    "\n",
    "def _plot_scatter(df: pd.DataFrame, config: dict):\n",
    "    points = _filter_valid_points(df, config['x'], config['y'])\n",
    "    if not points:\n",
    "        print(f\"Plot '{config['title']}' skipped: insufficient data.\")\n",
    "        return\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    xs, ys, labels = zip(*points)\n",
    "    plt.scatter(xs, ys, alpha=0.7)\n",
    "    for x, y, label in points:\n",
    "        plt.annotate(label, (x, y), textcoords='offset points', xytext=(5, 3), fontsize=8)\n",
    "    xlabel = config.get('xlabel') or _column_display_name(config['x'])\n",
    "    ylabel = config.get('ylabel') or _column_display_name(config['y'])\n",
    "    plt.title(config['title'])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _plot_bar(df: pd.DataFrame, *, columns: Iterable[str], title: str, ylabel: str, stacked: bool = False, sort_by: str | None = None):\n",
    "    available_columns = [col for col in columns if col in df.columns]\n",
    "    if not available_columns:\n",
    "        print(f\"Plot '{title}' skipped: columns not available.\")\n",
    "        return\n",
    "    subset = df[['worker_label'] + available_columns].copy()\n",
    "    subset = subset.dropna(how='all', subset=available_columns)\n",
    "    if subset.empty:\n",
    "        print(f\"Plot '{title}' skipped: insufficient data.\")\n",
    "        return\n",
    "    subset[available_columns] = subset[available_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    subset = subset.dropna()\n",
    "    if subset.empty:\n",
    "        print(f\"Plot '{title}' skipped: non-numeric data.\")\n",
    "        return\n",
    "    renamed_subset = _rename_columns(subset)\n",
    "    plot_df = renamed_subset.set_index('Worker')\n",
    "    value_columns = []\n",
    "    for original in available_columns:\n",
    "        display_name = _column_display_name(original)\n",
    "        if display_name in plot_df.columns:\n",
    "            value_columns.append(display_name)\n",
    "    if not value_columns:\n",
    "        print(f\"Plot '{title}' skipped: no numeric columns available after renaming.\")\n",
    "        return\n",
    "    sort_column = _column_display_name(sort_by) if sort_by else None\n",
    "    if sort_column and sort_column in plot_df.columns:\n",
    "        plot_df = plot_df.sort_values(sort_column, ascending=False)\n",
    "    plt.figure(figsize=(max(8, len(plot_df) * 0.5), 5))\n",
    "    plot_df[value_columns].plot(kind='bar', stacked=stacked, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('Worker')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _collect_heatmap_rows(entries):\n",
    "    extracted = []\n",
    "    for entry in entries or []:\n",
    "        try:\n",
    "            key_bytes = decode_key(entry)\n",
    "            key_text = key_bytes.decode('utf-8', errors='replace')\n",
    "        except Exception:\n",
    "            continue\n",
    "        preview, details = try_decode_value(entry)\n",
    "        timestamp = None\n",
    "        if isinstance(preview, bytes):\n",
    "            preview = preview.decode('utf-8', errors='replace')\n",
    "        if isinstance(preview, str):\n",
    "            candidate = preview.strip()\n",
    "            if candidate:\n",
    "                timestamp = candidate\n",
    "        if timestamp is None and isinstance(details, dict):\n",
    "            decoded_bytes = details.get('decoded_bytes')\n",
    "            if isinstance(decoded_bytes, (bytes, bytearray)):\n",
    "                candidate = decoded_bytes.decode('utf-8', errors='replace').strip()\n",
    "                if candidate:\n",
    "                    timestamp = candidate\n",
    "        extracted.append((timestamp, key_text))\n",
    "    if not extracted:\n",
    "        return np.empty((0, 0), dtype=int), []\n",
    "    def _sort_key(item):\n",
    "        ts, _ = item\n",
    "        if ts:\n",
    "            try:\n",
    "                return datetime.fromisoformat(ts)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return datetime.max\n",
    "    extracted.sort(key=_sort_key)\n",
    "    width = max(len(key) for _, key in extracted)\n",
    "    matrix = np.full((len(extracted), width), np.nan, dtype=float)\n",
    "    labels = []\n",
    "    for row_idx, (ts, key_text) in enumerate(extracted):\n",
    "        if len(key_text) != width:\n",
    "            key_text = key_text.ljust(width, '0')\n",
    "        matrix[row_idx, :] = [1 if ch == '1' else 0 for ch in key_text]\n",
    "        labels.append(ts or 'n/a')\n",
    "    return matrix, labels\n",
    "def render_bitmap_heatmaps():\n",
    "    heatmap_dbs = [\n",
    "        (2, DB_DISPLAY_NAMES.get('R', 'Reasons')),\n",
    "        (3, DB_DISPLAY_NAMES.get('NR', 'Non-reasons')),\n",
    "        (5, DB_DISPLAY_NAMES.get('AR', 'Anti-reasons')),\n",
    "        (6, DB_DISPLAY_NAMES.get('GP', 'Good profiles')),\n",
    "        (7, DB_DISPLAY_NAMES.get('BP', 'Bad profiles')),\n",
    "        (8, DB_DISPLAY_NAMES.get('PR', 'Preferred reasons')),\n",
    "        (9, DB_DISPLAY_NAMES.get('AP', 'Anti-reason profiles')),\n",
    "    ]\n",
    "    if not selected_manifest or not selected_backups:\n",
    "        print('Bitmap heatmaps unavailable: no backups loaded.')\n",
    "        return\n",
    "    files_map = selected_manifest.get('files') or {}\n",
    "    generated_any = False\n",
    "    for db_index, label in heatmap_dbs:\n",
    "        file_name = files_map.get(str(db_index))\n",
    "        if not file_name:\n",
    "            continue\n",
    "        data = selected_backups.get(file_name) if isinstance(selected_backups, dict) else None\n",
    "        if not isinstance(data, dict):\n",
    "            continue\n",
    "        matrix, labels = _collect_heatmap_rows(data.get('entries'))\n",
    "        if matrix.size == 0:\n",
    "            continue\n",
    "        generated_any = True\n",
    "        fig_height = max(4, matrix.shape[0] * 0.25)\n",
    "        fig, ax = plt.subplots(figsize=(12, fig_height))\n",
    "        im = ax.imshow(matrix, aspect='auto', cmap='viridis')\n",
    "        ax.set_title(f\"{label} bitmap heatmap (timestamp order)\")\n",
    "        ax.set_xlabel('Bitmap position')\n",
    "        ax.set_ylabel('Timestamp')\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_yticklabels(labels, fontsize=8)\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Bit value')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    if not generated_any:\n",
    "        print('No bitmap heatmaps generated (missing or empty data).')\n",
    "def render_db0_eu_analysis():\n",
    "    entry_map = globals().get('selected_db0_values_by_key') or {}\n",
    "    eu_entry = entry_map.get('EU') if isinstance(entry_map, dict) else None\n",
    "    if not eu_entry:\n",
    "        print('DB 0 EU entry not available for the current selection.')\n",
    "        return\n",
    "    series_map = eu_entry.get('value_json')\n",
    "    if not isinstance(series_map, dict) or not series_map:\n",
    "        print('DB 0 EU entry does not contain a time series map.')\n",
    "        return\n",
    "    feature_names = sorted(series_map)\n",
    "    cleaned_series = []\n",
    "    lengths = []\n",
    "    max_len = 0\n",
    "    for name in feature_names:\n",
    "        values = series_map.get(name)\n",
    "        numeric_values = []\n",
    "        if isinstance(values, list):\n",
    "            for value in values:\n",
    "                if isinstance(value, (int, float)):\n",
    "                    numeric_values.append(float(value))\n",
    "                else:\n",
    "                    try:\n",
    "                        numeric_values.append(float(value))\n",
    "                    except (TypeError, ValueError):\n",
    "                        numeric_values.append(float('nan'))\n",
    "        cleaned_series.append(numeric_values)\n",
    "        lengths.append(len(numeric_values))\n",
    "        max_len = max(max_len, len(numeric_values))\n",
    "    if max_len == 0:\n",
    "        print('DB 0 EU entry does not contain numeric time series.')\n",
    "        return\n",
    "    matrix = np.full((len(cleaned_series), max_len), np.nan, dtype=float)\n",
    "    for idx, series in enumerate(cleaned_series):\n",
    "        if series:\n",
    "            matrix[idx, : len(series)] = series\n",
    "    lengths = np.array(lengths)\n",
    "    print('DB 0 EU summary:')\n",
    "    print(f'  features: {len(feature_names)}')\n",
    "    print(f'  min length: {np.nanmin(lengths) if lengths.size else 0}')\n",
    "    print(f'  max length: {np.nanmax(lengths) if lengths.size else 0}')\n",
    "    print(f'  average length: {np.nanmean(lengths) if lengths.size else 0:.2f}')\n",
    "    time_index = np.arange(max_len)\n",
    "    counts = np.sum(~np.isnan(matrix), axis=0)\n",
    "    mean_series = np.nanmean(matrix, axis=0)\n",
    "    std_series = np.nanstd(matrix, axis=0)\n",
    "    mean_mask = counts > 0\n",
    "    std_mask = counts > 1\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=False)\n",
    "    axes[0].bar(range(len(feature_names)), lengths, color='tab:blue')\n",
    "    axes[0].set_ylabel('Length')\n",
    "    axes[0].set_title('EU series lengths per feature')\n",
    "    axes[0].set_xticks(range(len(feature_names)))\n",
    "    axes[0].set_xticklabels(feature_names, rotation=90, fontsize=6)\n",
    "    box_data = [np.array(series, dtype=float) for series in cleaned_series if series]\n",
    "    if box_data:\n",
    "        box_values = [np.array(series)[~np.isnan(series)] for series in box_data]\n",
    "        axes[1].boxplot([values for values in box_values if values.size], vert=True, patch_artist=True)\n",
    "        axes[1].set_title('EU feature distribution (box plot)')\n",
    "        axes[1].set_ylabel('Value')\n",
    "        axes[1].set_xticks(range(1, len(feature_names) + 1))\n",
    "        axes[1].set_xticklabels(feature_names, rotation=90, fontsize=6)\n",
    "    axes[1].set_xlabel('Feature index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def _coerce_numeric_series(data):\n",
    "    if data is None:\n",
    "        return []\n",
    "    if isinstance(data, bool):\n",
    "        return []\n",
    "    if isinstance(data, (int, float)):\n",
    "        return [float(data)]\n",
    "    if isinstance(data, str):\n",
    "        trimmed = data.strip()\n",
    "        if not trimmed:\n",
    "            return []\n",
    "        try:\n",
    "            return [float(trimmed)]\n",
    "        except ValueError:\n",
    "            try:\n",
    "                parsed = json.loads(trimmed)\n",
    "            except json.JSONDecodeError:\n",
    "                tokens = []\n",
    "                for token in trimmed.replace(',', ' ').split():\n",
    "                    try:\n",
    "                        tokens.append(float(token))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                return tokens\n",
    "            else:\n",
    "                return _coerce_numeric_series(parsed)\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        collected = []\n",
    "        for item in data:\n",
    "            if isinstance(item, dict):\n",
    "                handled = False\n",
    "                for candidate in ('value', 'y', 'val', 'score'):\n",
    "                    if candidate in item:\n",
    "                        nested = _coerce_numeric_series(item[candidate])\n",
    "                        if nested:\n",
    "                            collected.extend(nested)\n",
    "                            handled = True\n",
    "                        break\n",
    "                if handled:\n",
    "                    continue\n",
    "                nested = _coerce_numeric_series(list(item.values()))\n",
    "                if nested:\n",
    "                    collected.extend(nested)\n",
    "            else:\n",
    "                nested = _coerce_numeric_series(item)\n",
    "                if nested:\n",
    "                    collected.extend(nested)\n",
    "        return collected\n",
    "    if isinstance(data, dict):\n",
    "        for candidate in (\n",
    "            'series',\n",
    "            'values',\n",
    "            'data',\n",
    "            'points',\n",
    "            'samples',\n",
    "            'sample',\n",
    "            'payload',\n",
    "            'entries',\n",
    "            'items',\n",
    "            'measurements',\n",
    "            'sample_dict',\n",
    "        ):\n",
    "            if candidate in data:\n",
    "                nested = _coerce_numeric_series(data[candidate])\n",
    "                if nested:\n",
    "                    return nested\n",
    "        numeric_items = []\n",
    "        for idx, (key, value) in enumerate(data.items()):\n",
    "            floats = _coerce_numeric_series(value)\n",
    "            if not floats:\n",
    "                continue\n",
    "            if len(floats) == 1:\n",
    "                numeric_items.append(((idx, 0), key, floats[0]))\n",
    "            else:\n",
    "                for offset, val in enumerate(floats):\n",
    "                    numeric_items.append(((idx, offset), f\"{key}[{offset}]\", val))\n",
    "        if not numeric_items:\n",
    "            return []\n",
    "        def _dict_sort_key(order, raw_key):\n",
    "            order_token = tuple(order)\n",
    "            if isinstance(raw_key, (int, float)):\n",
    "                return (0, float(raw_key), order_token)\n",
    "            if isinstance(raw_key, str):\n",
    "                stripped = raw_key.strip()\n",
    "                try:\n",
    "                    return (0, float(stripped), order_token)\n",
    "                except ValueError:\n",
    "                    numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', stripped)\n",
    "                    if numbers:\n",
    "                        return (1, tuple(float(num) for num in numbers), order_token)\n",
    "                    return (2, stripped.lower(), order_token)\n",
    "            return (3, str(raw_key), order_token)\n",
    "        numeric_items.sort(key=lambda item: _dict_sort_key(item[0], item[1]))\n",
    "        return [value for _, _, value in numeric_items]\n",
    "    return []\n",
    "def _extract_sample_series(entry):\n",
    "    if not isinstance(entry, dict):\n",
    "        return []\n",
    "    value_json = entry.get('value_json')\n",
    "    series = _coerce_numeric_series(value_json)\n",
    "    if series:\n",
    "        return series\n",
    "    value_text = entry.get('value_text')\n",
    "    if isinstance(value_text, str):\n",
    "        series = _coerce_numeric_series(value_text)\n",
    "        if series:\n",
    "            return series\n",
    "    value_bytes = entry.get('value_bytes')\n",
    "    if isinstance(value_bytes, (bytes, bytearray)):\n",
    "        try:\n",
    "            decoded = value_bytes.decode('utf-8', errors='replace')\n",
    "        except Exception:\n",
    "            decoded = ''\n",
    "        if decoded:\n",
    "            series = _coerce_numeric_series(decoded)\n",
    "            if series:\n",
    "                return series\n",
    "    details = entry.get('details')\n",
    "    if isinstance(details, dict):\n",
    "        decoded_bytes = details.get('decoded_bytes')\n",
    "        if isinstance(decoded_bytes, (bytes, bytearray)):\n",
    "            try:\n",
    "                decoded = decoded_bytes.decode('utf-8', errors='replace')\n",
    "            except Exception:\n",
    "                decoded = ''\n",
    "            if decoded:\n",
    "                series = _coerce_numeric_series(decoded)\n",
    "                if series:\n",
    "                    return series\n",
    "    return []\n",
    "def _extract_sample_timestamp(meta_entry):\n",
    "    if not isinstance(meta_entry, dict):\n",
    "        return None\n",
    "    value_json = meta_entry.get('value_json')\n",
    "    if isinstance(value_json, dict):\n",
    "        for key in ('timestamp', 'created_at', 'created'):\n",
    "            ts = value_json.get(key)\n",
    "            if isinstance(ts, str) and ts:\n",
    "                return ts\n",
    "    value_text = meta_entry.get('value_text') if isinstance(meta_entry, dict) else None\n",
    "    if isinstance(value_text, str):\n",
    "        trimmed = value_text.strip()\n",
    "        if trimmed.startswith('{') and trimmed.endswith('}'):\n",
    "            try:\n",
    "                parsed = json.loads(trimmed)\n",
    "            except json.JSONDecodeError:\n",
    "                parsed = None\n",
    "            if isinstance(parsed, dict):\n",
    "                for key in ('timestamp', 'created_at', 'created'):\n",
    "                    ts = parsed.get(key)\n",
    "                    if isinstance(ts, str) and ts:\n",
    "                        return ts\n",
    "        else:\n",
    "            return trimmed or None\n",
    "    return None\n",
    "def render_db0_sample_timeseries():\n",
    "    entry_map = globals().get('selected_db0_values_by_key') or {}\n",
    "    if not isinstance(entry_map, dict):\n",
    "        print('DB 0 sample entries not available for the current selection.')\n",
    "        return\n",
    "    collected = []\n",
    "    for key, entry in entry_map.items():\n",
    "        if not (key.startswith('sample_') and not key.endswith('_meta')):\n",
    "            continue\n",
    "        series = _extract_sample_series(entry)\n",
    "        if not series:\n",
    "            continue\n",
    "        meta_entry = entry_map.get(f\"{key}_meta\")\n",
    "        timestamp = _extract_sample_timestamp(meta_entry) if isinstance(meta_entry, dict) else None\n",
    "        collected.append((timestamp, key, series))\n",
    "    if not collected:\n",
    "        print('No DB 0 sample time series available.')\n",
    "        return\n",
    "    def _sort_key(item):\n",
    "        ts, key, _ = item\n",
    "        if ts:\n",
    "            try:\n",
    "                return datetime.fromisoformat(ts)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return key\n",
    "    collected.sort(key=_sort_key)\n",
    "    cols = 2\n",
    "    rows = (len(collected) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(14, rows * 3), squeeze=False)\n",
    "    flat_axes = axes.flatten()\n",
    "    for ax in flat_axes[len(collected):]:\n",
    "        ax.axis('off')\n",
    "    for ax, (timestamp, key, series) in zip(flat_axes, collected):\n",
    "        ax.plot(range(len(series)), series, marker='o', linewidth=1)\n",
    "        title = f\"{key} ({timestamp})\" if timestamp else key\n",
    "        ax.set_title(title, fontsize=8)\n",
    "        ax.set_xlabel('Index')\n",
    "        ax.set_ylabel('Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def _plot_histogram(df: pd.DataFrame, *, column: str, title: str, xlabel: str):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Histogram '{title}' skipped: column not available.\")\n",
    "        return\n",
    "    series = pd.to_numeric(df[column], errors='coerce').dropna()\n",
    "    if series.empty:\n",
    "        print(f\"Histogram '{title}' skipped: insufficient data.\")\n",
    "        return\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(series, bins=min(25, len(series)), alpha=0.7)\n",
    "    label = xlabel or _column_display_name(column)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Number of workers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def render_worker_report():\n",
    "    report = _ensure_report()\n",
    "    rows = _flatten_worker_summary(report)\n",
    "    if not rows:\n",
    "        print('No data available for the worker report.')\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values('worker_index')\n",
    "    base_columns = [col for col in BASE_COLUMNS if col in df.columns]\n",
    "    extra_columns = sorted(\n",
    "        col for col in df.columns if col.startswith('car_result_') or col.startswith('can_result_') or col.startswith('outcome_')\n",
    "    )\n",
    "\n",
    "    base_table_columns = ['worker_label', 'worker_id'] + [col for col in BASE_COLUMNS if col not in {'worker_id'}]\n",
    "    base_table_columns = [col for col in base_table_columns if col in df.columns]\n",
    "    display(_rename_columns(df[base_table_columns]))\n",
    "    print('Detailed results per worker:')\n",
    "    detail_columns = ['worker_label'] + extra_columns\n",
    "    detail_columns = [col for col in detail_columns if col in df.columns]\n",
    "    display(_rename_columns(df[detail_columns]))\n",
    "\n",
    "    for config in SCATTER_PLOTS:\n",
    "        _plot_scatter(df, config)\n",
    "\n",
    "    for x_key, y_key, title in ADDITIONAL_SCATTER_PREFIX_PAIRS:\n",
    "        if x_key in df.columns and y_key in df.columns:\n",
    "            _plot_scatter(\n",
    "                df,\n",
    "                {\n",
    "                    'title': title,\n",
    "                    'x': x_key,\n",
    "                    'y': y_key,\n",
    "                    'xlabel': _column_display_name(x_key),\n",
    "                    'ylabel': _column_display_name(y_key),\n",
    "                },\n",
    "            )\n",
    "\n",
    "    for config in BAR_PLOTS:\n",
    "        _plot_bar(\n",
    "            df,\n",
    "            columns=config['columns'],\n",
    "            title=config['title'],\n",
    "            ylabel=config['ylabel'],\n",
    "            stacked=False,\n",
    "            sort_by=config.get('sort_by'),\n",
    "        )\n",
    "\n",
    "    for config in STACKED_BAR_CONFIG:\n",
    "        columns = sorted(col for col in df.columns if col.startswith(config['prefix']))\n",
    "        if columns:\n",
    "            _plot_bar(\n",
    "                df,\n",
    "                columns=columns,\n",
    "                title=config['title'],\n",
    "                ylabel=config['ylabel'],\n",
    "                stacked=True,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Plot '{config['title']}' skipped: no columns with prefix {config['prefix']!r}.\")\n",
    "\n",
    "    for config in HISTOGRAMS:\n",
    "        _plot_histogram(df, column=config['column'], title=config['title'], xlabel=config['xlabel'])\n",
    "\n",
    "render_worker_report()\n",
    "render_db0_eu_analysis()\n",
    "render_db0_sample_timeseries()\n",
    "render_bitmap_heatmaps()\n",
    "render_db0_eu_analysis()\n",
    "render_db0_sample_timeseries()\n",
    "render_bitmap_heatmaps()\n",
    "render_db0_eu_analysis()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpmnpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}