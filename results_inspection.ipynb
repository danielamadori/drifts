{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55041dcae764f7ce",
   "metadata": {},
   "source": [
    "# Results archive inspection\n",
    "\n",
    "This notebook automatically inspects every `.zip` file stored in the `results` directory.\n",
    "It parses the filename of each archive to extract useful metadata, relies on the included\n",
    "`manifest.json` file to map Redis database dumps to their logical meaning, and previews\n",
    "all extracted files directly below. Large files are truncated to the first bytes so the\n",
    "notebook stays responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49976e1c760312e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:52:14.692624Z",
     "start_time": "2025-10-23T16:52:14.687601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ZIP archives:\n",
      "[0] ECG200_-1_false_0.zip\n",
      "[1] MelbournePedestrian_1_false_0.zip\n",
      "[2] MiddlePhalanxOutlineCorrect_0_false_0.zip\n",
      "Current selection: [0] ECG200_-1_false_0.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "import base64\n",
    "import binascii\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "HOST_BASE = 9\n",
    "DB_LABELS = {\n",
    "    0: \"DATA\",\n",
    "    1: \"CAN\",\n",
    "    2: \"R\",\n",
    "    3: \"NR\",\n",
    "    4: \"CAR\",\n",
    "    5: \"AR\",\n",
    "    6: \"GP\",\n",
    "    7: \"BP\",\n",
    "    8: \"PR\",\n",
    "    9: \"AP\",\n",
    "    10: \"LOGS\",\n",
    "}\n",
    "MAX_FULL_BYTES = 200_000\n",
    "MAX_PREVIEW_BYTES = 10_000\n",
    "\n",
    "zip_paths = sorted(RESULTS_DIR.glob(\"*.zip\"))\n",
    "zip_names = [path.name for path in zip_paths]\n",
    "zip_inventory = {\n",
    "    \"results_dir\": str(RESULTS_DIR),\n",
    "    \"count\": len(zip_paths),\n",
    "    \"found\": bool(zip_paths),\n",
    "    \"paths\": [str(path) for path in zip_paths],\n",
    "    \"names\": zip_names,\n",
    "}\n",
    "\n",
    "env_selected_index = os.environ.get(\"RESULTS_SELECTED_ZIP_INDEX\")\n",
    "env_selected_zip = os.environ.get(\"RESULTS_SELECTED_ZIP\")\n",
    "selected_zip_index = None\n",
    "selected_zip_name = None\n",
    "\n",
    "if env_selected_index is not None:\n",
    "    try:\n",
    "        candidate_index = int(env_selected_index)\n",
    "    except ValueError:\n",
    "        candidate_index = None\n",
    "    if isinstance(candidate_index, int) and 0 <= candidate_index < len(zip_paths):\n",
    "        selected_zip_index = candidate_index\n",
    "        selected_zip_name = zip_names[selected_zip_index]\n",
    "if selected_zip_name is None and env_selected_zip in zip_names:\n",
    "    selected_zip_name = env_selected_zip\n",
    "    selected_zip_index = zip_names.index(selected_zip_name)\n",
    "if selected_zip_name is None and zip_names:\n",
    "    selected_zip_index = 0\n",
    "    selected_zip_name = zip_names[0]\n",
    "\n",
    "selected_zip_path = zip_paths[selected_zip_index] if selected_zip_index is not None else None\n",
    "\n",
    "if zip_names:\n",
    "    print(\"Available ZIP archives:\")\n",
    "    for index, name in enumerate(zip_names):\n",
    "        print(f\"[{index}] {name}\")\n",
    "    user_choice = input(\"Select ZIP by index or name (press Enter to keep current selection): \").strip()\n",
    "    if user_choice:\n",
    "        resolved_index = None\n",
    "        try:\n",
    "            resolved_index = int(user_choice)\n",
    "        except ValueError:\n",
    "            resolved_index = None\n",
    "        if resolved_index is not None and 0 <= resolved_index < len(zip_paths):\n",
    "            selected_zip_index = resolved_index\n",
    "            selected_zip_name = zip_names[selected_zip_index]\n",
    "        elif user_choice in zip_names:\n",
    "            selected_zip_name = user_choice\n",
    "            selected_zip_index = zip_names.index(selected_zip_name)\n",
    "        else:\n",
    "            print(\"Invalid selection, keeping previous choice.\")\n",
    "    selected_zip_path = zip_paths[selected_zip_index] if selected_zip_index is not None else None\n",
    "    if selected_zip_name is not None and selected_zip_path is not None:\n",
    "        print(f\"Current selection: [{selected_zip_index}] {selected_zip_name}\")\n",
    "    else:\n",
    "        print(\"Current selection: none\")\n",
    "else:\n",
    "    print(\"No ZIP archives found in results directory.\")\n",
    "\n",
    "zip_inventory[\"selection\"] = {\n",
    "    \"name\": selected_zip_name,\n",
    "    \"index\": selected_zip_index,\n",
    "    \"path\": str(selected_zip_path) if selected_zip_path else None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb975d1e10ba092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:52:14.750483Z",
     "start_time": "2025-10-23T16:52:14.726038Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_bytes(size):\n",
    "    \"\"\"Return a human-readable representation of a file size.\"\"\"\n",
    "    if size is None:\n",
    "        return \"-\"\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    value = float(size)\n",
    "    for unit in units:\n",
    "        if value < 1024 or unit == units[-1]:\n",
    "            if unit == \"B\":\n",
    "                return f\"{int(value)} {unit}\"\n",
    "            return f\"{value:.2f} {unit}\"\n",
    "        value /= 1024\n",
    "    return f\"{value:.2f} B\"\n",
    "\n",
    "\n",
    "def parse_zip_metadata(zip_path):\n",
    "    \"\"\"Extract dataset, class, completion flag, and host numbers from the archive name.\"\"\"\n",
    "    dataset, class_name, completion_flag, host_fragment = zip_path.stem.rsplit(\"_\", 3)\n",
    "    try:\n",
    "        host_offset = int(host_fragment)\n",
    "        host_id = host_offset + HOST_BASE\n",
    "    except ValueError:\n",
    "        host_offset = None\n",
    "        host_id = None\n",
    "    flag_lower = completion_flag.lower()\n",
    "    if flag_lower in {\"true\", \"false\"}:\n",
    "        is_completed = flag_lower == \"true\"\n",
    "    else:\n",
    "        is_completed = None\n",
    "    size_bytes = zip_path.stat().st_size\n",
    "    return {\n",
    "        \"zip_path\": str(zip_path),\n",
    "        \"zip_name\": zip_path.name,\n",
    "        \"dataset\": dataset,\n",
    "        \"class\": class_name,\n",
    "        \"completion_raw\": completion_flag,\n",
    "        \"is_completed\": is_completed,\n",
    "        \"size_bytes\": size_bytes,\n",
    "        \"size_text\": format_bytes(size_bytes),\n",
    "        \"host_offset\": host_offset,\n",
    "        \"host_id\": host_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_root_prefix(archive, zip_path):\n",
    "    \"\"\"Guess the common directory prefix used inside the archive.\"\"\"\n",
    "    stem_prefix = f\"{zip_path.stem}/\"\n",
    "    has_stem = any(\n",
    "        info.filename.startswith(stem_prefix)\n",
    "        for info in archive.infolist()\n",
    "        if not info.is_dir()\n",
    "    )\n",
    "    if has_stem:\n",
    "        return stem_prefix\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def resolve_manifest(archive, zip_path):\n",
    "    \"\"\"Return the manifest data together with the prefix used inside the archive.\"\"\"\n",
    "    candidates = []\n",
    "    stem_prefix = f\"{zip_path.stem}/\"\n",
    "    candidates.append(stem_prefix)\n",
    "    for info in archive.infolist():\n",
    "        if info.is_dir():\n",
    "            dirname = info.filename\n",
    "            if dirname.startswith(\"__MACOSX/\"):\n",
    "                continue\n",
    "            if not dirname.endswith(\"/\"):\n",
    "                dirname += \"/\"\n",
    "            candidates.append(dirname)\n",
    "    candidates.append(\"\")\n",
    "    seen = set()\n",
    "    for prefix in candidates:\n",
    "        if prefix in seen:\n",
    "            continue\n",
    "        seen.add(prefix)\n",
    "        manifest_path = f\"{prefix}manifest.json\"\n",
    "        try:\n",
    "            with archive.open(manifest_path) as manifest_file:\n",
    "                manifest = json.load(manifest_file)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        else:\n",
    "            return prefix, manifest\n",
    "    raise KeyError(\"manifest.json not found\")\n",
    "\n",
    "\n",
    "class DumpDecodeError(RuntimeError):\n",
    "    \"\"\"Generic error raised while decoding a Redis DUMP payload.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DumpSections:\n",
    "    payload: bytes\n",
    "    version: int\n",
    "    checksum: bytes\n",
    "\n",
    "\n",
    "class _LengthEncoding:\n",
    "    __slots__ = (\"value\", \"encoding\")\n",
    "\n",
    "    def __init__(self, value=None, encoding=None):\n",
    "        self.value = value\n",
    "        self.encoding = encoding\n",
    "\n",
    "\n",
    "RDB_ENCODING_INT8 = 0\n",
    "RDB_ENCODING_INT16 = 1\n",
    "RDB_ENCODING_INT32 = 2\n",
    "RDB_ENCODING_LZF = 3\n",
    "\n",
    "\n",
    "def split_dump_sections(raw: bytes) -> DumpSections:\n",
    "    \"\"\"Split payload, RDB version, and checksum from a Redis dump.\"\"\"\n",
    "    if len(raw) < 10:\n",
    "        raise DumpDecodeError(\"DUMP payload is too short to contain metadata\")\n",
    "    checksum = raw[-8:]\n",
    "    version_bytes = raw[-10:-8]\n",
    "    version = int.from_bytes(version_bytes, \"little\", signed=False)\n",
    "    payload = raw[:-10]\n",
    "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
    "\n",
    "\n",
    "def _read_length_info(buffer: bytes, offset: int):\n",
    "    if offset >= len(buffer):\n",
    "        raise DumpDecodeError(\"Offset out of range while reading length\")\n",
    "    first = buffer[offset]\n",
    "    prefix = first >> 6\n",
    "    if prefix == 0:\n",
    "        length = first & 0x3F\n",
    "        return _LengthEncoding(length), offset + 1\n",
    "    if prefix == 1:\n",
    "        if offset + 1 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 14-bit encoded length\")\n",
    "        second = buffer[offset + 1]\n",
    "        length = ((first & 0x3F) << 8) | second\n",
    "        return _LengthEncoding(length), offset + 2\n",
    "    if prefix == 2:\n",
    "        if offset + 4 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded length\")\n",
    "        length = int.from_bytes(buffer[offset + 1 : offset + 5], \"big\", signed=False)\n",
    "        return _LengthEncoding(length), offset + 5\n",
    "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
    "\n",
    "\n",
    "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
    "    \"\"\"Minimal implementation of the LZF decompression used by Redis.\"\"\"\n",
    "    output = bytearray()\n",
    "    idx = 0\n",
    "    data_len = len(data)\n",
    "    while idx < data_len:\n",
    "        ctrl = data[idx]\n",
    "        idx += 1\n",
    "        if ctrl < 32:\n",
    "            literal_len = ctrl + 1\n",
    "            if idx + literal_len > data_len:\n",
    "                raise DumpDecodeError(\"Truncated literal LZF sequence\")\n",
    "            output.extend(data[idx : idx + literal_len])\n",
    "            idx += literal_len\n",
    "        else:\n",
    "            length = ctrl >> 5\n",
    "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
    "            if length == 7:\n",
    "                if idx >= data_len:\n",
    "                    raise DumpDecodeError(\"Truncated LZF sequence while extending length\")\n",
    "                length += data[idx]\n",
    "                idx += 1\n",
    "            if idx >= data_len:\n",
    "                raise DumpDecodeError(\"Truncated LZF sequence while resolving reference\")\n",
    "            ref_offset -= data[idx]\n",
    "            idx += 1\n",
    "            length += 2\n",
    "            if ref_offset < 0:\n",
    "                raise DumpDecodeError(\"Negative LZF reference\")\n",
    "            for _ in range(length):\n",
    "                if ref_offset >= len(output):\n",
    "                    raise DumpDecodeError(\"LZF reference out of range\")\n",
    "                output.append(output[ref_offset])\n",
    "                ref_offset += 1\n",
    "    if len(output) != expected_length:\n",
    "        raise DumpDecodeError(\n",
    "            f\"Unexpected decompressed length: expected {expected_length}, got {len(output)}\"\n",
    "        )\n",
    "    return bytes(output)\n",
    "\n",
    "\n",
    "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int):\n",
    "    if encoding == RDB_ENCODING_INT8:\n",
    "        if offset >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 8-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 1], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 1\n",
    "    if encoding == RDB_ENCODING_INT16:\n",
    "        if offset + 2 > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 16-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 2], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 2\n",
    "    if encoding == RDB_ENCODING_INT32:\n",
    "        if offset + 4 > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 4], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 4\n",
    "    if encoding == RDB_ENCODING_LZF:\n",
    "        compressed_len_info, next_offset = _read_length_info(buffer, offset)\n",
    "        data_len_info, data_offset = _read_length_info(buffer, next_offset)\n",
    "        if compressed_len_info.value is None or data_len_info.value is None:\n",
    "            raise DumpDecodeError(\"Invalid LZF length encoding\")\n",
    "        end = data_offset + compressed_len_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        compressed = buffer[data_offset:end]\n",
    "        decompressed = lzf_decompress(compressed, data_len_info.value)\n",
    "        return decompressed, end\n",
    "    raise DumpDecodeError(\"Unknown string encoding\")\n",
    "\n",
    "\n",
    "def _read_encoded_string(buffer: bytes, offset: int):\n",
    "    length_info, next_offset = _read_length_info(buffer, offset)\n",
    "    if length_info.encoding is None:\n",
    "        end = next_offset + length_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        return buffer[next_offset:end], end\n",
    "    return _decode_special_encoding(buffer, next_offset, length_info.encoding)\n",
    "\n",
    "\n",
    "def decode_string_from_dump(raw: bytes) -> bytes:\n",
    "    sections = split_dump_sections(raw)\n",
    "    payload = sections.payload\n",
    "    if not payload:\n",
    "        raise DumpDecodeError(\"Empty payload\")\n",
    "    object_type = payload[0]\n",
    "    if object_type != 0:\n",
    "        raise DumpDecodeError(f\"Non-string object type: {object_type}\")\n",
    "    value, _ = _read_encoded_string(payload, 1)\n",
    "    return value\n",
    "\n",
    "\n",
    "def decode_bytes(value: str) -> bytes:\n",
    "    if not isinstance(value, str):\n",
    "        raise DumpDecodeError(\"Encoded value must be a string\")\n",
    "    try:\n",
    "        return base64.b64decode(value.encode(\"ascii\"))\n",
    "    except (UnicodeEncodeError, binascii.Error) as exc:\n",
    "        raise DumpDecodeError(f\"Invalid base64 payload: {exc}\") from exc\n",
    "\n",
    "\n",
    "def decode_key(entry):\n",
    "    return decode_bytes(entry[\"key\"])\n",
    "\n",
    "\n",
    "def text_preview(value: bytes, limit: int = 120) -> str:\n",
    "    text = value.decode(\"utf-8\", errors=\"replace\")\n",
    "    if len(text) > limit:\n",
    "        return text[: limit - 1] + \".\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def try_decode_value(entry):\n",
    "    value_info = dict(entry.get(\"value\") or {})\n",
    "    data_b64 = value_info.get(\"data\")\n",
    "    if not data_b64:\n",
    "        return \"<no value>\", value_info\n",
    "    try:\n",
    "        raw = decode_bytes(data_b64)\n",
    "    except DumpDecodeError as exc:\n",
    "        value_info[\"decode_error\"] = str(exc)\n",
    "        return \"<invalid base64>\", value_info\n",
    "    details = {\n",
    "        \"dump_size\": len(raw),\n",
    "    }\n",
    "    try:\n",
    "        sections = split_dump_sections(raw)\n",
    "        details[\"rdb_version\"] = sections.version\n",
    "        details[\"checksum\"] = sections.checksum.hex()\n",
    "    except DumpDecodeError as exc:\n",
    "        details[\"dump_error\"] = str(exc)\n",
    "        return \"<invalid dump>\", details\n",
    "    if entry.get(\"type\") == \"string\":\n",
    "        try:\n",
    "            decoded = decode_string_from_dump(raw)\n",
    "        except DumpDecodeError as exc:\n",
    "            details[\"decode_error\"] = str(exc)\n",
    "            return \"<string not decoded>\", details\n",
    "        details[\"decoded_bytes\"] = decoded\n",
    "        preview = text_preview(decoded)\n",
    "        return preview, details\n",
    "    return f\"<{entry.get('type')} - {len(sections.payload)} bytes>\", details\n",
    "\n",
    "\n",
    "def shorten_text(text: str, limit: int = 600) -> str:\n",
    "    sanitized = text.replace(\"````\", \"``` `\")\n",
    "    if len(sanitized) > limit:\n",
    "        return sanitized[: limit - 1] + \".\"\n",
    "    return sanitized\n",
    "\n",
    "\n",
    "def summarise_backup_entries(entries, limit: int = 3):\n",
    "    if not entries:\n",
    "        return [\"> No entries stored in this backup.\"]\n",
    "    lines = []\n",
    "    for index, entry in enumerate(entries[:limit], start=1):\n",
    "        try:\n",
    "            key_bytes = decode_key(entry)\n",
    "            key_text = key_bytes.decode(\"utf-8\", errors=\"replace\") or \"<empty key>\"\n",
    "        except (KeyError, DumpDecodeError) as exc:\n",
    "            key_text = f\"<unable to decode key: {exc}>\"\n",
    "        preview, details = try_decode_value(entry)\n",
    "        entry_type = entry.get(\"type\", \"unknown\")\n",
    "        ttl = entry.get(\"pttl\")\n",
    "        ttl_text = f\"{ttl}\" if isinstance(ttl, int) else \"persistent\"\n",
    "        lines.append(f\"Entry {index}: key `{key_text}`\")\n",
    "        lines.append(f\"Type: `{entry_type}`; TTL (ms): `{ttl_text}`\")\n",
    "        decoded_bytes = details.get(\"decoded_bytes\")\n",
    "        error = details.get(\"decode_error\") or details.get(\"dump_error\")\n",
    "        if isinstance(decoded_bytes, (bytes, bytearray)):\n",
    "            text_value = decoded_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "            lines.append(shorten_text(text_value))\n",
    "        else:\n",
    "            lines.append(shorten_text(str(preview)))\n",
    "        if error:\n",
    "            lines.append(f\"Warning: {error}\")\n",
    "    if len(entries) > limit:\n",
    "        lines.append(f\"Additional entries not shown: {len(entries) - limit}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def build_backup_preview(data):\n",
    "    entries = data.get(\"entries\") or []\n",
    "    metadata = data.get(\"metadata\") or {}\n",
    "    return {\n",
    "        \"key_count\": metadata.get(\"key_count\", len(entries)),\n",
    "        \"created_at\": metadata.get(\"created_at_utc\"),\n",
    "        \"source\": metadata.get(\"source\") or {},\n",
    "        \"type_summary\": metadata.get(\"type_summary\") or {},\n",
    "        \"sample_entries\": summarise_backup_entries(entries),\n",
    "    }\n",
    "\n",
    "\n",
    "def try_render_backup_preview(relative_name: str, payload: bytes):\n",
    "    try:\n",
    "        text = payload.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    if not isinstance(data, dict):\n",
    "        return None\n",
    "    if \"entries\" not in data or \"metadata\" not in data:\n",
    "        return None\n",
    "    return build_backup_preview(data)\n",
    "\n",
    "\n",
    "def get_relative_member_name(info, prefix):\n",
    "    member_name = info.filename\n",
    "    if prefix and member_name.startswith(prefix):\n",
    "        return member_name[len(prefix):]\n",
    "    return member_name\n",
    "\n",
    "\n",
    "def is_logs_entry(relative_name):\n",
    "    normalized = relative_name.replace('\\\\', '/').lstrip('./')\n",
    "    return normalized == 'logs' or normalized.startswith('logs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d9d26be28752f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:52:15.624863Z",
     "start_time": "2025-10-23T16:52:14.759766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB 0 entries for ECG200_-1_false_0.zip:\n",
      "EU (type=string, ttl=persistent)\n",
      "time series (Endpoint Univers) with 94 features; lengths 5, 6, 7, 8, 10\n",
      "    t_00: len=7, sample=[-∞, 0.487756, 1.03078…]\n",
      "    t_01: len=5, sample=[-∞, 0.340857, 0.544473…]\n",
      "    t_02: len=6, sample=[-∞, 0.987706, 1.01131…]\n",
      "    t_03: len=8, sample=[-∞, -0.69702, 0.5476…]\n",
      "    t_04: len=10, sample=[-∞, -0.494845, 0.308124…]\n",
      "    …\n",
      "RF (type=string, ttl=persistent)\n",
      "random forest with 233 trees\n",
      "    tree 0: root feature t_45, threshold 0.650059\n",
      "    tree 1: root feature t_29, threshold -1.61099\n",
      "    tree 2: root feature t_94, threshold 0.439568\n",
      "    tree 3: root feature t_43, threshold 0.555605\n",
      "    tree 4: root feature t_29, threshold -1.56959\n",
      "    …\n",
      "RF_OPTIMIZATION_RESULTS (type=string, ttl=persistent)\n",
      "RF optimisation results (best CV 0.880, test 0.810, n_iter 50)\n",
      "    best_params.bootstrap = True\n",
      "    best_params.ccp_alpha = 0.05\n",
      "    best_params.criterion = entropy\n",
      "    best_params.max_depth = 46\n",
      "    best_params.max_features = None\n",
      "    …\n",
      "    used_test_for_validation: False\n",
      "    timestamp: 2025-10-21T15:36:59.258269\n",
      "TRAINING_SET (type=string, ttl=persistent)\n",
      "object with 7 keys: X_train, y_train, feature_names, dataset_name, n_samples…\n",
      "    X_train: array(100)\n",
      "    y_train: array(100)\n",
      "    feature_names: array(96)\n",
      "    …\n",
      "label (type=string, ttl=persistent)\n",
      "-1\n",
      "summary_ECG200_-1 (type=string, ttl=persistent)\n",
      "object with 10 keys: dataset_name, target_class_label, total_samples_processed, total_test_samples, samples_with_target_label…\n",
      "    dataset_name: 'ECG200'\n",
      "    target_class_label: '-1'\n",
      "    total_samples_processed: 29\n",
      "    …\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "READ_JSON_LIMIT_BYTES = 5_000_000\n",
    "\n",
    "def collect_archive_data(zip_path):\n",
    "    meta = parse_zip_metadata(zip_path)\n",
    "    result = {\n",
    "        'zip_name': zip_path.name,\n",
    "        'zip_path': str(zip_path),\n",
    "        'metadata': meta,\n",
    "        'manifest': None,\n",
    "        'manifest_prefix': '',\n",
    "        'db_overview': [],\n",
    "        'members': [],\n",
    "        'backups': {},\n",
    "    }\n",
    "    with zipfile.ZipFile(zip_path) as archive:\n",
    "        try:\n",
    "            prefix, manifest = resolve_manifest(archive, zip_path)\n",
    "            result['manifest'] = manifest\n",
    "            result['manifest_prefix'] = prefix\n",
    "        except Exception:\n",
    "            prefix = detect_root_prefix(archive, zip_path)\n",
    "            manifest = None\n",
    "            result['manifest_prefix'] = prefix\n",
    "        if manifest:\n",
    "            files_map = manifest.get('files', {})\n",
    "            dbs = manifest.get('databases', [])\n",
    "            for db_index in dbs:\n",
    "                file_name = files_map.get(str(db_index))\n",
    "                if not file_name:\n",
    "                    continue\n",
    "                archive_name = f\"{prefix}{file_name}\"\n",
    "                try:\n",
    "                    size = archive.getinfo(archive_name).file_size\n",
    "                except KeyError:\n",
    "                    size = None\n",
    "                result['db_overview'].append({\n",
    "                    'db_index': db_index,\n",
    "                    'label': DB_LABELS.get(db_index, 'Unknown'),\n",
    "                    'json_file': file_name,\n",
    "                    'size_bytes': size,\n",
    "                    'size_text': format_bytes(size) if size is not None else None,\n",
    "                })\n",
    "        members = sorted((info for info in archive.infolist() if not info.is_dir()), key=lambda info: info.filename)\n",
    "        for info in members:\n",
    "            relative = get_relative_member_name(info, prefix)\n",
    "            if is_logs_entry(relative):\n",
    "                continue\n",
    "            size = info.file_size\n",
    "            entry = {\n",
    "                'relative_name': relative,\n",
    "                'size_bytes': size,\n",
    "                'size_text': format_bytes(size),\n",
    "                'json_data': None,\n",
    "                'json_truncated': False,\n",
    "                'text_preview': None,\n",
    "                'backup_preview': None,\n",
    "            }\n",
    "            read_entire = size <= MAX_FULL_BYTES or relative.endswith('.json')\n",
    "            with archive.open(info.filename) as handle:\n",
    "                payload = handle.read() if read_entire else handle.read(MAX_PREVIEW_BYTES)\n",
    "            if relative.endswith('.json') and (size is None or size <= READ_JSON_LIMIT_BYTES):\n",
    "                try:\n",
    "                    text = payload.decode('utf-8')\n",
    "                    data = json.loads(text)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "                if data is not None:\n",
    "                    entry['json_data'] = data\n",
    "                    preview = try_render_backup_preview(relative, payload)\n",
    "                    if preview is not None:\n",
    "                        entry['backup_preview'] = preview\n",
    "                        result['backups'][relative] = data\n",
    "                else:\n",
    "                    entry['text_preview'] = payload.decode('utf-8', errors='replace')[:1000]\n",
    "            else:\n",
    "                entry['json_truncated'] = relative.endswith('.json') and (size is not None and size > READ_JSON_LIMIT_BYTES)\n",
    "                try:\n",
    "                    entry['text_preview'] = payload.decode('utf-8', errors='replace')[:1000]\n",
    "                except Exception:\n",
    "                    entry['text_preview'] = None\n",
    "            result['members'].append(entry)\n",
    "    return result\n",
    "\n",
    "def _format_number(value: float) -> str:\n",
    "    if math.isinf(value):\n",
    "        return '∞' if value > 0 else '-∞'\n",
    "    if math.isnan(value):\n",
    "        return 'NaN'\n",
    "    if abs(value) >= 1_000:\n",
    "        return f\"{value:.3g}\"\n",
    "    return f\"{value:.6g}\"\n",
    "\n",
    "def _short_text(text: str, limit: int = 120) -> str:\n",
    "    return text if len(text) <= limit else text[: limit - 1] + '…'\n",
    "\n",
    "def _inline_summary(value, depth: int = 0) -> str:\n",
    "    if isinstance(value, dict):\n",
    "        return f\"object({len(value)})\"\n",
    "    if isinstance(value, list):\n",
    "        return f\"array({len(value)})\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        return _format_number(value)\n",
    "    if isinstance(value, str):\n",
    "        return repr(_short_text(value))\n",
    "    if value is None:\n",
    "        return 'null'\n",
    "    return repr(value)\n",
    "\n",
    "def _extract_metadata_from_chunk(text: str):\n",
    "    key = '\"metadata\"'\n",
    "    idx = text.find(key)\n",
    "    if idx == -1:\n",
    "        return {}\n",
    "    brace_start = text.find('{', idx)\n",
    "    if brace_start == -1:\n",
    "        return {}\n",
    "    depth = 0\n",
    "    for pos in range(brace_start, len(text)):\n",
    "        char = text[pos]\n",
    "        if char == '{':\n",
    "            depth += 1\n",
    "        elif char == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                snippet = text[brace_start : pos + 1]\n",
    "                try:\n",
    "                    return json.loads(snippet)\n",
    "                except json.JSONDecodeError:\n",
    "                    return {}\n",
    "    return {}\n",
    "\n",
    "def _fetch_backup_metadata(zip_path, member_name, size_limit=2_000_000):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path) as archive:\n",
    "            with archive.open(member_name) as handle:\n",
    "                chunk = handle.read(size_limit)\n",
    "    except KeyError:\n",
    "        return {}\n",
    "    text = chunk.decode('utf-8', errors='replace')\n",
    "    return _extract_metadata_from_chunk(text)\n",
    "\n",
    "def _describe_json(value, depth: int = 0):\n",
    "    if isinstance(value, dict):\n",
    "        keys = list(value.keys())\n",
    "        preview_keys = keys[:5]\n",
    "        headline = f\"object with {len(keys)} keys: {', '.join(preview_keys)}\" + (\"…\" if len(keys) > 5 else '')\n",
    "        detail_lines = []\n",
    "        for key in keys[:3]:\n",
    "            detail_lines.append(f\"{key}: {_inline_summary(value[key], depth + 1)}\")\n",
    "        if len(keys) > 3:\n",
    "            detail_lines.append('…')\n",
    "        return headline, detail_lines\n",
    "    if isinstance(value, list):\n",
    "        length = len(value)\n",
    "        headline = f\"array with {length} items\"\n",
    "        sample = [_inline_summary(item, depth + 1) for item in value[:3]]\n",
    "        detail_lines = []\n",
    "        if sample:\n",
    "            detail = ', '.join(sample)\n",
    "            detail_lines.append(f\"sample: {detail}{'…' if length > 3 else ''}\")\n",
    "        return headline, detail_lines\n",
    "    return _inline_summary(value, depth), []\n",
    "\n",
    "def summarise_endpoint_univers(series_map):\n",
    "    if not isinstance(series_map, dict):\n",
    "        return 'time series (unexpected structure)', []\n",
    "    feature_names = sorted(series_map.keys())\n",
    "    lengths = []\n",
    "    details = []\n",
    "    for name in feature_names[:5]:\n",
    "        points = series_map.get(name)\n",
    "        if isinstance(points, list):\n",
    "            length = len(points)\n",
    "            lengths.append(length)\n",
    "            sample = []\n",
    "            for value in points[:3]:\n",
    "                if isinstance(value, (int, float)):\n",
    "                    sample.append(_format_number(value))\n",
    "                else:\n",
    "                    sample.append(str(value))\n",
    "            preview = ', '.join(sample)\n",
    "            details.append(f\"{name}: len={length}, sample=[{preview}{'…' if len(points) > 3 else ''}]\")\n",
    "        else:\n",
    "            details.append(f\"{name}: unexpected {type(points).__name__}\")\n",
    "    if len(series_map) > 5:\n",
    "        details.append('…')\n",
    "    if lengths:\n",
    "        distinct_lengths = sorted(set(lengths))\n",
    "        if len(distinct_lengths) == 1:\n",
    "            headline = f\"time series (Endpoint Univers) with {len(series_map)} features; length {distinct_lengths[0]}\"\n",
    "        else:\n",
    "            headline = (\n",
    "                f\"time series (Endpoint Univers) with {len(series_map)} features; \"\n",
    "                + f\"lengths {', '.join(str(l) for l in distinct_lengths)}\"\n",
    "            )\n",
    "    else:\n",
    "        headline = f\"time series (Endpoint Univers) with {len(series_map)} features\"\n",
    "    return headline, details\n",
    "\n",
    "def summarise_random_forest(trees):\n",
    "    if not isinstance(trees, list):\n",
    "        return 'random forest (unexpected structure)', []\n",
    "    count = len(trees)\n",
    "    details = []\n",
    "    for tree in trees[:5]:\n",
    "        if isinstance(tree, dict):\n",
    "            tree_id = tree.get('tree_id')\n",
    "            feature = tree.get('feature')\n",
    "            value = tree.get('value')\n",
    "            feature_text = feature if feature is not None else '?'\n",
    "            threshold = _format_number(value) if isinstance(value, (int, float)) else str(value)\n",
    "            prefix = f\"tree {tree_id}\" if tree_id is not None else 'tree'\n",
    "            details.append(f\"{prefix}: root feature {feature_text}, threshold {threshold}\")\n",
    "        else:\n",
    "            details.append(f\"tree: unexpected {type(tree).__name__}\")\n",
    "    if count > 5:\n",
    "        details.append('…')\n",
    "    return f\"random forest with {count} trees\", details\n",
    "\n",
    "def summarise_rf_optimization(result):\n",
    "    if not isinstance(result, dict):\n",
    "        return 'RF optimisation summary (unexpected structure)', []\n",
    "    best = result.get('best_params') or {}\n",
    "    best_keys = list(best.keys())\n",
    "    headline_parts = []\n",
    "    cv_score = result.get('best_cv_score')\n",
    "    test_score = result.get('test_score')\n",
    "    if isinstance(cv_score, (int, float)):\n",
    "        headline_parts.append(f\"best CV {cv_score:.3f}\")\n",
    "    if isinstance(test_score, (int, float)):\n",
    "        headline_parts.append(f\"test {test_score:.3f}\")\n",
    "    iter_count = result.get('n_iter')\n",
    "    if isinstance(iter_count, int):\n",
    "        headline_parts.append(f\"n_iter {iter_count}\")\n",
    "    headline = \"RF optimisation results\"\n",
    "    if headline_parts:\n",
    "        headline += \" (\" + ', '.join(headline_parts) + \")\"\n",
    "    details = []\n",
    "    for param in best_keys[:5]:\n",
    "        details.append(f\"best_params.{param} = {best[param]}\")\n",
    "    if len(best_keys) > 5:\n",
    "        details.append('…')\n",
    "    used_test = result.get('used_test_for_validation')\n",
    "    if isinstance(used_test, bool):\n",
    "        details.append(f\"used_test_for_validation: {used_test}\")\n",
    "    timestamp = result.get('timestamp')\n",
    "    if timestamp:\n",
    "        details.append(f\"timestamp: {timestamp}\")\n",
    "    return headline, details\n",
    "\n",
    "def summarise_entry(key, value_json, value_text, skip_sample_keys=True):\n",
    "    if skip_sample_keys and key.startswith('sample_'):\n",
    "        return None, None\n",
    "    if key == 'EU' and value_json is not None:\n",
    "        return summarise_endpoint_univers(value_json)\n",
    "    if key == 'RF' and value_json is not None:\n",
    "        return summarise_random_forest(value_json)\n",
    "    if key == 'RF_OPTIMIZATION_RESULTS' and value_json is not None:\n",
    "        return summarise_rf_optimization(value_json)\n",
    "    if value_json is not None:\n",
    "        return _describe_json(value_json)\n",
    "    return _short_text(value_text), []\n",
    "\n",
    "def summarise_entry_generic(key, value_json, value_text):\n",
    "    if key == 'EU' and value_json is not None:\n",
    "        return summarise_endpoint_univers(value_json)\n",
    "    if key == 'RF' and value_json is not None:\n",
    "        return summarise_random_forest(value_json)\n",
    "    if key == 'RF_OPTIMIZATION_RESULTS' and value_json is not None:\n",
    "        return summarise_rf_optimization(value_json)\n",
    "    if value_json is not None:\n",
    "        return _describe_json(value_json)\n",
    "    return _short_text(value_text), []\n",
    "\n",
    "archives_metadata = [parse_zip_metadata(path) for path in zip_paths]\n",
    "archives_data = [collect_archive_data(path) for path in zip_paths]\n",
    "manifests_by_archive = {item['zip_name']: item['manifest'] for item in archives_data}\n",
    "manifest_prefix_by_archive = {item['zip_name']: item.get('manifest_prefix', '') for item in archives_data}\n",
    "backups_by_archive = {item['zip_name']: item['backups'] for item in archives_data}\n",
    "selected_archive_data = next((item for item in archives_data if item['zip_name'] == selected_zip_name), None)\n",
    "selected_manifest = manifests_by_archive.get(selected_zip_name)\n",
    "selected_manifest_prefix = manifest_prefix_by_archive.get(selected_zip_name, '')\n",
    "selected_backups = backups_by_archive.get(selected_zip_name)\n",
    "\n",
    "selected_db0_file_name = None\n",
    "if selected_manifest:\n",
    "    files_map = selected_manifest.get('files', {}) or {}\n",
    "    selected_db0_file_name = files_map.get('0')\n",
    "\n",
    "selected_db0_backup = None\n",
    "if selected_backups and selected_db0_file_name:\n",
    "    selected_db0_backup = selected_backups.get(selected_db0_file_name)\n",
    "\n",
    "selected_db0_entries = []\n",
    "selected_db0_values = []\n",
    "if selected_db0_backup:\n",
    "    selected_db0_entries = selected_db0_backup.get('entries') or []\n",
    "    for entry in selected_db0_entries:\n",
    "        try:\n",
    "            key_bytes = decode_key(entry)\n",
    "            key_text = key_bytes.decode('utf-8', errors='replace')\n",
    "        except Exception as exc:\n",
    "            key_text = f'<unable to decode key: {exc}>'\n",
    "        preview, details = try_decode_value(entry)\n",
    "        value_bytes = details.get('decoded_bytes') if isinstance(details, dict) else None\n",
    "        if isinstance(value_bytes, (bytes, bytearray)):\n",
    "            value_text = value_bytes.decode('utf-8', errors='replace')\n",
    "        else:\n",
    "            value_text = str(preview)\n",
    "        value_json = None\n",
    "        if isinstance(value_text, str):\n",
    "            try:\n",
    "                value_json = json.loads(value_text)\n",
    "            except Exception:\n",
    "                value_json = None\n",
    "        selected_db0_values.append({\n",
    "            'key': key_text,\n",
    "            'type': entry.get('type'),\n",
    "            'ttl_ms': entry.get('pttl'),\n",
    "            'value_text': value_text,\n",
    "            'value_bytes': value_bytes,\n",
    "            'value_json': value_json,\n",
    "            'details': details,\n",
    "        })\n",
    "\n",
    "selected_db0_values_by_key = {item['key']: item for item in selected_db0_values}\n",
    "\n",
    "db1_entries = []\n",
    "if selected_manifest and selected_backups is not None:\n",
    "    files_map = selected_manifest.get('files', {}) or {}\n",
    "    db1_file = files_map.get('1')\n",
    "    if db1_file:\n",
    "        data = selected_backups.get(db1_file) if isinstance(selected_backups, dict) else None\n",
    "        if isinstance(data, dict):\n",
    "            db1_entries = data.get('entries') or []\n",
    "\n",
    "db1_entries_summary = []\n",
    "for entry in db1_entries:\n",
    "    try:\n",
    "        key_bytes = decode_key(entry)\n",
    "        key_text = key_bytes.decode('utf-8', errors='replace')\n",
    "    except Exception as exc:\n",
    "        key_text = f'<unable to decode key: {exc}>'\n",
    "    preview, details = try_decode_value(entry)\n",
    "    value_bytes = details.get('decoded_bytes') if isinstance(details, dict) else None\n",
    "    if isinstance(value_bytes, (bytes, bytearray)):\n",
    "        value_text = value_bytes.decode('utf-8', errors='replace')\n",
    "    else:\n",
    "        value_text = str(preview)\n",
    "    value_json = None\n",
    "    if isinstance(value_text, str):\n",
    "        try:\n",
    "            value_json = json.loads(value_text)\n",
    "        except Exception:\n",
    "            value_json = None\n",
    "    headline, extra = summarise_entry_generic(key_text, value_json, value_text)\n",
    "    db1_entries_summary.append({\n",
    "        'key': key_text,\n",
    "        'type': entry.get('type'),\n",
    "        'ttl_ms': entry.get('pttl'),\n",
    "        'headline': headline,\n",
    "        'details': extra,\n",
    "    })\n",
    "\n",
    "other_db_summaries = []\n",
    "if selected_manifest and selected_backups is not None:\n",
    "    files_map = selected_manifest.get('files', {}) or {}\n",
    "    selected_zip_path = Path(selected_archive_data['zip_path']) if selected_archive_data else None\n",
    "    for db_index in range(1, 10):\n",
    "        file_name = files_map.get(str(db_index))\n",
    "        if not file_name or selected_zip_path is None:\n",
    "            continue\n",
    "        label = DB_LABELS.get(db_index, 'Unknown')\n",
    "        data = selected_backups.get(file_name) if isinstance(selected_backups, dict) else None\n",
    "        metadata = {}\n",
    "        if isinstance(data, dict):\n",
    "            metadata = data.get('metadata') or {}\n",
    "        if not metadata:\n",
    "            member_name = f\"{selected_manifest_prefix}{file_name}\"\n",
    "            metadata = _fetch_backup_metadata(selected_zip_path, member_name)\n",
    "        key_count = metadata.get('key_count') if isinstance(metadata, dict) else None\n",
    "        type_summary = metadata.get('type_summary') if isinstance(metadata, dict) else None\n",
    "        other_db_summaries.append({\n",
    "            'db_index': db_index,\n",
    "            'label': label,\n",
    "            'file_name': file_name,\n",
    "            'key_count': key_count,\n",
    "            'type_summary': type_summary if isinstance(type_summary, dict) else None,\n",
    "        })\n",
    "\n",
    "selected_db0_summary = []\n",
    "for item in selected_db0_values:\n",
    "    headline, extra = summarise_entry(item['key'], item['value_json'], item['value_text'], skip_sample_keys=True)\n",
    "    if headline is None and extra is None:\n",
    "        continue\n",
    "    selected_db0_summary.append({\n",
    "        'key': item['key'],\n",
    "        'type': item['type'],\n",
    "        'ttl_ms': item['ttl_ms'],\n",
    "        'headline': headline,\n",
    "        'details': extra,\n",
    "    })\n",
    "\n",
    "if selected_db0_summary:\n",
    "    print(f\"DB 0 entries for {selected_zip_name}:\")\n",
    "    for entry in selected_db0_summary:\n",
    "        ttl = entry['ttl_ms'] if isinstance(entry['ttl_ms'], int) else 'persistent'\n",
    "        print(f\"{entry['key']} (type={entry['type']}, ttl={ttl})\")\n",
    "        print(f\"{entry['headline']}\")\n",
    "        for detail in entry['details']:\n",
    "            print(f\"    {detail}\")\n",
    "else:\n",
    "    print('No DB 0 data available for the current selection.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpmnpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
