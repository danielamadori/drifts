{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis Backup Analysis\n",
    "\n",
    "This notebook lets you inspect JSON files generated by ``redis_backup``. It loads the payload, prints summary information, and attempts to decode string values so you can read them easily.\n"
   ],
   "id": "9fa53d39315b698b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the backup path\n",
    "\n",
    "Set the path to the JSON backup exported with ``save_backup_to_file``. The notebook never connects to Redis; it reads everything straight from the file on disk.\n"
   ],
   "id": "f7d159ed5ec5d112"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:24:52.660380Z",
     "start_time": "2025-10-26T17:24:52.656311Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterable\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from redis_backup import (\n",
    "    decode_bytes,\n",
    "    display_backup_summary,\n",
    "    load_backup_from_file,\n",
    ")\n"
   ],
   "id": "23c57a9ed7d13a3d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:24:52.679039Z",
     "start_time": "2025-10-26T17:24:52.674698Z"
    }
   },
   "source": [
    "# Path to the backup you want to inspect.\n",
    "BACKUP_PATH = Path('/path/to/your/backup.json')\n",
    "\n",
    "backup = None\n",
    "if BACKUP_PATH.exists():\n",
    "    backup = load_backup_from_file(BACKUP_PATH)\n",
    "    display_backup_summary(backup)\n",
    "else:\n",
    "    print(\"⚠️ Update BACKUP_PATH with the correct path to your backup file.\")\n"
   ],
   "id": "ff03e496ace20925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Update BACKUP_PATH with the correct path to your backup file.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:24:52.713356Z",
     "start_time": "2025-10-26T17:24:52.694798Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class DumpDecodeError(RuntimeError):\n",
    "    \"\"\"Generic error raised while decoding a Redis DUMP payload.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DumpSections:\n",
    "    payload: bytes\n",
    "    version: int\n",
    "    checksum: bytes\n",
    "\n",
    "\n",
    "def split_dump_sections(raw: bytes) -> DumpSections:\n",
    "    \"\"\"Split payload, RDB version, and checksum from a Redis dump.\"\"\"\n",
    "\n",
    "    if len(raw) < 10:\n",
    "        raise DumpDecodeError(\"DUMP payload is too short to contain metadata\")\n",
    "    checksum = raw[-8:]\n",
    "    version_bytes = raw[-10:-8]\n",
    "    version = int.from_bytes(version_bytes, \"little\", signed=False)\n",
    "    payload = raw[:-10]\n",
    "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
    "\n",
    "\n",
    "class _LengthEncoding:\n",
    "    __slots__ = (\"value\", \"encoding\")\n",
    "\n",
    "    def __init__(self, value: int | None, encoding: int | None = None) -> None:\n",
    "        self.value = value\n",
    "        self.encoding = encoding\n",
    "\n",
    "\n",
    "RDB_ENCODING_INT8 = 0\n",
    "RDB_ENCODING_INT16 = 1\n",
    "RDB_ENCODING_INT32 = 2\n",
    "RDB_ENCODING_LZF = 3\n",
    "\n",
    "\n",
    "def _read_length_info(buffer: bytes, offset: int) -> tuple[_LengthEncoding, int]:\n",
    "    if offset >= len(buffer):\n",
    "        raise DumpDecodeError(\"Offset out of range while reading length\")\n",
    "    first = buffer[offset]\n",
    "    prefix = first >> 6\n",
    "    if prefix == 0:\n",
    "        length = first & 0x3F\n",
    "        return _LengthEncoding(length), offset + 1\n",
    "    if prefix == 1:\n",
    "        if offset + 1 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 14-bit encoded length\")\n",
    "        second = buffer[offset + 1]\n",
    "        length = ((first & 0x3F) << 8) | second\n",
    "        return _LengthEncoding(length), offset + 2\n",
    "    if prefix == 2:\n",
    "        if offset + 4 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded length\")\n",
    "        length = int.from_bytes(buffer[offset + 1 : offset + 5], \"big\", signed=False)\n",
    "        return _LengthEncoding(length), offset + 5\n",
    "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
    "\n",
    "\n",
    "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
    "    \"\"\"Minimal implementation of the LZF decompression used by Redis.\"\"\"\n",
    "\n",
    "    output = bytearray()\n",
    "    idx = 0\n",
    "    data_len = len(data)\n",
    "    while idx < data_len:\n",
    "        ctrl = data[idx]\n",
    "        idx += 1\n",
    "        if ctrl < 32:\n",
    "            literal_len = ctrl + 1\n",
    "            if idx + literal_len > data_len:\n",
    "                raise DumpDecodeError(\"Truncated literal LZF sequence\")\n",
    "            output.extend(data[idx : idx + literal_len])\n",
    "            idx += literal_len\n",
    "        else:\n",
    "            length = ctrl >> 5\n",
    "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
    "            if length == 7:\n",
    "                if idx >= data_len:\n",
    "                    raise DumpDecodeError(\"Truncated LZF sequence while extending length\")\n",
    "                length += data[idx]\n",
    "                idx += 1\n",
    "            if idx >= data_len:\n",
    "                raise DumpDecodeError(\"Truncated LZF sequence while resolving reference\")\n",
    "            ref_offset -= data[idx]\n",
    "            idx += 1\n",
    "            length += 2\n",
    "            if ref_offset < 0:\n",
    "                raise DumpDecodeError(\"Negative LZF reference\")\n",
    "            for _ in range(length):\n",
    "                if ref_offset >= len(output):\n",
    "                    raise DumpDecodeError(\"LZF reference out of range\")\n",
    "                output.append(output[ref_offset])\n",
    "                ref_offset += 1\n",
    "    if len(output) != expected_length:\n",
    "        raise DumpDecodeError(\n",
    "            f\"Unexpected decompressed length: expected {expected_length}, got {len(output)}\"\n",
    "        )\n",
    "    return bytes(output)\n",
    "\n",
    "\n",
    "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int) -> tuple[bytes, int]:\n",
    "    if encoding == RDB_ENCODING_INT8:\n",
    "        if offset >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 8-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 1], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 1\n",
    "    if encoding == RDB_ENCODING_INT16:\n",
    "        if offset + 1 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 16-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 2], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 2\n",
    "    if encoding == RDB_ENCODING_INT32:\n",
    "        if offset + 3 >= len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated 32-bit encoded integer\")\n",
    "        value = int.from_bytes(buffer[offset : offset + 4], \"little\", signed=True)\n",
    "        return str(value).encode(\"ascii\"), offset + 4\n",
    "    if encoding == RDB_ENCODING_LZF:\n",
    "        length_info, offset = _read_length_info(buffer, offset)\n",
    "        if length_info.value is None:\n",
    "            raise DumpDecodeError(\"Invalid compressed length\")\n",
    "        compressed_len = length_info.value\n",
    "        length_info, offset = _read_length_info(buffer, offset)\n",
    "        if length_info.value is None:\n",
    "            raise DumpDecodeError(\"Invalid original length\")\n",
    "        uncompressed_len = length_info.value\n",
    "        end = offset + compressed_len\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated compressed LZF data\")\n",
    "        chunk = buffer[offset:end]\n",
    "        offset = end\n",
    "        return lzf_decompress(chunk, uncompressed_len), offset\n",
    "    raise DumpDecodeError(f\"Unsupported special encoding: {encoding}\")\n",
    "\n",
    "\n",
    "def _read_encoded_string(buffer: bytes, offset: int) -> tuple[bytes, int]:\n",
    "    length_info, offset = _read_length_info(buffer, offset)\n",
    "    if length_info.value is not None:\n",
    "        length = length_info.value\n",
    "        end = offset + length\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError(\"Truncated encoded string\")\n",
    "        return buffer[offset:end], end\n",
    "    if length_info.encoding is None:\n",
    "        raise DumpDecodeError(\"Unknown string encoding\")\n",
    "    return _decode_special_encoding(buffer, offset, length_info.encoding)\n",
    "\n",
    "\n",
    "def decode_string_from_dump(raw: bytes) -> bytes:\n",
    "    sections = split_dump_sections(raw)\n",
    "    payload = sections.payload\n",
    "    if not payload:\n",
    "        raise DumpDecodeError(\"Empty payload\")\n",
    "    object_type = payload[0]\n",
    "    if object_type != 0:\n",
    "        raise DumpDecodeError(f\"Non-string object type: {object_type}\")\n",
    "    value, offset = _read_encoded_string(payload, 1)\n",
    "    if offset != len(payload):\n",
    "        # Ignore unexpected trailing bytes\n",
    "        value = value\n",
    "    return value\n",
    "\n",
    "\n",
    "def decode_key(entry: dict[str, Any]) -> bytes:\n",
    "    return decode_bytes(entry[\"key\"])\n",
    "\n",
    "\n",
    "def text_preview(value: bytes, *, limit: int = 120) -> str:\n",
    "    text = value.decode(\"utf-8\", errors=\"replace\")\n",
    "    if len(text) > limit:\n",
    "        return text[: limit - 1] + \"…\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def try_decode_value(entry: dict[str, Any]) -> tuple[str, dict[str, Any]]:\n",
    "    value_info = dict(entry.get(\"value\") or {})\n",
    "    data_b64 = value_info.get(\"data\")\n",
    "    if not data_b64:\n",
    "        return \"<no value>\", value_info\n",
    "    raw = decode_bytes(data_b64)\n",
    "    details: dict[str, Any] = {\n",
    "        \"dump_size\": len(raw),\n",
    "    }\n",
    "    try:\n",
    "        sections = split_dump_sections(raw)\n",
    "        details[\"rdb_version\"] = sections.version\n",
    "        details[\"checksum\"] = sections.checksum.hex()\n",
    "    except DumpDecodeError as exc:\n",
    "        details[\"dump_error\"] = str(exc)\n",
    "        return \"<invalid dump>\", details\n",
    "    if entry.get(\"type\") == \"string\":\n",
    "        try:\n",
    "            decoded = decode_string_from_dump(raw)\n",
    "        except DumpDecodeError as exc:\n",
    "            details[\"decode_error\"] = str(exc)\n",
    "            return \"<string not decoded>\", details\n",
    "        details[\"decoded_bytes\"] = decoded\n",
    "        preview = text_preview(decoded)\n",
    "        return preview, details\n",
    "    return f\"<{entry.get('type')} - {len(sections.payload)} bytes>\", details\n",
    "\n",
    "\n",
    "def summarise_entries(entries: Iterable[dict[str, Any]], limit: int = 20) -> list[dict[str, Any]]:\n",
    "    summary: list[dict[str, Any]] = []\n",
    "    for index, entry in enumerate(entries):\n",
    "        if index >= limit:\n",
    "            break\n",
    "        key_bytes = decode_key(entry)\n",
    "        value_preview, _ = try_decode_value(entry)\n",
    "        summary.append(\n",
    "            {\n",
    "                \"key\": text_preview(key_bytes),\n",
    "                \"type\": entry.get(\"type\"),\n",
    "                \"ttl_ms\": entry.get(\"pttl\"),\n",
    "                \"value_preview\": value_preview,\n",
    "            }\n",
    "        )\n",
    "    return summary\n",
    "\n",
    "\n",
    "def find_entry(entries: Iterable[dict[str, Any]], key: bytes | str) -> dict[str, Any]:\n",
    "    if isinstance(key, str):\n",
    "        key_bytes = key.encode(\"utf-8\")\n",
    "    else:\n",
    "        key_bytes = key\n",
    "    for entry in entries:\n",
    "        if decode_key(entry) == key_bytes:\n",
    "            return entry\n",
    "    raise KeyError(key)\n",
    "\n",
    "\n",
    "def inspect_entry(entry: dict[str, Any]) -> None:\n",
    "    key_bytes = decode_key(entry)\n",
    "    print(f\"Key: {key_bytes!r}\")\n",
    "    print(f\"Redis type: {entry.get('type')}\")\n",
    "    ttl = entry.get(\"pttl\")\n",
    "    print(f\"TTL (ms): {ttl if ttl is not None else 'persistent'}\")\n",
    "    preview, details = try_decode_value(entry)\n",
    "    print(f\"Value preview: {preview}\")\n",
    "    print(\"Details:\")\n",
    "    pprint.pprint(details)\n",
    "    decoded = details.get(\"decoded_bytes\")\n",
    "    if isinstance(decoded, (bytes, bytearray)):\n",
    "        text = decoded.decode(\"utf-8\", errors=\"replace\")\n",
    "        print(\"\\nPlaintext content:\")\n",
    "        print(text)\n",
    "        stripped = text.strip()\n",
    "        if stripped.startswith(\"{\") and stripped.endswith(\"}\"):\n",
    "            try:\n",
    "                parsed = json.loads(text)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"\\nParsed JSON:\")\n",
    "                pprint.pprint(parsed)\n"
   ],
   "id": "41a6e613b2a0a955",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:24:52.720801Z",
     "start_time": "2025-10-26T17:24:52.717777Z"
    }
   },
   "source": [
    "if backup is not None:\n",
    "    entries = list(backup.get('entries', []))\n",
    "    print(f'Total keys in backup: {len(entries)}')\n",
    "    preview_rows = summarise_entries(entries, limit=20)\n",
    "    try:\n",
    "        import pandas as pd  # type: ignore\n",
    "    except ModuleNotFoundError:\n",
    "        pd = None\n",
    "    if preview_rows:\n",
    "        if 'pd' in locals() and pd is not None:\n",
    "            display(pd.DataFrame(preview_rows))\n",
    "        else:\n",
    "            for row in preview_rows:\n",
    "                print(row)\n",
    "    else:\n",
    "        print('No keys found in the selected backup.')\n"
   ],
   "id": "e8fa5e89119a284f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a specific key\n",
    "\n",
    "Use ``find_entry`` to retrieve a key from the backup and ``inspect_entry`` to display its metadata and decoded value when possible.\n"
   ],
   "id": "6c28962057514b99"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:24:52.727662Z",
     "start_time": "2025-10-26T17:24:52.725803Z"
    }
   },
   "source": [
    "if backup is not None:\n",
    "    # Example: replace 'namespace:your:key' with the key you want to inspect.\n",
    "    try:\n",
    "        entry = find_entry(entries, 'namespace:your:key')\n",
    "    except KeyError:\n",
    "        print('Key not found: update the name to inspect its contents.')\n",
    "    else:\n",
    "        inspect_entry(entry)\n"
   ],
   "id": "31a484bc87f7cb9",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
