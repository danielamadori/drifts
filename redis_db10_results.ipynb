{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redis DB10 Results Explorer\n",
        "\n",
        "This notebook loads the Redis backup for database 10 stored in the results archives and decodes the serialized JSON payloads so they can be analysed as regular pandas tables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage\n",
        "\n",
        "1. Update `BACKUP_SOURCE` below if you want to inspect a different archive or a raw JSON file.\n",
        "2. Run the notebook top-to-bottom to load the backup and build a DataFrame with decoded entries.\n",
        "3. Use the provided summaries or extend the notebook with your own analysis steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import base64\n",
        "import binascii\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DumpDecodeError(RuntimeError):\n",
        "    'Raised when a Redis DUMP payload cannot be decoded.'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DumpSections:\n",
        "    payload: bytes\n",
        "    version: int\n",
        "    checksum: bytes\n",
        "\n",
        "\n",
        "class _LengthEncoding:\n",
        "    __slots__ = ('value', 'encoding')\n",
        "\n",
        "    def __init__(self, value=None, encoding=None):\n",
        "        self.value = value\n",
        "        self.encoding = encoding\n",
        "\n",
        "\n",
        "RDB_ENCODING_INT8 = 0\n",
        "RDB_ENCODING_INT16 = 1\n",
        "RDB_ENCODING_INT32 = 2\n",
        "RDB_ENCODING_LZF = 3\n",
        "\n",
        "\n",
        "def split_dump_sections(raw: bytes) -> DumpSections:\n",
        "    if len(raw) < 10:\n",
        "        raise DumpDecodeError('DUMP payload is too short to contain metadata')\n",
        "    checksum = raw[-8:]\n",
        "    version_bytes = raw[-10:-8]\n",
        "    version = int.from_bytes(version_bytes, 'little', signed=False)\n",
        "    payload = raw[:-10]\n",
        "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
        "\n",
        "\n",
        "def _read_length_info(buffer: bytes, offset: int):\n",
        "    if offset >= len(buffer):\n",
        "        raise DumpDecodeError('Offset out of range while reading length')\n",
        "    first = buffer[offset]\n",
        "    prefix = first >> 6\n",
        "    if prefix == 0:\n",
        "        length = first & 0x3F\n",
        "        return _LengthEncoding(length), offset + 1\n",
        "    if prefix == 1:\n",
        "        if offset + 1 >= len(buffer):\n",
        "            raise DumpDecodeError('Truncated 14-bit encoded length')\n",
        "        second = buffer[offset + 1]\n",
        "        length = ((first & 0x3F) << 8) | second\n",
        "        return _LengthEncoding(length), offset + 2\n",
        "    if prefix == 2:\n",
        "        if offset + 4 >= len(buffer):\n",
        "            raise DumpDecodeError('Truncated 32-bit encoded length')\n",
        "        length = int.from_bytes(buffer[offset + 1 : offset + 5], 'big', signed=False)\n",
        "        return _LengthEncoding(length), offset + 5\n",
        "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
        "\n",
        "\n",
        "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
        "    output = bytearray()\n",
        "    idx = 0\n",
        "    data_len = len(data)\n",
        "    while idx < data_len:\n",
        "        ctrl = data[idx]\n",
        "        idx += 1\n",
        "        if ctrl < 32:\n",
        "            literal_len = ctrl + 1\n",
        "            if idx + literal_len > data_len:\n",
        "                raise DumpDecodeError('Truncated literal LZF sequence')\n",
        "            output.extend(data[idx : idx + literal_len])\n",
        "            idx += literal_len\n",
        "        else:\n",
        "            length = ctrl >> 5\n",
        "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
        "            if length == 7:\n",
        "                if idx >= data_len:\n",
        "                    raise DumpDecodeError('Truncated LZF sequence while extending length')\n",
        "                length += data[idx]\n",
        "                idx += 1\n",
        "            if idx >= data_len:\n",
        "                raise DumpDecodeError('Truncated LZF sequence while resolving reference')\n",
        "            ref_offset -= data[idx]\n",
        "            idx += 1\n",
        "            length += 2\n",
        "            if ref_offset < 0:\n",
        "                raise DumpDecodeError('Negative LZF reference')\n",
        "            for _ in range(length):\n",
        "                if ref_offset >= len(output):\n",
        "                    raise DumpDecodeError('LZF reference out of range')\n",
        "                output.append(output[ref_offset])\n",
        "                ref_offset += 1\n",
        "    if len(output) != expected_length:\n",
        "        raise DumpDecodeError(\n",
        "            f\"Unexpected decompressed length: expected {expected_length}, got {len(output)}\"\n",
        "        )\n",
        "    return bytes(output)\n",
        "\n",
        "\n",
        "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int):\n",
        "    if encoding == RDB_ENCODING_INT8:\n",
        "        if offset >= len(buffer):\n",
        "            raise DumpDecodeError('Truncated 8-bit encoded integer')\n",
        "        value = int.from_bytes(buffer[offset : offset + 1], 'little', signed=True)\n",
        "        return str(value).encode('ascii'), offset + 1\n",
        "    if encoding == RDB_ENCODING_INT16:\n",
        "        if offset + 2 > len(buffer):\n",
        "            raise DumpDecodeError('Truncated 16-bit encoded integer')\n",
        "        value = int.from_bytes(buffer[offset : offset + 2], 'little', signed=True)\n",
        "        return str(value).encode('ascii'), offset + 2\n",
        "    if encoding == RDB_ENCODING_INT32:\n",
        "        if offset + 4 > len(buffer):\n",
        "            raise DumpDecodeError('Truncated 32-bit encoded integer')\n",
        "        value = int.from_bytes(buffer[offset : offset + 4], 'little', signed=True)\n",
        "        return str(value).encode('ascii'), offset + 4\n",
        "    if encoding == RDB_ENCODING_LZF:\n",
        "        compressed_len_info, next_offset = _read_length_info(buffer, offset)\n",
        "        data_len_info, data_offset = _read_length_info(buffer, next_offset)\n",
        "        if compressed_len_info.value is None or data_len_info.value is None:\n",
        "            raise DumpDecodeError('Invalid LZF length encoding')\n",
        "        end = data_offset + compressed_len_info.value\n",
        "        if end > len(buffer):\n",
        "            raise DumpDecodeError('Truncated encoded string')\n",
        "        compressed = buffer[data_offset:end]\n",
        "        decompressed = lzf_decompress(compressed, data_len_info.value)\n",
        "        return decompressed, end\n",
        "    raise DumpDecodeError('Unknown string encoding')\n",
        "\n",
        "\n",
        "def _read_encoded_string(buffer: bytes, offset: int):\n",
        "    length_info, next_offset = _read_length_info(buffer, offset)\n",
        "    if length_info.encoding is None:\n",
        "        end = next_offset + length_info.value\n",
        "        if end > len(buffer):\n",
        "            raise DumpDecodeError('Truncated encoded string')\n",
        "        return buffer[next_offset:end], end\n",
        "    return _decode_special_encoding(buffer, next_offset, length_info.encoding)\n",
        "\n",
        "\n",
        "def decode_string_from_dump(raw: bytes) -> bytes:\n",
        "    sections = split_dump_sections(raw)\n",
        "    payload = sections.payload\n",
        "    if not payload:\n",
        "        raise DumpDecodeError('Empty payload')\n",
        "    object_type = payload[0]\n",
        "    if object_type != 0:\n",
        "        raise DumpDecodeError(f'Non-string object type: {object_type}')\n",
        "    value, _ = _read_encoded_string(payload, 1)\n",
        "    return value\n",
        "\n",
        "\n",
        "def decode_base64_bytes(value: str) -> bytes:\n",
        "    if not isinstance(value, str):\n",
        "        raise DumpDecodeError('Encoded value must be a base64 string')\n",
        "    try:\n",
        "        return base64.b64decode(value.encode('ascii'))\n",
        "    except (UnicodeEncodeError, binascii.Error) as exc:\n",
        "        raise DumpDecodeError(f'Invalid base64 payload: {exc}') from exc\n",
        "\n",
        "\n",
        "def decode_entry(entry: dict) -> dict:\n",
        "    key_bytes = decode_base64_bytes(entry['key'])\n",
        "    key_text = key_bytes.decode('utf-8', errors='replace')\n",
        "    value_info = entry.get('value') or {}\n",
        "    data_b64 = value_info.get('data')\n",
        "    if not data_b64:\n",
        "        raise DumpDecodeError('Missing DUMP payload in backup entry')\n",
        "    raw_value = decode_base64_bytes(data_b64)\n",
        "    decoded_bytes = decode_string_from_dump(raw_value)\n",
        "    text_value = decoded_bytes.decode('utf-8', errors='replace')\n",
        "    try:\n",
        "        json_value = json.loads(text_value)\n",
        "    except json.JSONDecodeError:\n",
        "        json_value = None\n",
        "    return {\n",
        "        'redis_key': key_text,\n",
        "        'decoded_bytes': decoded_bytes,\n",
        "        'text': text_value,\n",
        "        'json': json_value,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_backup_payload(source: Path, db_number: int = 10) -> dict:\n",
        "    'Load a redis_backup_dbXX.json payload from a JSON file, directory, or ZIP archive.'\n",
        "    source = source.expanduser()\n",
        "    if source.is_dir():\n",
        "        candidate = source / f'redis_backup_db{db_number}.json'\n",
        "        if not candidate.exists():\n",
        "            raise FileNotFoundError(f'Backup file not found under {source}')\n",
        "        return json.loads(candidate.read_text(encoding='utf-8'))\n",
        "    if not source.exists():\n",
        "        raise FileNotFoundError(f'Backup source not found: {source}')\n",
        "    suffix = source.suffix.lower()\n",
        "    if suffix == '.json':\n",
        "        return json.loads(source.read_text(encoding='utf-8'))\n",
        "    if suffix == '.zip':\n",
        "        target_name = f'redis_backup_db{db_number}.json'\n",
        "        with ZipFile(source) as archive:\n",
        "            matches = [name for name in archive.namelist() if name.endswith(target_name)]\n",
        "            if not matches:\n",
        "                raise FileNotFoundError(f'{target_name} not found inside {source.name}')\n",
        "            if len(matches) > 1:\n",
        "                print(f'Warning: multiple matches found, using {matches[0]}')\n",
        "            data = archive.read(matches[0])\n",
        "            return json.loads(data.decode('utf-8'))\n",
        "    raise ValueError(f'Unsupported backup source: {source}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('results/ECG200_-1_false_0.zip'),\n",
              " WindowsPath('results/HandOutlines_0_false_0.zip'),\n",
              " WindowsPath('results/MelbournePedestrian_1_false_0.zip'),\n",
              " WindowsPath('results/MiddlePhalanxOutlineCorrect_0_false_0.zip'),\n",
              " WindowsPath('results/SonyAIBORobotSurface1_1_false_0.zip'),\n",
              " WindowsPath('results/Wafer_-1_false_0.zip'),\n",
              " WindowsPath('results/Wine_1_false_0.zip')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_dir = Path('results')\n",
        "available_archives = sorted(results_dir.glob('*.zip'))\n",
        "available_archives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 32738 entries from results\\SonyAIBORobotSurface1_1_false_0.zip\n",
            "Metadata:\n",
            "{\n",
            "  \"created_at_utc\": \"2025-10-25T13:52:26.924139Z\",\n",
            "  \"key_count\": 32738,\n",
            "  \"scan_count\": 1000,\n",
            "  \"source\": {\n",
            "    \"db\": 10,\n",
            "    \"host\": \"127.0.0.1\",\n",
            "    \"port\": 6379\n",
            "  },\n",
            "  \"type_summary\": {\n",
            "    \"string\": 32738\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "BACKUP_SOURCE = Path('results') / 'SonyAIBORobotSurface1_1_false_0.zip'\n",
        "DB_NUMBER = 10\n",
        "\n",
        "backup_payload = load_backup_payload(BACKUP_SOURCE, db_number=DB_NUMBER)\n",
        "entries = backup_payload.get('entries', [])\n",
        "\n",
        "print(f'Loaded {len(entries)} entries from {BACKUP_SOURCE}')\n",
        "metadata = backup_payload.get('metadata')\n",
        "if metadata:\n",
        "    print('Metadata:')\n",
        "    print(json.dumps(metadata, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded 32738 entries; 0 failures.\n"
          ]
        }
      ],
      "source": [
        "decoded_entries = []\n",
        "failed_entries = []\n",
        "\n",
        "for entry in entries:\n",
        "    try:\n",
        "        decoded_entries.append(decode_entry(entry))\n",
        "    except DumpDecodeError as exc:\n",
        "        failed_entries.append({'entry': entry, 'error': str(exc)})\n",
        "\n",
        "print(f'Decoded {len(decoded_entries)} entries; {len(failed_entries)} failures.')\n",
        "if failed_entries:\n",
        "    print('First failure:')\n",
        "    print(failed_entries[0]['error'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample key: thething:worker_24003:14\n",
            "{\n",
            "  \"worker_id\": \"thething:worker_24003\",\n",
            "  \"iteration\": 14,\n",
            "  \"timestamp_start\": \"2025-10-25T13:54:48.637414\",\n",
            "  \"queue_size\": 54,\n",
            "  \"car_queue_size\": 1,\n",
            "  \"car_processing\": {\n",
            "    \"candidate_bitmap\": \"111011011110111011011011101101111101101100110111111111111110110110111011011\",\n",
            "    \"icf_size\": 24,\n",
            "    \"result\": \"CONFIRMED_AR\",\n",
            "    \"time_seconds\": 0.10586810111999512,\n",
            "    \"raw_info\": {\n",
            "      \"ar_iterations\": 1,\n",
            "      \"ar_profile_dominated_by_AP\": 1,\n",
            "      \"deleted_from_AR\": 1,\n",
            "      \"deleted_from_AP\": 2,\n",
            "      \"deleted_from_CAR\": 9,\n",
            "      \"ar_extensions_total\": 17,\n",
            "      \"ar_ext_AR_cache_checks\": 3638,\n",
            "      \"ar_ext_R_cache_checks\": 4040,\n",
            "      \"ar_ext_R_cache_hits\": 1,\n",
            "      \"ar_ext_R_shares_sample\": 1,\n",
            "      \"ar_extensions_filtered_by_R\": 1,\n",
            "      \"ar_extensions_added\": 16,\n",
            "      \"ar_extensions_added_to_CAR\": 16\n",
            "    },\n",
            "    \"extensions\": {\n",
            "      \"total\": 17,\n",
            "      \"added\": 0,\n",
            "      \"filtered\": 17,\n",
            "      \"filtered_by_ar\": 0,\n",
            "      \"filtered_by_r_sharing\": 1,\n",
            "      \"filtered_by_ap\": 16\n",
            "    }\n",
            "  },\n",
            "  \"can_processing\": {\n",
            "    \"candidate_bitmap\": \"110011111110111111110011101111111001111000111101101101100111100011011011110\",\n",
            "    \"icf_size\": 24,\n",
            "    \"result\": \"GOOD\",\n",
            "    \"time_seconds\": 0.002104520797729492,\n",
            "    \"raw_info\": {\n",
            "      \"iterations\": 1,\n",
            "      \"dominated_by_R\": 1,\n",
            "      \"deleted_from_R\": 0,\n",
            "      \"deleted_from_GP\": 0,\n",
            "      \"deleted_from_CAN\": 0\n",
            "    },\n",
            "    \"extensions\": {\n",
            "      \"total\": 20,\n",
            "      \"added\": 12,\n",
            "      \"filtered\": 8\n",
            "    }\n",
            "  },\n",
            "  \"outcomes\": {\n",
            "    \"car_confirmed_ar\": true,\n",
            "    \"good\": true\n",
            "  },\n",
            "  \"timings\": {\n",
            "    \"total_seconds\": 0.5415184497833252,\n",
            "    \"car_seconds\": 0.10586810111999512,\n",
            "    \"can_seconds\": 0.002104520797729492\n",
            "  },\n",
            "  \"timestamp_end\": \"2025-10-25T13:54:49.178940\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "if decoded_entries:\n",
        "    sample = decoded_entries[0]\n",
        "    print(f\"Sample key: {sample['redis_key']}\")\n",
        "    print(json.dumps(sample['json'], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m payload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m flat = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m flat[\u001b[33m'\u001b[39m\u001b[33mredis_key\u001b[39m\u001b[33m'\u001b[39m] = decoded[\u001b[33m'\u001b[39m\u001b[33mredis_key\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m records.append(flat)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\io\\json\\_normalize.py:457\u001b[39m, in \u001b[36mjson_normalize\u001b[39m\u001b[34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# check to see if a simple recursive function is possible to\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# improve performance (see #15621) but only for cases such\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# as pd.Dataframe(data) or pd.Dataframe(data, sep)\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    451\u001b[39m     record_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    452\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    455\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m max_level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    456\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_simple_json_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m record_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y.values()] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m data):\n\u001b[32m    461\u001b[39m         \u001b[38;5;66;03m# naive normalization, this is idempotent for flat records\u001b[39;00m\n\u001b[32m    462\u001b[39m         \u001b[38;5;66;03m# and potentially will inflate the data considerably for\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m         \u001b[38;5;66;03m# TODO: handle record value which are lists, at least error\u001b[39;00m\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m#       reasonably\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\core\\frame.py:859\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    850\u001b[39m         columns = ensure_index(columns)\n\u001b[32m    851\u001b[39m     arrays, columns, index = nested_data_to_arrays(\n\u001b[32m    852\u001b[39m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[32m    853\u001b[39m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    857\u001b[39m         dtype,\n\u001b[32m    858\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m     mgr = \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    867\u001b[39m     mgr = ndarray_to_mgr(\n\u001b[32m    868\u001b[39m         data,\n\u001b[32m    869\u001b[39m         index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    873\u001b[39m         typ=manager,\n\u001b[32m    874\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2142\u001b[39m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[32m0\u001b[39m].shape, axes, e)\n\u001b[32m   2143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[32m-> \u001b[39m\u001b[32m2144\u001b[39m     \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\base\\bpmnpy\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2301\u001b[39m     new_values = new_values[argsort]\n\u001b[32m   2302\u001b[39m     new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m     bp = \u001b[43mBlockPlacement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement=bp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2307\u001b[39m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "records = []\n",
        "for decoded in decoded_entries:\n",
        "    payload = decoded['json']\n",
        "    if payload is None:\n",
        "        continue\n",
        "    flat = pd.json_normalize(payload, sep='_')\n",
        "    flat['redis_key'] = decoded['redis_key']\n",
        "    records.append(flat)\n",
        "\n",
        "if records:\n",
        "    df = pd.concat(records, ignore_index=True)\n",
        "    df = df.set_index('redis_key')\n",
        "    df.head()\n",
        "else:\n",
        "    df = pd.DataFrame()\n",
        "    df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        if not df.empty:\n",
        "            summary = {\n",
        "                'worker_count': int(df['worker_id'].nunique()),\n",
        "                'total_iterations': int(df['iteration'].max()),\n",
        "                'total_runtime_hours': float(df['timings_total_seconds'].sum() / 3600.0),\n",
        "            }\n",
        "            print('Summary:')\n",
        "            print(json.dumps(summary, indent=2))\n",
        "            print('\n",
        "CAR results:')\n",
        "            print(df['car_processing_result'].value_counts())\n",
        "            print('\n",
        "CAN results:')\n",
        "            print(df['can_processing_result'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    per_worker = (\n",
        "        df.groupby('worker_id')\n",
        "        .agg({\n",
        "            'iteration': ['count', 'max'],\n",
        "            'queue_size': ['mean', 'max'],\n",
        "            'timings_total_seconds': 'sum',\n",
        "        })\n",
        "    )\n",
        "    per_worker.columns = ['_'.join(map(str, col)).strip('_') for col in per_worker.columns]\n",
        "    per_worker.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
